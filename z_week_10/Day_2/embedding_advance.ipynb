{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3b0f30",
   "metadata": {},
   "source": [
    "# üìò Introduction to Word Embeddings for AI Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b60290",
   "metadata": {},
   "source": [
    "### Welcome to Your 2-Hour Journey into Word Embeddings!\n",
    "\n",
    "Over the next two hours, we will explore one of the most important concepts in modern Artificial Intelligence and Natural Language Processing (NLP). Get ready to learn how we teach computers to understand human language!\n",
    "\n",
    "**üéØ Learning Objectives:**\n",
    "1.  Understand what word embeddings are and why they are essential for AI.\n",
    "2.  Learn about different types of embeddings, from classic to state-of-the-art.\n",
    "3.  See how vector math can capture word meanings (e.g., king - man + woman = queen).\n",
    "4.  Write basic Python code using the Keras `Embedding` layer.\n",
    "5.  Know when to use pre-trained embeddings versus training your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e551aa",
   "metadata": {},
   "source": [
    "## Topic 1: What Are Word Embeddings?\n",
    "\n",
    "Word embeddings are numerical representations of words. Think of them as coordinates for each word in a multi-dimensional 'meaning space'. \n",
    "\n",
    "This technique allows words with similar meanings to have similar vector representations (similar coordinates). This is how we get computers to understand that 'cat' is more similar to 'dog' than it is to 'car'.\n",
    "\n",
    "The core idea comes from the **distributional hypothesis**: *\"You shall know a word by the company it keeps.\"* This means words that appear in similar sentences (or contexts) are likely to have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea064c",
   "metadata": {},
   "source": [
    "## Topic 2: Why Not Just Use Numbers? (Old Methods)\n",
    "\n",
    "Before we had smart embeddings, we had simpler methods. One of the most basic is **One-Hot Encoding**.\n",
    "\n",
    "Imagine our entire vocabulary is just five words: `['the', 'cat', 'sat', 'on', 'mat']`.\n",
    "\n",
    "To represent the word 'cat', we create a vector that is all zeros, except for a '1' at the position for 'cat'.\n",
    "\n",
    "- `the`: `[1, 0, 0, 0, 0]`\n",
    "- `cat`: `[0, 1, 0, 0, 0]`\n",
    "- `sat`: `[0, 0, 1, 0, 0]`\n",
    "\n",
    "This seems simple, but it has big problems:\n",
    "\n",
    "üëé **High Dimensionality**: If you have a vocabulary of 50,000 words, each vector will have 50,000 dimensions! This is computationally very expensive.\n",
    "\n",
    "üëé **Sparsity**: The vectors are mostly zeros, which is not efficient.\n",
    "\n",
    "üëé **No Semantic Relationship**: The vectors for 'cat' and 'dog' are no more similar than the vectors for 'cat' and 'car'. The model can't see any relationship between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c420ac",
   "metadata": {},
   "source": [
    "### üß† Practice Task\n",
    "\n",
    "Imagine our vocabulary is `['apple', 'banana', 'fruit', 'car']`.\n",
    "\n",
    "What would the one-hot encoded vector for the word **'banana'** look like? Write it down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74ebd2",
   "metadata": {},
   "source": [
    "## Topic 3: Static Embeddings (Word2Vec, GloVe, FastText)\n",
    "\n",
    "To solve the problems of one-hot encoding, researchers created **static word embeddings**. These models learn a single, fixed vector for each word from a huge amount of text.\n",
    "\n",
    "#### Word2Vec (Google, 2013)\n",
    "A groundbreaking model that learns word associations. It has two main 'flavors':\n",
    "- **CBOW (Continuous Bag-of-Words):** Predicts a target word from its context. (e.g., given `['The', 'cat', 'on', 'the', 'mat']`, predict `sat`). It's fast and good for common words.\n",
    "- **Skip-Gram:** Predicts the context words from a target word. (e.g., given `sat`, predict `['The', 'cat', 'on', 'the', 'mat']`). It's slower but excellent for representing rare words.\n",
    "\n",
    "#### GloVe (Stanford, 2014)\n",
    "GloVe stands for **Global Vectors**. It learns by looking at how often words appear together across an entire corpus, giving it a more 'global' understanding of language.\n",
    "\n",
    "#### FastText (Facebook, 2016)\n",
    "FastText's superpower is handling **out-of-vocabulary (OOV) words**. It breaks words down into character parts (n-grams). So, if it has never seen the word `embedding`, but it has seen `embed` and `ing`, it can create a reasonable vector for the new word. This is great for languages with many word forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11101c13",
   "metadata": {},
   "source": [
    "### üí° The Magic of Semantic Arithmetic\n",
    "\n",
    "One of the coolest discoveries about word embeddings is that they capture relationships, which you can explore with simple math!\n",
    "\n",
    "The most famous example is:\n",
    "\n",
    "`vector('king') - vector('man') + vector('woman') ‚âà vector('queen')`\n",
    "\n",
    "This shows that the model has learned the concept of gender and royalty. The distance and direction between 'king' and 'man' is very similar to the distance and direction between 'queen' and 'woman'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab4b72",
   "metadata": {},
   "source": [
    "### üß† Practice Task\n",
    "\n",
    "Using the same logic, what do you think the result of the following vector math would be?\n",
    "\n",
    "`vector('Paris') - vector('France') + vector('Germany') ‚âà ?`\n",
    "\n",
    "Think about the relationship between the first two words and apply it to the third."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac12d21",
   "metadata": {},
   "source": [
    "## Topic 4: Context is King! (Contextual Embeddings)\n",
    "\n",
    "Static embeddings have a major weakness: a word has only one meaning. But what about the word **'bank'**?\n",
    "\n",
    "- I went to the **bank** to deposit money.\n",
    "- We had a picnic on the river **bank**.\n",
    "\n",
    "The meaning is completely different! **Contextual embeddings** solve this by generating a different vector for a word each time it appears, based on the surrounding sentence.\n",
    "\n",
    "- **ELMo (2018):** One of the first models to do this effectively.\n",
    "- **BERT (2018):** A revolutionary model from Google that reads the *entire sentence at once* (it's bidirectional) to create incredibly rich, context-aware embeddings.\n",
    "- **GPT:** The model behind ChatGPT, which is excellent at understanding context to generate human-like text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d531e68",
   "metadata": {},
   "source": [
    "### üß† Practice Task\n",
    "\n",
    "Write down two sentences where the word **'right'** has completely different meanings. This shows why context is so important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d2f3d",
   "metadata": {},
   "source": [
    "## Topic 5: Using Embeddings in Code (Keras)\n",
    "\n",
    "Let's see how we can use embeddings in a real deep learning model using the Keras library.\n",
    "\n",
    "The `Embedding` layer in Keras is a dictionary that maps integer indices (representing words) to dense vectors. We can either train these vectors from scratch on our own data or load in pre-trained ones.\n",
    "\n",
    "### Example: Training an Embedding Layer from Scratch\n",
    "\n",
    "This is useful when you have a lot of specific data for your task (e.g., medical records, legal documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import the necessary tools from TensorFlow and Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "# --- Model Parameters ---\n",
    "# Imagine we have 10,000 unique words in our vocabulary\n",
    "vocab_size = 10000\n",
    "\n",
    "# We will represent each word with a 128-dimensional vector\n",
    "embedding_dim = 128\n",
    "\n",
    "# Each input sentence will be padded/truncated to 50 words\n",
    "max_length = 50\n",
    "\n",
    "# --- Building the Model ---\n",
    "model = Sequential()\n",
    "\n",
    "# 1. The Embedding Layer\n",
    "# This layer will learn a 'lookup table' of size (10000 x 128)\n",
    "# It takes integer-encoded sentences of length 50 as input\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "\n",
    "# 2. The Flatten Layer\n",
    "# This layer converts the 3D tensor from the embedding layer into a 2D tensor\n",
    "# Shape changes from (None, 50, 128) to (None, 50 * 128)\n",
    "model.add(Flatten())\n",
    "\n",
    "# 3. The Output Layer\n",
    "# A standard Dense layer for a binary classification task (e.g., positive/negative sentiment)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model and print its summary\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545fa61",
   "metadata": {},
   "source": [
    "### üß™ Practice Task\n",
    "\n",
    "In the code cell above, change the `output_dim` of the `Embedding` layer from `128` to `64` and re-run the cell.\n",
    "\n",
    "Look at the model summary. How does the number of parameters in the embedding layer change? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ede41d",
   "metadata": {},
   "source": [
    "## Topic 6: Using Pre-trained GloVe Embeddings\n",
    "\n",
    "Training embeddings from scratch requires a lot of data. A more common approach is **Transfer Learning**: using embeddings that have already been trained on a massive dataset (like all of Wikipedia!).\n",
    "\n",
    "Here's how you would conceptually load pre-trained GloVe embeddings and 'freeze' them in your model, so they don't change during training.\n",
    "\n",
    "*(Note: The following code is for demonstration. You would need to download the GloVe file to run it fully.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91847e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "import tensorflow as tf # Often needed for initializers\n",
    "\n",
    "# --- Step 1: Pretend we loaded a GloVe file ---\n",
    "# In a real scenario, you'd parse the GloVe file here.\n",
    "# Let's create a fake dictionary for demonstration.\n",
    "embeddings_index = {\n",
    "    'the': np.random.rand(100), \n",
    "    'a': np.random.rand(100),\n",
    "    'cat': np.random.rand(100),\n",
    "    'dog': np.random.rand(100)\n",
    "}\n",
    "print(f\"Loaded {len(embeddings_index)} pre-trained word vectors.\")\n",
    "\n",
    "# --- Step 2: Create an embedding matrix for our vocabulary ---\n",
    "# Let's say our project's vocabulary is this:\n",
    "word_index = {'the': 1, 'cat': 2, 'dog': 3, 'house': 4} # Note: 'house' is not in GloVe\n",
    "num_tokens = len(word_index) + 1 # +1 for the padding token\n",
    "embedding_dim = 100 # Must match the GloVe file dimension\n",
    "\n",
    "# Create a matrix of zeros\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "# Fill the matrix with the GloVe vectors\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words found in the embedding index will be non-zero.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"\\nCreated our project's embedding matrix.\")\n",
    "print(\"Shape of matrix:\", embedding_matrix.shape)\n",
    "\n",
    "# --- Step 3: Create the Keras Embedding layer ---\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,  # IMPORTANT: Freeze the pre-trained weights\n",
    ")\n",
    "\n",
    "# --- Step 4: Build your model ---\n",
    "model = Sequential([\n",
    "    embedding_layer,\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nFinal Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7669e",
   "metadata": {},
   "source": [
    "‚úÖ **Well done!** You've now seen the two main ways to use embeddings in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2ccf1",
   "metadata": {},
   "source": [
    "## üèÜ Final Revision Assignment\n",
    "\n",
    "Time to test your knowledge! Try to answer these questions to solidify what you've learned. This is great for home practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6df5c6",
   "metadata": {},
   "source": [
    "#### Question 1 (MCQ)\n",
    "\n",
    "What is the primary advantage of FastText over Word2Vec and GloVe?\n",
    "\n",
    "A) It is faster to train.\n",
    "\n",
    "B) It can handle out-of-vocabulary words.\n",
    "\n",
    "C) It produces lower-dimensional vectors.\n",
    "\n",
    "D) It uses a Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd265a",
   "metadata": {},
   "source": [
    "#### Question 2 (MCQ)\n",
    "\n",
    "Which of the following models generates contextualized word embeddings?\n",
    "\n",
    "A) Word2Vec\n",
    "\n",
    "B) GloVe\n",
    "\n",
    "C) BERT\n",
    "\n",
    "D) Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905782f",
   "metadata": {},
   "source": [
    "#### Question 3 (Short Answer)\n",
    "\n",
    "Explain the difference between the CBOW and Skip-Gram architectures in Word2Vec. Which one is generally better for representing rare words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec4e72",
   "metadata": {},
   "source": [
    "#### Question 4 (Short Answer)\n",
    "\n",
    "Why is one-hot encoding not ideal for representing words in NLP tasks? Mention at least two reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc455a",
   "metadata": {},
   "source": [
    "#### Question 5 (Problem-Solving)\n",
    "\n",
    "You are given the following word vectors: `v_apple`, `v_fruit`, `v_car`, `v_vehicle`. How would you expect the **cosine similarity** (a measure of how similar vectors are) to compare between the pair `(v_apple, v_fruit)` and the pair `(v_apple, v_car)`? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48581317",
   "metadata": {},
   "source": [
    "#### Question 6 (Case Study)\n",
    "\n",
    "A company wants to build a sentiment analysis model for customer reviews of their new electronic product. They have a relatively small dataset of 5,000 reviews. Would you recommend they train their own word embeddings from scratch or use pre-trained embeddings like GloVe? Explain the trade-offs of each approach in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9a4c1",
   "metadata": {},
   "source": [
    "### üéâ Congratulations!\n",
    "\n",
    "You have completed the introduction to word embeddings. This is a foundational skill in NLP and will help you understand many advanced AI models. Keep experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
