{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49488fcc",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Introduction to Gensim and Custom Word Embeddings\n",
    "\n",
    "Welcome to this 2-hour interactive session on Natural Language Processing (NLP)! Today, we'll dive into the fascinating world of word embeddings using one of the most popular Python libraries: **Gensim**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79b1d4",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1.  Understand what Gensim and Word Embeddings are.\n",
    "2.  Recognize why custom-trained embeddings are powerful.\n",
    "3.  Prepare a text corpus for model training.\n",
    "4.  Train your own `Word2Vec` model from scratch.\n",
    "5.  Train your own `FastText` model and understand its advantages.\n",
    "6.  Explore the results to find word similarities and solve analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6d19c",
   "metadata": {},
   "source": [
    "### What is Gensim?\n",
    "\n",
    "**Gensim** (which stands for \"Generate Similar\") is a fantastic open-source Python library for NLP. It's famous for being highly efficient, especially with large text collections, because it can process text in a streaming fashion (meaning it doesn't need to load everything into memory at once!).\n",
    "\n",
    "### What are Word Embeddings?\n",
    "\n",
    "Imagine you could represent words as numbers in a way that captures their meaning and relationships. That's exactly what **word embeddings** do! They are dense vector representations of words in a multi-dimensional space. The core idea is simple: **words that appear in similar contexts tend to have similar meanings.** For example, the vectors for \"king\" and \"queen\" would be much closer to each other than the vectors for \"king\" and \"apple\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfc2ae",
   "metadata": {},
   "source": [
    "## Topic 1: Corpus Preparation\n",
    "\n",
    "The quality of our word embeddings depends heavily on the quality of our input text, or **corpus**. Gensim models expect the corpus in a specific format: a **list of lists of strings**, where each inner list represents a tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fce5830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'the', 'first', 'sentence'], ['this', 'document', 'is', 'the', 'second', 'sentence'], ['and', 'this', 'is', 'the', 'third', 'one'], ['is', 'this', 'the', 'first', 'document']]\n"
     ]
    }
   ],
   "source": [
    "# This is the format Gensim expects:\n",
    "# A list, where each item is another list containing the words of a sentence.\n",
    "\n",
    "corpus = [\n",
    "    ['this', 'is', 'the', 'first', 'sentence'],\n",
    "    ['this', 'document', 'is', 'the', 'second', 'sentence'],\n",
    "    ['and', 'this', 'is', 'the', 'third', 'one'],\n",
    "    ['is', 'this', 'the', 'first', 'document']\n",
    "]\n",
    "\n",
    "# Let's print it to see!\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d6b30",
   "metadata": {},
   "source": [
    "To get our text into this format, we usually perform a few preprocessing steps:\n",
    "1.  **Tokenization**: Splitting sentences into individual words (tokens).\n",
    "2.  **Lowercasing**: Converting all text to lowercase (e.g., treating \"Apple\" and \"apple\" as the same).\n",
    "3.  **Removing Punctuation/Numbers**: Getting rid of characters that don't carry semantic meaning.\n",
    "\n",
    "Let's prepare a small sample corpus for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1166c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is ready!\n"
     ]
    }
   ],
   "source": [
    "# Our sample corpus for today's session\n",
    "# Notice it's already tokenized and lowercased for us!\n",
    "sentences = [\n",
    "    ['the', 'king', 'is', 'a', 'strong', 'ruler'],\n",
    "    ['the', 'queen', 'is', 'a', 'wise', 'leader'],\n",
    "    ['the', 'prince', 'is', 'a', 'young', 'man'],\n",
    "    ['the', 'princess', 'is', 'a', 'young', 'woman'],\n",
    "    ['a', 'man', 'can', 'be', 'a', 'king'],\n",
    "    ['a', 'woman', 'can', 'be', 'a', 'queen'],\n",
    "    ['royalty', 'includes', 'the', 'king', 'and', 'queen']\n",
    "]\n",
    "\n",
    "print(\"Corpus is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb5dcc",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 1: Prepare Your Own Corpus\n",
    "\n",
    "You are given a list of raw sentences. Your task is to turn them into the `list of lists` format that Gensim needs. You'll need to lowercase each sentence and then split it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = [\n",
    "    \"The weather is sunny and bright.\",\n",
    "    \"I love a bright sunny day.\"\n",
    "]\n",
    "\n",
    "processed_corpus = []\n",
    "# Your code here! Loop through raw_sentences.\n",
    "# For each sentence, convert it to lowercase and split it into words.\n",
    "# Then, append the list of words to processed_corpus.\n",
    "\n",
    "# for sentence in raw_sentences:\n",
    "#     lower_sentence = ...\n",
    "#     words = ...\n",
    "#     processed_corpus.append(words)\n",
    "\n",
    "# print(processed_corpus)\n",
    "# Expected output: [['the', 'weather', 'is', 'sunny', 'and', 'bright.'], ['i', 'love', 'a', 'bright', 'sunny', 'day.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa314b8",
   "metadata": {},
   "source": [
    "## Topic 2: Training a Word2Vec Model\n",
    "\n",
    "`Word2Vec` is a famous model developed at Google that learns word embeddings. It has two main architectures:\n",
    "\n",
    "-   **CBOW (Continuous Bag of Words)**: Predicts a target word from its context words. It's fast and works well for frequent words.\n",
    "-   **Skip-Gram**: Predicts the context words from a target word. It's slower but great for rare words and often produces higher-quality embeddings.\n",
    "\n",
    "Let's train our first `Word2Vec` model using the **Skip-Gram** architecture (`sg=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44312637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:32:42,794 : INFO : collecting all words and their counts\n",
      "2025-11-13 11:32:42,796 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-11-13 11:32:42,797 : INFO : collected 19 word types from a corpus of 42 raw words and 7 sentences\n",
      "2025-11-13 11:32:42,799 : INFO : Creating a fresh vocabulary\n",
      "2025-11-13 11:32:42,801 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 19 unique words (100.00% of original 19, drops 0)', 'datetime': '2025-11-13T11:32:42.801188', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-11-13 11:32:42,802 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 42 word corpus (100.00% of original 42, drops 0)', 'datetime': '2025-11-13T11:32:42.802197', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-11-13 11:32:42,804 : INFO : deleting the raw counts dictionary of 19 items\n",
      "2025-11-13 11:32:42,808 : INFO : sample=0.001 downsamples 19 most-common words\n",
      "2025-11-13 11:32:42,810 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 6.249309056445171 word corpus (14.9%% of prior 42)', 'datetime': '2025-11-13T11:32:42.810180', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-11-13 11:32:42,814 : INFO : estimated required memory for 19 words and 100 dimensions: 24700 bytes\n",
      "2025-11-13 11:32:42,816 : INFO : resetting layer weights\n",
      "2025-11-13 11:32:42,821 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-11-13T11:32:42.821414', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-11-13 11:32:42,823 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 19 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-11-13T11:32:42.823415', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-11-13 11:32:42,835 : INFO : EPOCH 0: training on 42 raw words (7 effective words) took 0.0s, 1274 effective words/s\n",
      "2025-11-13 11:32:42,842 : INFO : EPOCH 1: training on 42 raw words (5 effective words) took 0.0s, 5911 effective words/s\n",
      "2025-11-13 11:32:42,847 : INFO : EPOCH 2: training on 42 raw words (6 effective words) took 0.0s, 6365 effective words/s\n",
      "2025-11-13 11:32:42,853 : INFO : EPOCH 3: training on 42 raw words (3 effective words) took 0.0s, 3576 effective words/s\n",
      "2025-11-13 11:32:42,859 : INFO : EPOCH 4: training on 42 raw words (4 effective words) took 0.0s, 4697 effective words/s\n",
      "2025-11-13 11:32:42,860 : INFO : Word2Vec lifecycle event {'msg': 'training on 210 raw words (25 effective words) took 0.0s, 692 effective words/s', 'datetime': '2025-11-13T11:32:42.860414', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-11-13 11:32:42,861 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=19, vector_size=100, alpha=0.025>', 'datetime': '2025-11-13T11:32:42.861415', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Word2Vec Model ---\n",
      "\n",
      "âœ… Word2Vec model training complete!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "# This helps us see the training progress in the notebook output\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "print(\"--- Training Word2Vec Model ---\")\n",
    "\n",
    "# Let's define the model with its key parameters\n",
    "word2vec_model = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # Our prepared corpus\n",
    "    vector_size=100,      # Dimensionality of the word vectors (how many numbers per word)\n",
    "    window=5,             # Max distance between current and predicted word within a sentence\n",
    "    min_count=1,          # Ignores all words with total frequency lower than this\n",
    "    sg=1,                 # Training algorithm: 1 for Skip-Gram; 0 for CBOW\n",
    "    workers=4             # Use 4 CPU threads to speed up training\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Word2Vec model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4388c",
   "metadata": {},
   "source": [
    "ðŸ’¡ **A Note on Parameters**\n",
    "-   `vector_size`: A common range is 100-300. More dimensions can capture more info but need more data.\n",
    "-   `window`: A larger window considers more context.\n",
    "-   `min_count`: Helps filter out rare words or typos.\n",
    "-   `sg=1`: We chose Skip-Gram. Try changing it to `sg=0` later to see how it affects the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf050fe4",
   "metadata": {},
   "source": [
    "## Topic 3: Exploring the Word2Vec Model\n",
    "\n",
    "Now for the fun part! Let's see what our model has learned. We can access all the word vectors and query them through the `model.wv` object (which stands for 'word vectors')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b975e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'king': [('wise', 0.2528882920742035), ('is', 0.17026387155056), ('and', 0.15015792846679688)]\n"
     ]
    }
   ],
   "source": [
    "# Get the KeyedVectors instance from our trained model\n",
    "wv = word2vec_model.wv\n",
    "\n",
    "# Let's find the words most similar to 'king'\n",
    "# topn=3 means we want the top 3 results\n",
    "print(\"Most similar to 'king':\", wv.most_similar('king', topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca63358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': -0.0445\n"
     ]
    }
   ],
   "source": [
    "# We can also check the cosine similarity between two words.\n",
    "# A value closer to 1.0 means they are very similar.\n",
    "similarity_score = wv.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da5fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy 'king - man + woman': [('princess', 0.22007820010185242)]\n"
     ]
    }
   ],
   "source": [
    "# Now for the classic word analogy: king - man + woman = ?\n",
    "# The model should predict 'queen'!\n",
    "# `positive` words are added, `negative` words are subtracted.\n",
    "analogy_result = wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print(\"Analogy 'king - man + woman':\", analogy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a624401",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 2: Explore Relationships\n",
    "\n",
    "Now it's your turn! Use the `wv` object to:\n",
    "1.  Find the top 3 most similar words to `'woman'`.\n",
    "2.  Calculate the similarity between `'prince'` and `'princess'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7489ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the top 3 most similar words to 'woman'\n",
    "# print(\"Most similar to 'woman':\", ...)\n",
    "\n",
    "# 2. Calculate the similarity between 'prince' and 'princess'\n",
    "# print(\"Similarity between 'prince' and 'princess':\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74784c8b",
   "metadata": {},
   "source": [
    "## Topic 4: Training a FastText Model\n",
    "\n",
    "`FastText`, developed by Facebook AI Research, is a powerful extension of `Word2Vec`. Its key innovation is that it learns vectors for **character n-grams** (sub-parts of words) instead of just whole words.\n",
    "\n",
    "For example, the word `apple` (with n=3) is broken down into `<ap, app, ppl, ple, le>`. The final vector for `apple` is the sum of these n-gram vectors.\n",
    "\n",
    "This gives it a superpower: **it can create vectors for words it has never seen before (Out-of-Vocabulary or OOV words)!** This is extremely useful for real-world text which often contains typos, slang, or new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034feed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:39:18,498 : INFO : collecting all words and their counts\n",
      "2025-11-13 11:39:18,501 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-11-13 11:39:18,503 : INFO : collected 19 word types from a corpus of 42 raw words and 7 sentences\n",
      "2025-11-13 11:39:18,504 : INFO : Creating a fresh vocabulary\n",
      "2025-11-13 11:39:18,507 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 19 unique words (100.00% of original 19, drops 0)', 'datetime': '2025-11-13T11:39:18.507165', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-11-13 11:39:18,510 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 42 word corpus (100.00% of original 42, drops 0)', 'datetime': '2025-11-13T11:39:18.510226', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-11-13 11:39:18,513 : INFO : deleting the raw counts dictionary of 19 items\n",
      "2025-11-13 11:39:18,516 : INFO : sample=0.001 downsamples 19 most-common words\n",
      "2025-11-13 11:39:18,518 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 6.249309056445171 word corpus (14.9%% of prior 42)', 'datetime': '2025-11-13T11:39:18.518216', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-11-13 11:39:18,522 : INFO : estimated required memory for 19 words, 2000000 buckets and 100 dimensions: 800027604 bytes\n",
      "2025-11-13 11:39:18,524 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training FastText Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:39:20,608 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-11-13T11:39:20.608505', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-11-13 11:39:20,612 : INFO : FastText lifecycle event {'msg': 'training model with 4 workers on 19 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-11-13T11:39:20.612440', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-11-13 11:39:20,628 : INFO : EPOCH 0: training on 42 raw words (7 effective words) took 0.0s, 663 effective words/s\n",
      "2025-11-13 11:39:20,636 : INFO : EPOCH 1: training on 42 raw words (5 effective words) took 0.0s, 3862 effective words/s\n",
      "2025-11-13 11:39:20,648 : INFO : EPOCH 2: training on 42 raw words (6 effective words) took 0.0s, 6252 effective words/s\n",
      "2025-11-13 11:39:20,653 : INFO : EPOCH 3: training on 42 raw words (3 effective words) took 0.0s, 3705 effective words/s\n",
      "2025-11-13 11:39:20,658 : INFO : EPOCH 4: training on 42 raw words (4 effective words) took 0.0s, 3970 effective words/s\n",
      "2025-11-13 11:39:20,660 : INFO : FastText lifecycle event {'msg': 'training on 210 raw words (25 effective words) took 0.0s, 535 effective words/s', 'datetime': '2025-11-13T11:39:20.660865', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-11-13 11:39:20,663 : INFO : FastText lifecycle event {'params': 'FastText<vocab=19, vector_size=100, alpha=0.025>', 'datetime': '2025-11-13T11:39:20.663864', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… FastText model training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training FastText Model ---\")\n",
    "\n",
    "# The parameters are very similar to Word2Vec\n",
    "fasttext_model = gensim.models.FastText(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    min_n=3,          # Minimum length of char n-grams\n",
    "    max_n=6           # Maximum length of char n-grams\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… FastText model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ed062",
   "metadata": {},
   "source": [
    "## Topic 5: Handling OOV Words with FastText\n",
    "\n",
    "Let's test FastText's superpower. The word `'royal'` does not appear in our training sentences, but the word `'royalty'` does. Because they share n-grams (like `roy`), FastText can generate a meaningful vector for `'royal'`. Word2Vec cannot do this and will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b418ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText can create a vector for the OOV word 'royal'!\n",
      "[-1.5122148e-03  1.1532352e-03  1.9209286e-03 -8.8359398e-04\n",
      "  3.3625741e-03 -2.1618661e-03  7.6207158e-04  4.1295314e-04\n",
      " -2.7965769e-04 -1.8673305e-03  1.7786868e-03 -2.0222842e-04\n",
      " -9.1285910e-04  6.8314903e-04  8.3535007e-04 -8.3769549e-04\n",
      " -2.3814852e-03 -2.2419088e-03  1.6209234e-03  5.5180426e-04\n",
      " -8.8045344e-04 -7.6456659e-04  8.1196550e-04 -2.7520659e-03\n",
      "  5.6462327e-04  1.6299620e-03 -1.3160080e-03  2.1835254e-03\n",
      " -2.8695953e-03 -1.7305206e-03  8.6225721e-04 -6.0667080e-04\n",
      "  6.9958577e-04 -2.3803189e-03 -4.8611444e-04 -6.2479317e-04\n",
      " -1.3061651e-03 -5.1116204e-04 -6.6606834e-04 -1.0390515e-03\n",
      "  2.5509144e-03  2.2024622e-03 -6.8543415e-04 -5.3005147e-04\n",
      "  2.5311424e-03 -4.4328329e-04 -1.7440926e-03  3.3177424e-03\n",
      "  2.2625123e-04  2.1600495e-03 -5.1944656e-04 -9.9181675e-04\n",
      " -2.5285182e-03 -6.9755915e-04 -6.9205300e-05  3.9778976e-04\n",
      " -4.1564279e-05  7.9426903e-04  2.7825802e-03 -1.1663277e-03\n",
      "  5.5489287e-04 -1.1981005e-03  8.3259883e-04 -1.7565956e-04\n",
      " -9.1353664e-05  5.8851397e-04 -9.2287426e-04  1.0495463e-03\n",
      "  1.2258055e-03  3.5179380e-04 -8.5618120e-04  1.2829601e-03\n",
      " -1.0104671e-03 -8.1817350e-05 -6.0882571e-04  1.7712923e-03\n",
      " -1.5702670e-03  1.1802167e-03  3.0190352e-04  5.4068159e-04\n",
      "  1.5402995e-03  1.1098356e-03  1.8761574e-03  4.8752688e-04\n",
      " -1.5788668e-04  4.5856371e-04  5.6858553e-04 -8.1350462e-04\n",
      "  6.2456279e-04 -1.9930403e-03 -4.2106514e-03 -1.2396014e-03\n",
      " -4.2538797e-03 -2.1471893e-03 -8.4582408e-04 -4.3293581e-04\n",
      " -1.3808130e-04  1.4130465e-03  2.6687763e-03  1.0037293e-03]\n",
      "\n",
      "--- Trying to access OOV word 'royal' with Word2Vec ---\n",
      "Word2Vec error as expected: \"Key 'royal' not present\"\n"
     ]
    }
   ],
   "source": [
    "# Get the KeyedVectors for the FastText model\n",
    "ft_wv = fasttext_model.wv\n",
    "\n",
    "# Demonstrate handling of an Out-of-Vocabulary (OOV) word\n",
    "print(\"FastText can create a vector for the OOV word 'royal'!\")\n",
    "print(ft_wv['royal'])\n",
    "\n",
    "# Now, let's see what happens when we try the same with Word2Vec\n",
    "print(\"\\n--- Trying to access OOV word 'royal' with Word2Vec ---\")\n",
    "try:\n",
    "    print(wv['royal'])\n",
    "except KeyError as e:\n",
    "    print(\"Word2Vec error as expected:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6c18c",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 3: Test Another OOV Word\n",
    "\n",
    "Try to get a vector for another OOV word, such as `'kingdom'`. Does FastText handle it? What about Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940aed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Try to access the vector for 'kingdom' using ft_wv (FastText)\n",
    "\n",
    "# print(ft_wv['kingdom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9477540",
   "metadata": {},
   "source": [
    "##  Final Revision Assignment\n",
    "\n",
    "Congratulations on making it to the end of the session! Now it's time to combine everything you've learned. We will use a new corpus of customer reviews to build and test our models.\n",
    "\n",
    "**Your goal:** Analyze customer reviews to find similar concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77025f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = [\n",
    "    \"The customer service was excellent and friendly.\",\n",
    "    \"I was not happy with the product quality.\",\n",
    "    \"The delivery was slow but the service was helpful.\",\n",
    "    \"Excellent product and quick delivery.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fae11",
   "metadata": {},
   "source": [
    "### Task 1: Preprocess the Reviews\n",
    "\n",
    "Complete the code below to tokenize and lowercase the `reviews_raw` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687516df",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_corpus = []\n",
    "for review in reviews_raw:\n",
    "    # 1. Lowercase the review\n",
    "    # 2. Split it into words (tokens)\n",
    "    # 3. Append the list of words to reviews_corpus\n",
    "    pass\n",
    "\n",
    "# print(reviews_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7afea",
   "metadata": {},
   "source": [
    "### Task 2: Train a Word2Vec Model\n",
    "\n",
    "Train a `Word2Vec` model on the `reviews_corpus`.\n",
    "-   `vector_size` = 50\n",
    "-   `window` = 3\n",
    "-   `sg` = 1 (Skip-Gram)\n",
    "-   `min_count` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aacae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to train a Word2Vec model\n",
    "# review_w2v_model = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53e299",
   "metadata": {},
   "source": [
    "### Task 3: Find Similar Words\n",
    "\n",
    "Using your new Word2Vec model, find the top 3 words most similar to `\"service\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ae977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# review_wv = review_w2v_model.wv\n",
    "# print(review_wv.most_similar('service', topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bf879",
   "metadata": {},
   "source": [
    "### Task 4: Train a FastText Model\n",
    "\n",
    "Now, train a `FastText` model on the same `reviews_corpus`. Use the same parameters as the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to train a FastText model\n",
    "# review_ft_model = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217b63d",
   "metadata": {},
   "source": [
    "### Task 5: Compare OOV Handling\n",
    "\n",
    "The word `\"friendliness\"` is not in our corpus, but `\"friendly\"` is. Try to get the vector for `\"friendliness\"` using both your Word2Vec and FastText models. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d05195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors from the FastText model\n",
    "# review_ft_wv = review_ft_model.wv\n",
    "# print(\"FastText vector for 'friendliness':\", review_ft_wv['friendliness'])\n",
    "\n",
    "# Try to get the vectors from the Word2Vec model\n",
    "# try:\n",
    "#     print(review_wv['friendliness'])\n",
    "# except KeyError as e:\n",
    "#     print(\"Word2Vec error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48a49f",
   "metadata": {},
   "source": [
    "### Task 6: Conceptual Question\n",
    "\n",
    "A hospital wants to build a search engine for its patient discharge summaries, which contain a lot of specific medical terms. Would you recommend using a generic pre-trained model (like one from Google News) or training a custom **FastText** model on the hospital's data?\n",
    "\n",
    "**Double-click here to write your answer:**\n",
    "\n",
    "*Your answer here. Justify your choice!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9440617",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ You've completed the session! \n",
    "\n",
    "Well done! You now have hands-on experience training and using custom word embedding models with Gensim. This is a fundamental skill in modern NLP and opens the door to building powerful, domain-aware applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
