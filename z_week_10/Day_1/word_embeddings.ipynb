{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d34d26",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Introduction to Word Embeddings with Word2vec\n",
    "\n",
    "Welcome to your 2-hour journey into the fascinating world of Natural Language Processing (NLP)! Today, we'll explore one of the most important concepts that revolutionized how machines understand human language: **Word Embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f10bb8",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1.  **Understand** what word embeddings are and why they are useful.\n",
    "2.  **Explain** the core idea behind Word2vec and its two main architectures: CBOW and Skip-gram.\n",
    "3.  **See** how word embeddings capture word meanings and relationships.\n",
    "4.  **Apply** these concepts through simple, hands-on practice tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3061ad",
   "metadata": {},
   "source": [
    "## Topic 1: What are Word Embeddings?\n",
    "\n",
    "Imagine you had to represent every word in the dictionary as a number. An easy way would be to assign a unique ID to each word. This is called **One-Hot Encoding**.\n",
    "\n",
    "However, this method has two big problems:\n",
    "1.  **It's Inefficient:** If you have 10,000 words, each word's representation is a list with 9,999 zeros and only one '1'. That's a lot of wasted space!\n",
    "2.  **It's Not Smart:** The vectors for \"cat\" and \"dog\" are no more related than the vectors for \"cat\" and \"car\". It doesn't capture any meaning.\n",
    "\n",
    "**Word Embeddings solve this!** They represent words as dense, multi-dimensional vectors (like `[0.2, -0.4, 0.7, ...]`). The magic is that words with similar meanings are placed close to each other in this vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ab3c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary is: ['cat', 'dog', 'car', 'house']\n",
      "\n",
      "'cat' vector: [1, 0, 0, 0]\n",
      "'dog' vector: [0, 1, 0, 0]\n",
      "'car' vector: [0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example: Let's visualize the problem with One-Hot Encoding\n",
    "\n",
    "# Our small vocabulary\n",
    "vocabulary = [\"cat\", \"dog\", \"car\", \"house\"]\n",
    "print(f\"Our vocabulary is: {vocabulary}\\n\")\n",
    "\n",
    "# A simple function to create a one-hot vector\n",
    "def one_hot_encode(word, vocab):\n",
    "    # Create a vector of zeros with the length of the vocabulary\n",
    "    vector = [0] * len(vocab)\n",
    "    # Find the index of our word\n",
    "    try:\n",
    "        index = vocab.index(word)\n",
    "        # Place a '1' at that index\n",
    "        vector[index] = 1\n",
    "        return vector\n",
    "    except ValueError:\n",
    "        return \"Word not in vocabulary\"\n",
    "\n",
    "# Let's see the vectors\n",
    "cat_vector = one_hot_encode(\"cat\", vocabulary)\n",
    "dog_vector = one_hot_encode(\"dog\", vocabulary)\n",
    "car_vector = one_hot_encode(\"car\", vocabulary)\n",
    "\n",
    "print(f\"'cat' vector: {cat_vector}\")\n",
    "print(f\"'dog' vector: {dog_vector}\")\n",
    "print(f\"'car' vector: {car_vector}\")\n",
    "\n",
    "# Notice how the vectors don't show any relationship between 'cat' and 'dog'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d8b3b",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 1\n",
    "\n",
    "Using the `one_hot_encode` function from the cell above, create a one-hot vector for the word `\"house\"` from our vocabulary. What do you expect the output to be? Write your code in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e542cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'house' vector: [0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "vocabulary = [\"cat\", \"dog\", \"car\", \"house\"]\n",
    "\n",
    "def one_hot_encode(word, vocab):\n",
    "    vector = [0] * len(vocab)\n",
    "    try:\n",
    "        index = vocab.index(word)\n",
    "        vector[index] = 1\n",
    "        return vector\n",
    "    except ValueError:\n",
    "        return \"Word not in vocabulary\"\n",
    "\n",
    "# Create the vector for 'house'\n",
    "house_vector = one_hot_encode(\"house\", vocabulary) # Your turn to complete this line!\n",
    "print(f\"'house' vector: {house_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b107b4",
   "metadata": {},
   "source": [
    "## Topic 2: Introducing Word2vec ðŸ’¡\n",
    "\n",
    "**Word2vec** is a powerful model developed at Google that learns to create these amazing word embeddings. Its foundation is a simple but profound idea from linguistics called the **Distributional Hypothesis**: \n",
    "\n",
    "> \"A word is characterized by the company it keeps.\"\n",
    "\n",
    "In simple terms, Word2vec looks at tons of text (a **corpus**) and learns that words appearing in similar contexts (e.g., around the same words) should have similar meanings. For example, it will see sentences like \"My dog loves to play fetch\" and \"The cat is sleeping on the mat\". It will notice that 'dog' and 'cat' often appear near words like 'pet', 'food', and 'play', so it will place their vectors close together.\n",
    "\n",
    "### The Famous Analogy: King - Man + Woman = Queen\n",
    "\n",
    "The vectors created by Word2vec are so powerful they can even capture relationships. The most famous example is:\n",
    "\n",
    "`vector('King') - vector('Man') + vector('Woman')` results in a vector that is very close to `vector('Queen')`!\n",
    "\n",
    "This shows the model learned the concept of gender and royalty just by reading text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7dbbe",
   "metadata": {},
   "source": [
    "## Topic 3: The Two Flavors of Word2vec - CBOW & Skip-gram\n",
    "\n",
    "Word2vec isn't one single algorithm; it's a family of two model architectures. Let's imagine we have the sentence: `\"The quick brown fox jumps over the lazy dog\"` and we're looking at the word `\"fox\"` with a **window size** of 2. This means we consider 2 words before and 2 words after our target word.\n",
    "\n",
    "**Target Word:** `fox`\n",
    "**Context Words:** `quick`, `brown` (before), `jumps`, `over` (after)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a59089",
   "metadata": {},
   "source": [
    "### ðŸ“„ 1. Continuous Bag-of-Words (CBOW)\n",
    "\n",
    "The CBOW model learns by doing the following:\n",
    "- **Goal:** Predict the target word from its context words.\n",
    "- **Analogy:** It's like a fill-in-the-blanks puzzle.\n",
    "\n",
    "**How it works for our example:**\n",
    "1.  **Input:** The context words `[quick, brown, jumps, over]`.\n",
    "2.  **Task:** Predict the word `_____` in the middle.\n",
    "3.  **Output:** The model should predict `fox`.\n",
    "\n",
    "During training, the model adjusts its vectors so it gets better at this prediction. CBOW is fast and works well for words that appear often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe632dd",
   "metadata": {},
   "source": [
    "### ðŸ“„ 2. Continuous Skip-gram\n",
    "\n",
    "The Skip-gram model does the exact opposite:\n",
    "- **Goal:** Predict the context words from the target word.\n",
    "- **Analogy:** Given one word, guess its neighbors.\n",
    "\n",
    "**How it works for our example:**\n",
    "1.  **Input:** The target word `fox`.\n",
    "2.  **Task:** Predict the words that are likely to be its neighbors.\n",
    "3.  **Output:** The model should predict `quick`, `brown`, `jumps`, and `over`.\n",
    "\n",
    "Skip-gram is slower than CBOW but is excellent at learning good representations for rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899d5c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'The quick brown fox jumps over the lazy dog'\n",
      "Target Word: 'fox' with window size 2\n",
      "\n",
      "Generated Skip-gram pairs (input, output):\n",
      "('fox', 'quick')\n",
      "('fox', 'brown')\n",
      "('fox', 'jumps')\n",
      "('fox', 'over')\n",
      "\n",
      "ðŸ§ª Try changing the target word or the window size and see what happens!\n"
     ]
    }
   ],
   "source": [
    "# Example: Generating training samples for Skip-gram\n",
    "# Let's write a simple function to see what the training data looks like.\n",
    "\n",
    "def generate_skipgram_pairs(sentence, target_word, window_size):\n",
    "    words = sentence.split()\n",
    "    target_index = words.index(target_word)\n",
    "    \n",
    "    pairs = []\n",
    "    # Iterate through the window around the target word\n",
    "    for i in range(max(0, target_index - window_size), min(len(words), target_index + window_size + 1)):\n",
    "        # Make sure we don't pair the word with itself\n",
    "        if i != target_index:\n",
    "            context_word = words[i]\n",
    "            pairs.append((target_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "target = \"fox\"\n",
    "window = 2\n",
    "\n",
    "training_pairs = generate_skipgram_pairs(sentence, target, window)\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Target Word: '{target}' with window size {window}\\n\")\n",
    "print(f\"Generated Skip-gram pairs (input, output):\")\n",
    "for pair in training_pairs:\n",
    "    print(pair)\n",
    "\n",
    "print(\"\\nðŸ§ª Try changing the target word or the window size and see what happens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71de76",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 2\n",
    "\n",
    "You are given the sentence `\"Natural language processing is fun\"` and a window size of 1. \n",
    "\n",
    "What would be the training samples generated for the **Skip-gram model** with the target word `\"processing\"`? \n",
    "\n",
    "Use the code cell below to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcb777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated pairs are: [('processing', 'language'), ('processing', 'is')]\n"
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "def generate_skipgram_pairs(sentence, target_word, window_size):\n",
    "    words = sentence.split()\n",
    "    target_index = words.index(target_word)\n",
    "    pairs = []\n",
    "    for i in range(max(0, target_index - window_size), min(len(words), target_index + window_size + 1)):\n",
    "        if i != target_index:\n",
    "            context_word = words[i]\n",
    "            pairs.append((target_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "# Define your new sentence and parameters\n",
    "my_sentence = \"Natural language processing is fun\"\n",
    "my_target = \"processing\"\n",
    "my_window = 1\n",
    "\n",
    "# Generate the pairs\n",
    "my_pairs = generate_skipgram_pairs(my_sentence, my_target, my_window)\n",
    "print(f\"The generated pairs are: {my_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d8b02",
   "metadata": {},
   "source": [
    "âœ… **Well done!** You've just seen how Word2vec models turn sentences into training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99d706",
   "metadata": {},
   "source": [
    "## Topic 4: Measuring Similarity\n",
    "\n",
    "How do we know if two word vectors are \"close\" to each other in the vector space? We use a metric called **Cosine Similarity**.\n",
    "\n",
    "Imagine two vectors as arrows starting from the same point. \n",
    "- If the arrows point in the exact same direction, their similarity is **1**.\n",
    "- If they are perpendicular (90 degrees apart), their similarity is **0**.\n",
    "- If they point in opposite directions, their similarity is **-1**.\n",
    "\n",
    "Cosine similarity measures the angle between the vectors, not their length. This is perfect for word embeddings, as it tells us about orientation (meaning) rather than magnitude (which can be related to word frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65f7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cat' and 'dog': 0.9926\n",
      "Similarity between 'cat' and 'car': 0.4880\n"
     ]
    }
   ],
   "source": [
    "# Example: Calculating Cosine Similarity\n",
    "# We'll use the numpy library for this. It's a fundamental library for numerical operations in Python.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Let's create some simple, fake word vectors\n",
    "cat_vec = np.array([1, 2, 3])\n",
    "dog_vec = np.array([2, 3, 4])  # Should be similar to cat\n",
    "car_vec = np.array([-1, -2, 5]) # Should be different from cat\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity_cat_dog = cosine_similarity(cat_vec, dog_vec)\n",
    "similarity_cat_car = cosine_similarity(cat_vec, car_vec)\n",
    "\n",
    "print(f\"Similarity between 'cat' and 'dog': {similarity_cat_dog:.4f}\") # The .4f formats the number nicely\n",
    "print(f\"Similarity between 'cat' and 'car': {similarity_cat_car:.4f}\")\n",
    "\n",
    "# As expected, the similarity score for cat/dog is much higher (closer to 1)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4864cac",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 3\n",
    "\n",
    "You have three new word vectors:\n",
    "- `fruit` = `[3, 4, 0]`\n",
    "- `apple` = `[4, 5, 1]`\n",
    "- `book` = `[-2, 1, 5]`\n",
    "\n",
    "Which pair do you think will be more similar: (`fruit`, `apple`) or (`fruit`, `book`)?\n",
    "\n",
    "Use the code cell below to define these vectors and calculate their similarities to check your hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361bfc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'fruit' and 'apple': 0.9875\n",
      "Similarity between 'fruit' and 'book': -0.0730\n"
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Define the vectors\n",
    "fruit_vec = np.array([3, 4, 0])\n",
    "apple_vec = np.array([4, 5, 1])\n",
    "book_vec = np.array([-2, 1, 5])\n",
    "\n",
    "# Calculate similarity for the first pair\n",
    "sim_fruit_apple = cosine_similarity(fruit_vec, apple_vec)\n",
    "\n",
    "# Calculate similarity for the second pair\n",
    "sim_fruit_book = cosine_similarity(fruit_vec, book_vec)\n",
    "\n",
    "print(f\"Similarity between 'fruit' and 'apple': {sim_fruit_apple:.4f}\")\n",
    "print(f\"Similarity between 'fruit' and 'book': {sim_fruit_book:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfe165",
   "metadata": {},
   "source": [
    "##  Final Revision Assignment\n",
    "\n",
    "Congratulations on making it this far! It's time to put everything you've learned together. These tasks are designed for you to practice at home and solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec9895",
   "metadata": {},
   "source": [
    "--- \n",
    "**Task 1 (MCQ):** Which statement best describes the primary objective of the Continuous Bag-of-Words (CBOW) model?\n",
    "\n",
    "A) To predict the context words given a target word.\n",
    "B) To predict a target word from a bag of its context words.\n",
    "C) To count the co-occurrence of words in a corpus.\n",
    "D) To represent words as character n-grams.\n",
    "\n",
    "*Hint: Think \"fill-in-the-blanks\".*\n",
    "\n",
    "---\n",
    "**Task 2 (MCQ):** In the context of Word2vec, what is the main advantage of the Skip-gram architecture over CBOW?\n",
    "\n",
    "A) It is computationally faster to train.\n",
    "B) It handles rare words more effectively.\n",
    "C) It performs better for representing frequent words.\n",
    "D) It uses a global co-occurrence matrix.\n",
    "\n",
    "*Hint: Which model pays more attention to each specific word?*\n",
    "\n",
    "---\n",
    "**Task 3 (Short Question):** In your own words, briefly explain why an optimization like **Negative Sampling** is needed. Why can't we just use the standard approach for a large vocabulary?\n",
    "\n",
    "*Hint: Think about how many words are in a real-world dictionary.*\n",
    "\n",
    "---\n",
    "**Task 4 (Problem-Solving):** The vector equation `vector('King') - vector('Man') + vector('Woman') â‰ˆ vector('Queen')` is a powerful demonstration of what Word2vec learns. What kind of relationship is being captured here? Can you think of another example, like `Paris - France + Germany`? What would you expect the result to be?\n",
    "\n",
    "---\n",
    "**Task 5 (Case Study):** A startup wants to build a recommendation engine for news articles. They need to represent articles in a way that allows them to find similar articles. How could they use word embeddings for this task? Would you recommend CBOW or Skip-gram, and why?\n",
    "\n",
    "*Hint: Think about the goal. Do they need speed for massive amounts of text, or do they care more about capturing the meaning of specific, important (and possibly rare) keywords in the articles?*\n",
    "\n",
    "---\n",
    "**Task 6 (Code Challenge):** Look back at our `cosine_similarity` function. Imagine you have real word vectors for `python` (a programming language), `java` (another language), and `snake` (an animal). Which pair do you think will have the highest similarity? Which will have the lowest? Write some code to prove it!\n",
    "\n",
    "```python\n",
    "# Pre-defined, simplified vectors for this exercise\n",
    "python_vec = np.array([0.8, 0.2, -0.5])\n",
    "java_vec = np.array([0.7, 0.1, -0.4])\n",
    "snake_vec = np.array([-0.1, 0.9, 0.3])\n",
    "\n",
    "# Your code here to calculate and print the similarities for:\n",
    "# 1. (python, java)\n",
    "# 2. (python, snake)\n",
    "# 3. (java, snake)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a90a74",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary & Further Learning\n",
    "\n",
    "You've done an amazing job today! Let's quickly recap the key takeaways.\n",
    "\n",
    "| Concept             | Description                                                                                             |\n",
    "|---------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| **Word Embedding**  | A dense vector representation of a word that captures its semantic and syntactic meaning.               |\n",
    "| **Word2vec**        | A predictive model that uses a shallow neural network to learn word embeddings from a large text corpus.    |\n",
    "| **CBOW**            | Predicts the target word from its context words. Fast and efficient for frequent words.                   |\n",
    "| **Skip-gram**       | Predicts context words from a target word. Slower but better for rare words.                            |\n",
    "| **Core Idea**       | Words appearing in similar contexts will have similar vector representations (Distributional Hypothesis). |\n",
    "| **Cosine Similarity**| A metric to measure the similarity (angle) between two word vectors.                                    |\n",
    "\n",
    "### ðŸ”— Related Study Resources\n",
    "\n",
    "To continue your learning journey, check out these fantastic resources:\n",
    "\n",
    "- **[The Illustrated Word2vec by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)**: An excellent and intuitive visual explanation of Word2vec's mechanics.\n",
    "- **[TensorFlow Word Embeddings Tutorial](https://www.tensorflow.org/text/guide/word_embeddings)**: A practical guide to creating and visualizing word embeddings.\n",
    "- **[Stanford's CS224N: NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)**: A comprehensive university course covering word vectors in depth.\n",
    "- **[Gensim Library Documentation](https://radimrehurek.com/gensim/models/word2vec.html)**: A popular Python library for implementing Word2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
