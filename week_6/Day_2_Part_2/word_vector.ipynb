{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce04eb29",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Introduction to Natural Language Processing (NLP) for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684930c",
   "metadata": {},
   "source": [
    "Welcome to your first 2-hour session on NLP! ðŸš€\n",
    "\n",
    "**Natural Language Processing (NLP)** is a fascinating field of AI that teaches computers how to understand and work with human language. Think of it as the bridge between how we talk and how machines process information. From Siri and Alexa to Google Translate, NLP is everywhere!\n",
    "\n",
    "In this notebook, we will transform raw text into structured data that a machine learning model can understand. \n",
    "\n",
    "### ðŸŽ¯ Learning Objectives for Today:\n",
    "\n",
    "By the end of this 2-hour session, you will be able to:\n",
    "1.  **Understand Text Structure**: Identify parts of speech (POS) and named entities (NER).\n",
    "2.  **Group Words**: Learn how to group words into meaningful phrases (Chunking).\n",
    "3.  **Normalize Text**: Reduce words to their root forms (Lemmatization).\n",
    "4.  **Explore Word Meanings**: Use WordNet to find word relationships.\n",
    "5.  **Convert Text to Numbers**: Create numerical features from text using the Bag-of-Words model.\n",
    "6.  **Clean Up Features**: Understand basic feature selection by removing common words (stopwords).\n",
    "7.  **Measure Similarity**: Calculate how similar two documents are using their numerical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3727b0",
   "metadata": {},
   "source": [
    "--- \n",
    "### **Setup: Installing Necessary Libraries**\n",
    "First, let's make sure we have the Python libraries we need. Run the cell below to install `nltk` and `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2dae14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\qasim\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\qasim\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\qasim\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup Complete! You're ready to start.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This command installs the libraries. The '!' lets us run terminal commands in Jupyter.\n",
    "!pip install nltk scikit-learn\n",
    "\n",
    "# Now, let's import them and download some required data packages from NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"âœ… Setup Complete! You're ready to start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2a4897-e5b9-4df6-ae6f-615c951a01b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd07aa",
   "metadata": {},
   "source": [
    "## Topic 1: Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e8681",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "**Part-of-Speech (POS) Tagging** is like labeling words in a sentence with their grammatical type. Is the word a noun, a verb, an adjective, or something else? \n",
    "\n",
    "This is a crucial first step because the meaning of a word can change based on its POS. For example, \"book\" can be a noun (\"I read a **book**\") or a verb (\"I need to **book** a flight\"). POS tagging helps the computer understand this context.\n",
    "\n",
    "Common tags include:\n",
    "- `NN`: Noun (e.g., *cat, building*)\n",
    "- `VB`: Verb (e.g., *run, housed*)\n",
    "- `JJ`: Adjective (e.g., *old, beautiful*)\n",
    "- `DT`: Determiner (e.g., *the, a*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6091b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('old', 'JJ'), ('building', 'NN'), ('housed', 'VBD'), ('Apple', 'NNP'), ('Inc.', 'NNP'), ('in', 'IN'), ('California', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Let's tag a sentence!\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Our example sentence\n",
    "sentence = \"The old building housed Apple Inc. in California.\"\n",
    "\n",
    "# First, we break the sentence into individual words (tokens)\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Then, we apply POS tagging to the tokens\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5eb925-f59b-4482-b652-f1372681af25",
   "metadata": {},
   "source": [
    "| Tag | Description |\n",
    "|---|---|\n",
    "| CC | Coordinating conjunction |\n",
    "| CD | Cardinal number |\n",
    "| DT | Determiner |\n",
    "| EX | Existential there |\n",
    "| FW | Foreign word |\n",
    "| IN | Preposition or subordinating conjunction |\n",
    "| JJ | Adjective |\n",
    "| JJR | Adjective, comparative |\n",
    "| JJS | Adjective, superlative |\n",
    "| LS | List item marker |\n",
    "| MD | Modal |\n",
    "| NN | Noun, singular or mass |\n",
    "| NNS | Noun, plural |\n",
    "| NNP | Proper noun, singular |\n",
    "| NNPS | Proper noun, plural |\n",
    "| PDT | Predeterminer |\n",
    "| POS | Possessive ending |\n",
    "| PRP | Personal pronoun |\n",
    "| PRP$ | Possessive pronoun |\n",
    "| RB | Adverb |\n",
    "| RBR | Adverb, comparative |\n",
    "| RBS | Adverb, superlative |\n",
    "| RP | Particle |\n",
    "| SYM | Symbol |\n",
    "| TO | to |\n",
    "| UH | Interjection |\n",
    "| VB | Verb, base form |\n",
    "| VBD | Verb, past tense |\n",
    "| VBG | Verb, gerund or present participle |\n",
    "| VBN | Verb, past participle |\n",
    "| VBP | Verb, non-3rd person singular present |\n",
    "| VBZ | Verb, 3rd person singular present |\n",
    "| WDT | Wh-determiner |\n",
    "| WP | Wh-pronoun |\n",
    "| WP$ | Possessive wh-pronoun |\n",
    "| WRB | Wh-adverb |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eda813",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 1\n",
    "\n",
    "Now it's your turn! In the code cell below, create a new sentence and use the same process to find its POS tags. Try a sentence like: `\"A fast car runs smoothly.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbb6c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('fast', 'JJ'), ('car', 'NN'), ('runs', 'VBZ'), ('smoothly', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Your sentence here\n",
    "my_sentence = \"A fast car runs smoothly.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "my_tokens = word_tokenize(my_sentence)\n",
    "\n",
    "# Get the POS tags\n",
    "my_tags = pos_tag(my_tokens)\n",
    "\n",
    "# Print your results!\n",
    "print(my_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe21e0",
   "metadata": {},
   "source": [
    "---## Topic 2: Named Entity Recognition (NER) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4b170",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "**Named Entity Recognition (NER)** is the process of finding and classifying 'named entities' in text. These are real-world objects, such as:\n",
    "- **Persons**: `Steve Jobs`\n",
    "- **Organizations**: `Apple Inc.`\n",
    "- **Locations**: `California`\n",
    "- **Dates**: `October 5, 2011`\n",
    "\n",
    "NER systems often use a tagging scheme called **IOB**: \n",
    "- **B-** (Beginning): Marks the beginning of an entity.\n",
    "- **I-** (Inside): Marks a word that is inside an entity, but not the first word.\n",
    "- **O** (Outside): Marks a word that is not part of any entity.\n",
    "\n",
    "For example, in `Apple Inc.`, `Apple` would be `B-ORG` and `Inc.` would be `I-ORG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d1b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Jeff/NNP)\n",
      "  (GPE Bezos/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  founder/NN\n",
      "  of/IN\n",
      "  (GPE Amazon/NNP)\n",
      "  ,/,\n",
      "  visited/VBD\n",
      "  the/DT\n",
      "  main/JJ\n",
      "  headquarters/NN\n",
      "  in/IN\n",
      "  (GPE Seattle/NNP)\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Let's find the named entities!\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# We'll use the POS-tagged sentence from our previous step\n",
    "sentence = \"Jeff Bezos, the founder of Amazon, visited the main headquarters in Seattle on Monday.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Now, we apply Named Entity Chunking\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "\n",
    "print(ner_tree)\n",
    "\n",
    "# ðŸ’¡ Fun Fact: The output is a 'tree' structure. You can see how 'Apple' and 'Inc.' are grouped under the ORGANIZATION label, and 'California' is a GPE (Geo-Political Entity, like a location)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd2f2e-503b-4eec-a533-2d556cb09bed",
   "metadata": {},
   "source": [
    "| Tag | Description | Example |\n",
    "|---|---|---|\n",
    "| PERSON | People, including fictional characters. | \"Steve Jobs\", \"Marie Curie\" |\n",
    "| NORP | Nationalities or religious or political groups. | \"American\", \"Christian\" |\n",
    "| FAC | Facilities: buildings, airports, highways, bridges, etc. | \"Eiffel Tower\", \"JFK Airport\" |\n",
    "| ORG | Organizations: companies, agencies, institutions, etc. | \"Google\", \"United Nations\" |\n",
    "| GPE | Geopolitical Entity: countries, cities, states. | \"USA\", \"Paris\", \"California\" |\n",
    "| LOC | Non-GPE locations, mountain ranges, bodies of water. | \"Sahara Desert\", \"Nile River\" |\n",
    "| PRODUCT | Objects, vehicles, foods, etc. (not services). | \"iPhone\", \"Ford Mustang\" |\n",
    "| EVENT | Named hurricanes, battles, wars, sports events, etc. | \"Hurricane Katrina\", \"Super Bowl\" |\n",
    "| WORK_OF_ART | Titles of books, songs, etc. | \"The Mona Lisa\", \"Bohemian Rhapsody\" |\n",
    "| LAW | Named documents made into laws. | \"General Data Protection Regulation\" |\n",
    "| LANGUAGE | Any named language. | \"English\", \"Spanish\" |\n",
    "| DATE | Absolute or relative dates or periods. | \"2024-10-26\", \"yesterday\" |\n",
    "| TIME | Time units smaller than a day. | \"four o'clock\", \"10:30 a.m.\" |\n",
    "| PERCENT | Percentage, including \"%\". | \"20%\", \"fifty percent\" |\n",
    "| MONEY | Monetary values, including unit. | \"25 dollars\", \"â‚¬100\" |\n",
    "| QUANTITY | Measurements, as of weight or distance. | \"25 miles\", \"10 kg\" |\n",
    "| ORDINAL | \"first\", \"second\", etc. | \"first\", \"10th\" |\n",
    "| CARDINAL | Numerals that do not fall under another type. | \"one\", \"2\", \"three\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9b455",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 2\n",
    "\n",
    "Identify the named entities in the following sentence by writing them down and labeling them (e.g., PER, ORG, LOC).\n",
    "\n",
    "**Sentence**: `\"Dr. Jonas Salk discovered the polio vaccine in Pittsburgh, Pennsylvania.\"`\n",
    "\n",
    "*(Bonus: Try running this sentence through the code above to see what NLTK finds!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f59bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Dr./NNP\n",
      "  (PERSON Jonas/NNP Salk/NNP)\n",
      "  discovered/VBD\n",
      "  the/DT\n",
      "  polio/NN\n",
      "  vaccine/NN\n",
      "  in/IN\n",
      "  (GPE Pittsburgh/NNP)\n",
      "  ,/,\n",
      "  (GPE Pennsylvania/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Your code for the bonus task here\n",
    "ner_sentence = \"Dr. Jonas Salk discovered the polio vaccine in Pittsburgh, Pennsylvania.\"\n",
    "\n",
    "# Your turn! Tokenize, POS tag, and then run ne_chunk\n",
    "ner_tokens = word_tokenize(ner_sentence)\n",
    "ner_tags = pos_tag(ner_tokens)\n",
    "ner_result = ne_chunk(ner_tags)\n",
    "\n",
    "print(ner_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588d4b1",
   "metadata": {},
   "source": [
    "## Topic 3: Chunking (or Shallow Parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6e747",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "**Chunking** is a process of grouping related words into phrases, or \"chunks.\" It's like finding the basic building blocks of a sentence. Unlike a full grammatical parse, it's a simpler, 'shallow' way to see structure.\n",
    "\n",
    "The most common type is **Noun Phrase (NP) Chunking**, where we group words to find noun phrases.\n",
    "\n",
    "For example, in the sentence `\"The big red ball bounced\"`, the noun phrase is `[NP The big red ball]`.\n",
    "\n",
    "We can define a grammar rule to find these chunks. A common rule for an NP is: \"Find an optional Determiner (like 'The'), followed by any number of Adjectives, and then a Noun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bdaf7a8-88ed-4009-8b9b-82dcaf04e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’» Example: Let's find Noun Phrases!\n",
    "from nltk import RegexpParser\n",
    "\n",
    "# We'll use our tagged sentence again\n",
    "sentence = \"The old building housed Apple Inc. in California.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce8dd00-d648-40f9-894b-815e0a05390a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('old', 'JJ'),\n",
       " ('building', 'NN'),\n",
       " ('housed', 'VBD'),\n",
       " ('Apple', 'NNP'),\n",
       " ('Inc.', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('California', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a15cf8-c65d-4ba1-8eb3-46d2eef4c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define our Noun Phrase (NP) chunking grammar rule\n",
    "# <DT>? = optional Determiner\n",
    "# <JJ>* = zero or more Adjectives\n",
    "# <NN.*>+ = one or more Nouns of any type (NN, NNP, etc.)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dae6909-f35c-4ca5-b00d-cc305ac13d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP: {<DT>?<JJ>*<NN.*>+}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52852bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT old/JJ building/NN)\n",
      "  housed/VBD\n",
      "  (NP Apple/NNP Inc./NNP)\n",
      "  in/IN\n",
      "  (NP California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a parser with our grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "# Parse the sentence to find chunks\n",
    "chunk_tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(chunk_tree)\n",
    "\n",
    "# ðŸ’¡ Notice how it correctly grouped (NP The old building), (NP Apple Inc.), and (NP California)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8b060-00e7-4922-91ff-cf2c60af1f63",
   "metadata": {},
   "source": [
    "| Chunk Tag | Description | Example Sentence | Chunked Example |\n",
    "|---|---|---|---|\n",
    "| NP | Noun Phrase | The quick brown fox | [NP The quick brown fox] |\n",
    "| VP | Verb Phrase | is jumping over | [VP is jumping over] |\n",
    "| PP | Prepositional Phrase | on the lazy dog | [PP on the lazy dog] |\n",
    "| ADJP | Adjective Phrase | very quick | [ADJP very quick] |\n",
    "| ADVP | Adverb Phrase | almost too quickly | [ADVP almost too quickly] |\n",
    "| SBAR | Subordinating Conjunction | because he is happy | [SBAR because he is happy] |\n",
    "| PRT | Particle | looked up the info | [PRT up] |\n",
    "| CONJP | Conjunction Phrase | and but or | [CONJP and] |\n",
    "| INTJ | Interjection | Wow! | [INTJ Wow!] |\n",
    "| LST | List Marker | a) b) c) | [LST a)] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24284fb",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 3\n",
    "\n",
    "Try to create a simple grammar rule to chunk **verb phrases (VP)** that consist of a verb (`VB` or `VBD`) and an adverb (`RB`).\n",
    "\n",
    "**Sentence**: `The cat ran quickly.`\n",
    "**Target Chunk**: `[VP ran quickly]`\n",
    "\n",
    "Fill in the grammar rule in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d17eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S The/DT cat/NN (VP ran/VBD quickly/RB) ./.)\n"
     ]
    }
   ],
   "source": [
    "practice_sentence = \"The cat ran quickly.\"\n",
    "practice_tokens = word_tokenize(practice_sentence)\n",
    "practice_tags = pos_tag(practice_tokens)\n",
    "\n",
    "# Define your grammar rule for a Verb Phrase (VP) here\n",
    "# A verb can be VBD (past tense) or VB (present). Let's use <VB.*> to catch both.\n",
    "# An adverb is RB.\n",
    "vp_grammar = \"VP: {<VB.*><RB>}\"  # Your rule here!\n",
    "\n",
    "vp_parser = RegexpParser(vp_grammar)\n",
    "vp_tree = vp_parser.parse(practice_tags)\n",
    "\n",
    "print(vp_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de9b13",
   "metadata": {},
   "source": [
    "## Topic 4: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decec0a6",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its root or dictionary form, which is called the **lemma**.\n",
    "\n",
    "Unlike its simpler cousin, **Stemming** (which just chops off ends of words), lemmatization is smarter. It considers the word's Part-of-Speech to find the correct dictionary form.\n",
    "\n",
    "- The lemma of `running` (verb) is `run`.\n",
    "- The lemma of `ran` (verb) is `run`.\n",
    "- The lemma of `better` (adjective) is `good`.\n",
    "\n",
    "This helps group different forms of a word into a single concept, which is very useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39d418c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running (verb) -> run\n",
      "ran (verb) -> run\n",
      "better (adjective) -> good\n",
      "buildings (noun) -> building\n",
      "\n",
      "running (default pos) -> running\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Finding the lemma\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Let's lemmatize some words. The 'pos' argument tells it the part of speech.\n",
    "# 'v' for verb, 'a' for adjective, 'n' for noun.\n",
    "print(f\"running (verb) -> {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"ran (verb) -> {lemmatizer.lemmatize('ran', pos='v')}\")\n",
    "print(f\"better (adjective) -> {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "print(f\"buildings (noun) -> {lemmatizer.lemmatize('buildings', pos='n')}\")\n",
    "\n",
    "# ðŸ’¡ What happens if you don't specify the POS? Try it!\n",
    "print(f\"\\nrunning (default pos) -> {lemmatizer.lemmatize('running')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac5318",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 4\n",
    "\n",
    "Create a list of words: `['studies', 'studying', 'feet', 'leaves']`.\n",
    "\n",
    "Loop through the list and print the lemma for each word. Remember to use the correct `pos` tag (`'v'` for verbs, `'n'` for nouns) to get the right answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75248fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies (verb) -> study\n",
      "studying (verb) -> study\n",
      "feet (noun) -> foot\n",
      "leaves (noun) -> leaf\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "my_words = ['studies', 'studying', 'feet', 'leaves']\n",
    "\n",
    "# Hint: 'studies' and 'studying' are verbs. 'feet' and 'leaves' are nouns.\n",
    "print(f\"studies (verb) -> {lemmatizer.lemmatize('studies', pos='v')}\")\n",
    "print(f\"studying (verb) -> {lemmatizer.lemmatize('studying', pos='v')}\")\n",
    "print(f\"feet (noun) -> {lemmatizer.lemmatize('feet', pos='n')}\")\n",
    "print(f\"leaves (noun) -> {lemmatizer.lemmatize('leaves', pos='n')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f8a0c",
   "metadata": {},
   "source": [
    "## Topic 5: WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b16387",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "**WordNet** is a huge digital dictionary for English. But instead of just definitions, it groups words into sets of synonyms called **synsets** and shows how they are related to each other.\n",
    "\n",
    "It's like a giant web of word meanings! Some key relationships are:\n",
    "\n",
    "- **Hypernyms**: The 'is-a' relationship going up. A `car` *is a* `vehicle`. `vehicle` is the hypernym.\n",
    "- **Hyponyms**: The relationship going down. `car`, `truck`, and `bus` are hyponyms of `vehicle`.\n",
    "\n",
    "WordNet helps computers understand the meaning and context behind words, not just the words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b104a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets for 'car': [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "\n",
      "First synset: car.n.01\n",
      "Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Hypernyms: [Synset('motor_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Exploring WordNet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Let's find the synsets (different meanings) for the word 'car'\n",
    "syns = wordnet.synsets('car')\n",
    "print(\"Synsets for 'car':\", syns)\n",
    "\n",
    "# Let's look at the first meaning\n",
    "car_syn = syns[0]\n",
    "print(f\"\\nFirst synset: {car_syn.name()}\")\n",
    "print(f\"Definition: {car_syn.definition()}\")\n",
    "\n",
    "# Now let's find its hypernym (what is it an instance of?)\n",
    "hypernyms = car_syn.hypernyms()\n",
    "print(f\"Hypernyms: {hypernyms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398114b",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 5\n",
    "\n",
    "Using the code example above as a guide, find the **hyponyms** (more specific examples) of the first synset for `vehicle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6cd6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first synset for vehicle is: vehicle.n.01\n",
      "Its hyponyms are: [Synset('wheeled_vehicle.n.01'), Synset('rocket.n.01'), Synset('craft.n.02'), Synset('bumper_car.n.01'), Synset('steamroller.n.02'), Synset('skibob.n.01'), Synset('sled.n.01'), Synset('military_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Find the first synset for 'vehicle'\n",
    "vehicle_syn = wordnet.synsets('vehicle')[0]\n",
    "\n",
    "# Get the hyponyms of that synset\n",
    "vehicle_hyponyms = vehicle_syn.hyponyms()\n",
    "\n",
    "# Print the results\n",
    "print(f\"The first synset for vehicle is: {vehicle_syn.name()}\")\n",
    "print(f\"Its hyponyms are: {vehicle_hyponyms}\")\n",
    "\n",
    "# ðŸ§ª Try changing the word to 'dog' or 'cat' and see what you find!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff408f",
   "metadata": {},
   "source": [
    "## Topic 6: Bag-of-Words (BoW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf065ac4",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "Machine learning models don't understand text; they understand numbers. The **Bag-of-Words (BoW)** model is a simple way to turn text into numerical vectors.\n",
    "\n",
    "Here's how it works:\n",
    "1.  **Create a Vocabulary**: Collect all unique words from your entire set of documents.\n",
    "2.  **Count Words**: For each document, count how many times each word from the vocabulary appears.\n",
    "\n",
    "The result is a vector (a list of numbers) for each document, where each number represents the count of a specific word.\n",
    "\n",
    "ðŸ’¡ **Key Idea**: This model is called a \"bag\" of words because it ignores grammar and word order, only caring about word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "732a8e69-d8e6-40c0-acaf-6fac1470d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’» Example: Creating BoW vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fa5c9b5-4ce6-41c4-ac41-7cff034efe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP is fun and NLP is great.', 'NLP is great for text analysis.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our documents\n",
    "d1 = \"NLP is fun and NLP is great.\"\n",
    "d2 = \"NLP is great for text analysis.\"\n",
    "corpus = [d1, d2]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9be0da24-4de1-4301-9571-46d048b48015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>CountVectorizer()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71df3d43-430d-4386-83a7-85fe1a8b2c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn the vocabulary and create the BoW vectors\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00e04f0e-43d9-4be8-aaf9-6b172fd03aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['analysis' 'and' 'for' 'fun' 'great' 'is' 'nlp' 'text']\n"
     ]
    }
   ],
   "source": [
    "# Print the vocabulary (the features)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed761022-60f0-4770-9fab-f5c71c2b2ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BoW Vectors:\n",
      "[[0 1 0 1 1 2 2 0]\n",
      " [1 0 1 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print the BoW vectors as a dense array\n",
    "print(\"\\nBoW Vectors:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413e26d",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 6\n",
    "\n",
    "You have two new sentences:\n",
    "- `doc_A = \"The cat sat on the mat.\"`\n",
    "- `doc_B = \"The dog sat on the log.\"`\n",
    "\n",
    "Use `CountVectorizer` to create BoW vectors for these two documents and print the vocabulary and the resulting array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52632bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Vocabulary: ['cat' 'dog' 'log' 'mat' 'on' 'sat' 'the']\n",
      "\n",
      "New BoW Vectors:\n",
      "[[1 0 0 1 1 1 2]\n",
      " [0 1 1 0 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "doc_A = \"The cat sat on the mat.\"\n",
    "doc_B = \"The dog sat on the log.\"\n",
    "new_corpus = [doc_A, doc_B]\n",
    "\n",
    "# Create a new vectorizer\n",
    "my_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the new corpus\n",
    "my_X = my_vectorizer.fit_transform(new_corpus)\n",
    "\n",
    "# Print the results\n",
    "print(\"New Vocabulary:\", my_vectorizer.get_feature_names_out())\n",
    "print(\"\\nNew BoW Vectors:\")\n",
    "print(my_X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ee26e",
   "metadata": {},
   "source": [
    "## Topic 7: Feature Selection (Removing Stop Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f469cc",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "Often, our text contains very common words that don't add much meaning, like `the`, `is`, `a`, `in`. These are called **stop words**.\n",
    "\n",
    "**Feature Selection** is the process of choosing the most relevant features (words) for our model. A simple and effective form of feature selection is removing stop words.\n",
    "\n",
    "By removing them, we reduce the size of our vocabulary and help our model focus on the words that carry more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a88d2623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['nlp', 'is', 'great', 'for', 'text', 'analysis', '.']\n",
      "Filtered Tokens (no stop words): ['nlp', 'great', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"NLP is great for text analysis.\"\n",
    "tokens = word_tokenize(sentence.lower()) # Convert to lowercase to match stop words\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens (no stop words):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397c717",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 7\n",
    "\n",
    "Remove the stop words from the following sentence: `\"This is a simple example to show the process.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48267424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'show', 'process']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "practice_sentence = \"This is a simple example to show the process.\"\n",
    "\n",
    "# Your code here: tokenize, and then filter out stop words\n",
    "practice_tokens = word_tokenize(practice_sentence.lower())\n",
    "\n",
    "filtered_list = [word for word in practice_tokens if word not in stop_words and word.isalpha()]\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd86a28",
   "metadata": {},
   "source": [
    "## Topic 8: Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dad330",
   "metadata": {},
   "source": [
    "ðŸ“„ **Explanation**\n",
    "\n",
    "Now that we can turn text into numerical vectors (using BoW), we can measure how similar two documents are! **Document Similarity** is a score that tells us how close two documents are in terms of their content.\n",
    "\n",
    "A popular method is **Cosine Similarity**. It measures the angle between two vectors. \n",
    "- A score of **1** means the documents are identical (or at least point in the same direction).\n",
    "- A score of **0** means the documents have no words in common.\n",
    "- A score in between (like **0.6**) means they are moderately similar.\n",
    "\n",
    "This is very powerful for tasks like finding related articles, plagiarism detection, and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "955361c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for D1: [[0 1 0 1 1 2 2 0]]\n",
      "Vector for D2: [[1 0 1 0 1 1 1 1]]\n",
      "\n",
      "Cosine Similarity: 0.615\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Calculating Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Let's reuse our first BoW example\n",
    "d1 = \"NLP is fun and NLP is great.\"\n",
    "d2 = \"NLP is great for text analysis.\"\n",
    "corpus = [d1, d2]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectors = X.toarray()\n",
    "\n",
    "# The vectors are:\n",
    "vec1 = vectors[0].reshape(1, -1) # reshape for the function\n",
    "vec2 = vectors[1].reshape(1, -1)\n",
    "print(\"Vector for D1:\", vec1)\n",
    "print(\"Vector for D2:\", vec2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = cosine_similarity(vec1, vec2)\n",
    "\n",
    "print(f\"\\nCosine Similarity: {similarity_score[0][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a730c",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 8\n",
    "\n",
    "Based on the result above (our calculated score was around `0.615`), would you say the two documents are very similar, somewhat similar, or not similar at all? \n",
    "\n",
    "Write your answer as a comment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d56fb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# A score of 0.615 is greater than 0.5, which indicates the documents are somewhat similar.\n",
    "# They share important keywords like 'NLP' and 'great', but also have unique words, so they are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433ee6e",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Final Revision Assignment ðŸŽ‰\n",
    "\n",
    "Congratulations on completing the core concepts! Now it's time to put everything together. This assignment is designed for you to practice at home and solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b607c",
   "metadata": {},
   "source": [
    "You are given two new customer reviews:\n",
    "\n",
    "**Review 1**: `\"The new phone has an amazing camera and a great screen.\"`\n",
    "**Review 2**: `\"The camera is good, but the new screen is much better.\"`\n",
    "\n",
    "Complete the following tasks in the code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our reviews\n",
    "review1 = \"The new phone has an amazing camera and a great screen.\"\n",
    "review2 = \"The camera is good, but the new screen is much better.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64950ba5",
   "metadata": {},
   "source": [
    "**Task 1:** Perform POS tagging on `review1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Task 1\n",
    "tokens1 = word_tokenize(review1)\n",
    "tags1 = pos_tag(tokens1)\n",
    "print(\"POS Tags for Review 1:\", tags1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad647c5",
   "metadata": {},
   "source": [
    "**Task 2:** Perform Named Entity Recognition (NER) on this sentence: `\"Apple announced the new iPhone in California.\"` Is it finding the entities correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Task 2\n",
    "ner_review = \"Apple announced the new iPhone in California.\"\n",
    "ner_tokens = word_tokenize(ner_review)\n",
    "ner_tags = pos_tag(ner_tokens)\n",
    "ner_tree = ne_chunk(ner_tags)\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57295b",
   "metadata": {},
   "source": [
    "**Task 3:** Lemmatize the following words: `['amazing', 'better', 'has']`. Remember to use the correct POS tag (`a` for adjective, `v` for verb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Task 3\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(f\"amazing (adj) -> {lemmatizer.lemmatize('amazing', pos='a')}\")\n",
    "print(f\"better (adj) -> {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "print(f\"has (verb) -> {lemmatizer.lemmatize('has', pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37662c7",
   "metadata": {},
   "source": [
    "**Task 4:** Create Bag-of-Words vectors for `review1` and `review2`. Before you do, remove the stop words from both reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Task 4\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter review 1\n",
    "tokens1 = word_tokenize(review1.lower())\n",
    "filtered1 = [word for word in tokens1 if word not in stop_words and word.isalpha()]\n",
    "filtered_review1 = \" \".join(filtered1)\n",
    "\n",
    "# Filter review 2\n",
    "tokens2 = word_tokenize(review2.lower())\n",
    "filtered2 = [word for word in tokens2 if word not in stop_words and word.isalpha()]\n",
    "filtered_review2 = \" \".join(filtered2)\n",
    "\n",
    "print(\"Filtered Review 1:\", filtered_review1)\n",
    "print(\"Filtered Review 2:\", filtered_review2)\n",
    "\n",
    "# Now create BoW vectors\n",
    "final_corpus = [filtered_review1, filtered_review2]\n",
    "final_vectorizer = CountVectorizer()\n",
    "final_X = final_vectorizer.fit_transform(final_corpus)\n",
    "\n",
    "print(\"\\nFinal Vocabulary:\", final_vectorizer.get_feature_names_out())\n",
    "print(\"Final Vectors:\\n\", final_X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8088f",
   "metadata": {},
   "source": [
    "**Task 5:** Calculate the Cosine Similarity between the two vectors you created in Task 4. Are the reviews similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Task 5\n",
    "similarity = cosine_similarity(final_X[0], final_X[1])\n",
    "print(f\"The cosine similarity between the reviews is: {similarity[0][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eea724",
   "metadata": {},
   "source": [
    "**Task 6 (Conceptual):** In your own words, why is it a good idea to lemmatize words *before* creating Bag-of-Words vectors? Write your answer as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer for Task 6\n",
    "# It's a good idea because it groups different forms of a word into a single concept.\n",
    "# For example, 'run', 'running', and 'ran' would all become 'run'.\n",
    "# This means our vocabulary will be smaller, and the counts for the word 'run' will be more accurate,\n",
    "# which helps the model understand that all these words refer to the same idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab604b08",
   "metadata": {},
   "source": [
    "### âœ… Well done! You've completed the introduction to NLP. Keep experimenting and building on these foundational concepts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
