{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60cc0d2",
   "metadata": {},
   "source": [
    "# üöÄ Introduction to Natural Language Processing (NLP) for Beginners\n",
    "\n",
    "Welcome to your first adventure in Natural Language Processing! In this 2-hour session, we'll explore the fundamental techniques used to help computers understand and process human language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c143",
   "metadata": {},
   "source": [
    "### üìò Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1.  **Understand Tokenization**: Break down text into words and sentences.\n",
    "2.  **Grasp Word Segmentation**: Learn how to identify words in languages without spaces.\n",
    "3.  **Apply Stemming**: Reduce words to their root form.\n",
    "4.  **Perform Text Normalization**: Clean and standardize raw text.\n",
    "5.  **Use Regular Expressions**: Find patterns and extract information from text.\n",
    "\n",
    "Let's get started! üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d935861",
   "metadata": {},
   "source": [
    "## Topic 1: Word and Sentence Tokenization\n",
    "\n",
    "üìÑ **Explanation**\n",
    "\n",
    "Tokenization is the very first step in most NLP tasks. It's like chopping vegetables before you cook! We break down a large text into smaller, manageable pieces called **tokens**.\n",
    "\n",
    "- **Word Tokenization**: Splits a sentence into individual words. For example, `\"NLP is fun\"` becomes `['NLP', 'is', 'fun']`.\n",
    "- **Sentence Tokenization**: Splits a paragraph into individual sentences.\n",
    "\n",
    "This helps the computer see the text as a list of items it can work with, rather than just one big block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994eb73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word Tokens: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'Natural', 'Language', 'Processing', '.', 'It', 'provides', 'easy-to-use', 'interfaces', '.']\n",
      "‚úÖ Sentence Tokens: ['NLTK is a powerful library for Natural Language Processing.', 'It provides easy-to-use interfaces.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# First, we need a library called NLTK (Natural Language Toolkit).\n",
    "# This line downloads the 'punkt' package, which contains pre-trained models for tokenization.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now we import the specific functions we need.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"NLTK is a powerful library for Natural Language Processing. It provides easy-to-use interfaces.\"\n",
    "\n",
    "# Let's tokenize the text into words\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"‚úÖ Word Tokens:\", word_tokens)\n",
    "\n",
    "# Now, let's tokenize the same text into sentences\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"‚úÖ Sentence Tokens:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f1c64",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: Your Turn to Tokenize!\n",
    "\n",
    "Given the sentence `\"I'm learning NLP, aren't you?\"`, perform word tokenization. See how the tokenizer handles contractions like `I'm` and `aren't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812921a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = \"I'm learning NLP, aren't you?\"\n",
    "\n",
    "# Your code here: Use the word_tokenize function on my_sentence\n",
    "my_word_tokens = word_tokenize(my_sentence)\n",
    "\n",
    "# Print your results\n",
    "print(\"My Tokenized Words:\", my_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5e62d",
   "metadata": {},
   "source": [
    "## Topic 2: Word Segmentation\n",
    "\n",
    "üìÑ **Explanation**\n",
    "\n",
    "**Word Segmentation** is the process of breaking down text into individual words or tokens. Different languages require different approaches:\n",
    "\n",
    "### Languages Without Spaces (Chinese, Japanese, Thai)\n",
    "Imagine reading a sentence withnospacesbetweenwords. That's a challenge languages like Chinese, Japanese, and Thai present! These languages need special algorithms to identify word boundaries.\n",
    "\n",
    "**Example in Chinese:**\n",
    "- Original: `ÊàëÂñúÊ¨¢Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`\n",
    "- Segmented: `Êàë` (I) / `ÂñúÊ¨¢` (like) / `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` (Natural Language Processing)\n",
    "\n",
    "### Languages With Spaces (Urdu, English, Arabic)\n",
    "Languages like Urdu, English, and Arabic already have spaces between words, making tokenization simpler. We can split text by spaces to get individual words.\n",
    "\n",
    "**Example in Urdu:**\n",
    "- Original: `ŸÖ€å⁄∫ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ÿ≥€í ŸÖÿ≠ÿ®ÿ™ ⁄©ÿ±ÿ™ÿß €ÅŸà⁄∫`\n",
    "- Segmented: `ŸÖ€å⁄∫` (I) / `Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ` (Pakistan) / `ÿ≥€í` (from) / `ŸÖÿ≠ÿ®ÿ™` (love) / `⁄©ÿ±ÿ™ÿß` (do) / `€ÅŸà⁄∫` (am)\n",
    "\n",
    "**Example in English:**\n",
    "- Original: `I love natural language processing`\n",
    "- Segmented: `I` / `love` / `natural` / `language` / `processing`\n",
    "\n",
    "### Why is this important?\n",
    "Without proper word segmentation, computers can't understand individual words and their meanings. This is the foundation for all other NLP tasks like translation, sentiment analysis, and information extraction.\n",
    "\n",
    "**Key Tools:**\n",
    "- **Chinese:** jieba library\n",
    "- **Urdu/English/Arabic:** Simple split() method or specialized libraries\n",
    "- **Japanese:** MeCab or Janome\n",
    "- **Thai:** PyThaiNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a7a61f-c3e7-4836-a377-9aa0fa1fabfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration with Urdu text: ŸÖ€å⁄∫/ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ/ ÿ≥€í/ ŸÖÿ≠ÿ®ÿ™/ ⁄©ÿ±ÿ™ÿß/ €ÅŸà⁄∫\n",
      "\n",
      "--- More Examples ---\n",
      "ÿßÿ≥ŸÑÿßŸÖ ÿ¢ÿ®ÿßÿØ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©ÿß ÿØÿßÿ±ÿßŸÑÿ≠⁄©ŸàŸÖÿ™ €Å€í\n",
      "Tokens: ÿßÿ≥ŸÑÿßŸÖ / ÿ¢ÿ®ÿßÿØ / Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ / ⁄©ÿß / ÿØÿßÿ±ÿßŸÑÿ≠⁄©ŸàŸÖÿ™ / €Å€í\n",
      "\n",
      "⁄©€åÿß ÿ≠ÿßŸÑ €Å€í ÿ¢Ÿæ ⁄©ÿß\n",
      "Tokens: ⁄©€åÿß / ÿ≠ÿßŸÑ / €Å€í / ÿ¢Ÿæ / ⁄©ÿß\n",
      "\n",
      "€å€Å ÿß€å⁄© ÿÆŸàÿ®ÿµŸàÿ±ÿ™ ÿØŸÜ €Å€í\n",
      "Tokens: €å€Å / ÿß€å⁄© / ÿÆŸàÿ®ÿµŸàÿ±ÿ™ / ÿØŸÜ / €Å€í\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple and reliable solution for Urdu tokenization\n",
    "# No installation needed - works immediately in Google Colab\n",
    "\n",
    "text = \"ŸÖ€å⁄∫ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ÿ≥€í ŸÖÿ≠ÿ®ÿ™ ⁄©ÿ±ÿ™ÿß €ÅŸà⁄∫\"  # Example Urdu text\n",
    "seg_list = text.split()\n",
    "print(\"Demonstration with Urdu text: \" + \"/ \".join(seg_list))\n",
    "\n",
    "# Additional examples\n",
    "print(\"\\n--- More Examples ---\")\n",
    "examples = [\n",
    "    \"ÿßÿ≥ŸÑÿßŸÖ ÿ¢ÿ®ÿßÿØ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©ÿß ÿØÿßÿ±ÿßŸÑÿ≠⁄©ŸàŸÖÿ™ €Å€í\",\n",
    "    \"⁄©€åÿß ÿ≠ÿßŸÑ €Å€í ÿ¢Ÿæ ⁄©ÿß\",\n",
    "    \"€å€Å ÿß€å⁄© ÿÆŸàÿ®ÿµŸàÿ±ÿ™ ÿØŸÜ €Å€í\"\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    tokens = ex.split()\n",
    "    print(f\"{ex}\")\n",
    "    print(f\"Tokens: {' / '.join(tokens)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69468232",
   "metadata": {},
   "source": [
    "## Topic 3: Stemming\n",
    "\n",
    "üìÑ **Explanation**\n",
    "\n",
    "Computers are very literal. They see \"run\", \"running\", and \"ran\" as three completely different words. **Stemming** is a technique to chop off the ends of words to get to the basic root or **stem**.\n",
    "\n",
    "- `studies` -> `studi`\n",
    "- `studying` -> `studi`\n",
    "\n",
    "This helps us group similar words together. It's not always perfect (notice `studi` isn't a real word), but it's fast and simple! The most famous algorithm for this is the **Porter Stemmer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the PorterStemmer from our NLTK library\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# First, create a stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words_to_stem = [\"running\", \"runner\", \"runs\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Let's loop through the words and stem each one\n",
    "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
    "\n",
    "print(\"Original words:\", words_to_stem)\n",
    "print(\"‚úÖ Stemmed words:\", stemmed_words)\n",
    "\n",
    "# **Fun Fact**: Stemming can sometimes be too aggressive. For example, it might stem `\"university\"` and `\"universe\"` to the same stem `\"univers\"`, which can be confusing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd771281",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: Stem Your Own Words\n",
    "\n",
    "Create a list of words: `['connection', 'connected', 'connecting', 'connections']`. Apply the Porter Stemmer to see their common root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe83839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My stemmed list: ['connect', 'connect', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "# The stemmer object is already created from the previous cell\n",
    "my_words = ['connection', 'connected', 'connecting', 'connections']\n",
    "\n",
    "# Your code here: Create a new list with the stemmed versions of my_words\n",
    "my_stemmed_words = [stemmer.stem(w) for w in my_words]\n",
    "\n",
    "# Print the results\n",
    "print(\"My stemmed list:\", my_stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23ea9f",
   "metadata": {},
   "source": [
    "## Topic 4: Text Normalization (Putting It All Together)\n",
    "\n",
    "üìÑ **Explanation**\n",
    "\n",
    "Raw text from the real world is messy! It has capital letters, punctuation, numbers, and common but unimportant words (like \"a\", \"the\", \"is\"). **Text Normalization** is the process of cleaning and standardizing text to make it easier for a computer to analyze.\n",
    "\n",
    "A typical normalization pipeline includes:\n",
    "1.  **Case Folding**: Converting all text to lowercase.\n",
    "2.  **Punctuation Removal**: Getting rid of characters like `!`, `.`, and `?`.\n",
    "3.  **Stop Word Removal**: Removing common words that don't add much meaning (e.g., 'the', 'a', 'in').\n",
    "4.  **Tokenization & Stemming**: We've already learned these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b4d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown FOXES are JUMPING over 10 lazy dogs!\n",
      "\n",
      "Step 1 (Lowercase): the quick brown foxes are jumping over 10 lazy dogs!\n",
      "Step 2 (Punctuation/Number Removal): the quick brown foxes are jumping over  lazy dogs\n",
      "Step 3 (Tokenization): ['the', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'lazy', 'dogs']\n",
      "Step 4 (Stop Word Removal): ['quick', 'brown', 'foxes', 'jumping', 'lazy', 'dogs']\n",
      "\n",
      "‚úÖ Final Normalized Tokens: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re # This library is for regular expressions, great for finding patterns!\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the list of stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "raw_text = \"The quick brown FOXES are JUMPING over 10 lazy dogs!\"\n",
    "print(\"Original Text:\", raw_text)\n",
    "\n",
    "# 1. Lowercasing\n",
    "text = raw_text.lower()\n",
    "print(\"\\nStep 1 (Lowercase):\", text)\n",
    "\n",
    "# 2. Removing punctuation and numbers (using a simple regex)\n",
    "text = re.sub(r'[^a-z\\s]', '', text) # Keep only letters and spaces\n",
    "print(\"Step 2 (Punctuation/Number Removal):\", text)\n",
    "\n",
    "# 3. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Step 3 (Tokenization):\", tokens)\n",
    "\n",
    "# 4. Stop Word Removal\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Step 4 (Stop Word Removal):\", filtered_tokens)\n",
    "\n",
    "# 5. Stemming\n",
    "final_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\n‚úÖ Final Normalized Tokens:\", final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf32b8f",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: Normalize a Review\n",
    "\n",
    "You have a customer review: `\"This product is AMAZING!!! I bought 2 and I will be buying more.\"`\n",
    "\n",
    "Normalize this review by:\n",
    "1.  Converting it to lowercase.\n",
    "2.  Tokenizing it.\n",
    "3.  Removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b269c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"This product is AMAZING!!! I bought 2 and I will be buying more.\"\n",
    "\n",
    "# 1. Convert to lowercase\n",
    "lower_review = review.lower()\n",
    "\n",
    "# 2. Tokenize the lowercase review\n",
    "review_tokens = word_tokenize(lower_review)\n",
    "\n",
    "# 3. Remove stop words\n",
    "# Your code here: Create a new list containing only the tokens that are NOT in stop_words\n",
    "filtered_review = [token for token in review_tokens if token not in stop_words and token.isalpha()]\n",
    "\n",
    "print(\"Cleaned Review Tokens:\", filtered_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056c03d",
   "metadata": {},
   "source": [
    "## Topic 5: Regular Expressions (Regex)\n",
    "\n",
    "üìÑ **Explanation**\n",
    "\n",
    "A **Regular Expression** (or regex) is a powerful tool for finding patterns in text. Think of it as a super-powered search command. You can use it to find things like email addresses, phone numbers, or any specific sequence of characters you can imagine.\n",
    "\n",
    "Some basic patterns:\n",
    "- `\\d` matches any digit (0-9).\n",
    "- `\\s` matches any whitespace character (space, tab).\n",
    "- `\\w` matches any word character (letters, numbers, and underscore).\n",
    "- `+` means \"one or more\" of the preceding character.\n",
    "- `*` means \"zero or more\" of the preceding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99aac29-6f32-4da4-a580-6e194922f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prices found: ['$49.99']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"The price of the product is $49.99. The event is on 10/20/2025. Contact support@example.com for help.\"\n",
    "\n",
    "# Let's find a price that looks like $XX.XX\n",
    "# \\$ matches the dollar sign, \\d+ matches one or more digits, \\. matches the dot\n",
    "prices = re.findall(r'\\$\\d+\\.\\d{2}', text)\n",
    "print(f\"‚úÖ Prices found: {prices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e74b2bb-c2e6-4c0e-86af-4c83a5b7f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dates found: ['10/20/2025']\n"
     ]
    }
   ],
   "source": [
    "### # Now let's find a date in the format XX/XX/XXXX\n",
    "dates = re.findall(r'\\d{2}/\\d{2}/\\d{4}', text)\n",
    "print(f\"‚úÖ Dates found: {dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e384fd-3ce4-46ac-b356-b4dbed688502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Emails found: ['support@example.com']\n"
     ]
    }
   ],
   "source": [
    "# And finally, let's find the email address\n",
    "# \\S+ matches one or more non-whitespace characters\n",
    "emails = re.findall(r'\\S+@\\S+', text)\n",
    "print(f\"‚úÖ Emails found: {emails}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca0d16-283d-48fc-9dc3-4bfee6c7e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "üß™ **Experiment!** Try changing the text string to include other prices or dates and see if the regex can find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6790f-aa99-4c53-9e05-f472f89f45c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6edf3aac-c987-4d91-bd1a-59806b48117f",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb42a47",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: Find the Phone Numbers\n",
    "\n",
    "Write a regular expression to find all phone numbers in the format `XXX-XXX-XXXX` from the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fceb2bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found phone numbers: ['123-456-7890', '987-654-3210']\n"
     ]
    }
   ],
   "source": [
    "phone_text = \"You can reach me at 123-456-7890 or my colleague at 987-654-3210. Do not call 555-1111.\"\n",
    "\n",
    "# Your regex pattern here. Hint: \\d{3} matches exactly three digits.\n",
    "pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
    "\n",
    "# Use re.findall() with your pattern and the phone_text\n",
    "found_numbers = re.findall(pattern, phone_text)\n",
    "\n",
    "print(\"Found phone numbers:\", found_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228201f-53a3-492d-962f-d828db52dc34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d57b9b5-000f-487f-8462-088cc4e1da68",
   "metadata": {},
   "source": [
    "## Information Extraction with Regular Expressions\n",
    "\n",
    "### üìä Common Patterns for Data Extraction\n",
    "\n",
    "| **Pattern Type** | **Regex Pattern** | **Example Match** | **Use Case** |\n",
    "|------------------|-------------------|-------------------|--------------|\n",
    "| **Email Address** | `\\S+@\\S+\\.\\S+` | support@example.com | Contact information extraction |\n",
    "| **Price (USD)** | `\\$\\d+\\.\\d{2}` | $49.99 | E-commerce, financial documents |\n",
    "| **Phone (US)** | `\\d{3}-\\d{3}-\\d{4}` | 555-123-4567 | Contact details |\n",
    "| **Date (MM/DD/YYYY)** | `\\d{2}/\\d{2}/\\d{4}` | 10/20/2025 | Event scheduling, records |\n",
    "| **URL** | `https?://\\S+` | https://example.com | Web scraping, link extraction |\n",
    "| **Hashtag** | `#\\w+` | #NLP #AI | Social media analysis |\n",
    "| **Mention** | `@\\w+` | @username | Social media monitoring |\n",
    "| **Credit Card** | `\\d{4}-\\d{4}-\\d{4}-\\d{4}` | 1234-5678-9012-3456 | Payment processing |\n",
    "| **ZIP Code (US)** | `\\b\\d{5}\\b` | 12345 | Address parsing |\n",
    "| **IP Address** | `\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}` | 192.168.1.1 | Network logs, security |\n",
    "| **Time (HH:MM)** | `\\d{2}:\\d{2}` | 14:30 | Scheduling, timestamps |\n",
    "| **Percentage** | `\\d+\\.?\\d*%` | 75.5% | Reports, analytics |\n",
    "\n",
    "### üáµüá∞ Pakistan-Specific Patterns\n",
    "\n",
    "| **Pattern Type** | **Regex Pattern** | **Example Match** | **Use Case** |\n",
    "|------------------|-------------------|-------------------|--------------|\n",
    "| **CNIC** | `\\d{5}-\\d{7}-\\d{1}` | 12345-1234567-1 | Identity verification |\n",
    "| **Mobile Number** | `\\+92-?3\\d{9}` | +92-3001234567 | Contact information |\n",
    "| **Landline** | `\\d{2,4}-\\d{7,8}` | 051-1234567 | Business contacts |\n",
    "| **Postal Code** | `\\d{5}` | 46000 | Address validation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71868975-ad5a-43a0-af6e-7ff74152b4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715b69ef-325c-411e-9868-94bbaf1e219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Emails: ['support@example.com']\n",
      "üìû Phones: ['555-123-4567']\n",
      "üí∞ Prices: ['$49.99']\n",
      "üìÖ Dates: ['10/20/2025']\n",
      "üîó URLs: ['https://example.com']\n",
      "#Ô∏è‚É£ Hashtags: ['#GreatDeals']\n",
      "@ Mentions: ['@example', '@CompanyName']\n",
      "üÜî CNIC: ['12345-1234567-1']\n",
      "üì± PK Mobile: ['+92-3001234567']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "Contact us at support@example.com or call 555-123-4567.\n",
    "Product price: $49.99. Visit https://example.com\n",
    "Event date: 10/20/2025 at 14:30\n",
    "Follow us @CompanyName #GreatDeals\n",
    "Pakistani CNIC: 12345-1234567-1\n",
    "Mobile: +92-3001234567\n",
    "\"\"\"\n",
    "\n",
    "# Extract different patterns\n",
    "emails = re.findall(r'\\S+@\\S+\\.\\S+', text)\n",
    "phones = re.findall(r'\\d{3}-\\d{3}-\\d{4}', text)\n",
    "prices = re.findall(r'\\$\\d+\\.\\d{2}', text)\n",
    "dates = re.findall(r'\\d{2}/\\d{2}/\\d{4}', text)\n",
    "urls = re.findall(r'https?://\\S+', text)\n",
    "hashtags = re.findall(r'#\\w+', text)\n",
    "mentions = re.findall(r'@\\w+', text)\n",
    "cnic = re.findall(r'\\d{5}-\\d{7}-\\d{1}', text)\n",
    "pk_mobile = re.findall(r'\\+92-?3\\d{9}', text)\n",
    "\n",
    "print(f\"üìß Emails: {emails}\")\n",
    "print(f\"üìû Phones: {phones}\")\n",
    "print(f\"üí∞ Prices: {prices}\")\n",
    "print(f\"üìÖ Dates: {dates}\")\n",
    "print(f\"üîó URLs: {urls}\")\n",
    "print(f\"#Ô∏è‚É£ Hashtags: {hashtags}\")\n",
    "print(f\"@ Mentions: {mentions}\")\n",
    "print(f\"üÜî CNIC: {cnic}\")\n",
    "print(f\"üì± PK Mobile: {pk_mobile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962de7a",
   "metadata": {},
   "source": [
    "## üéâ Final Revision Assignment üéâ\n",
    "\n",
    "Congratulations on making it through the fundamentals of NLP! It's time to combine everything you've learned. These tasks are for you to practice at home to solidify your knowledge.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24365781",
   "metadata": {},
   "source": [
    "### Task 1: Clean Up a Messy Sentence\n",
    "\n",
    "Given the sentence below, use a regular expression to remove all the numbers and special characters, leaving only letters and spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13851946",
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_sentence = \"*** HELLO!! 123 This is a TEST 456 sentence... please clean me! 789 ***\"\n",
    "\n",
    "# Your code here\n",
    "cleaned_sentence = re.sub(r'[^a-zA-Z\\s]', '', messy_sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e615ad5",
   "metadata": {},
   "source": [
    "### Task 2: Full Normalization Pipeline\n",
    "\n",
    "Take your `cleaned_sentence` from Task 1 and perform the following steps:\n",
    "1.  Convert it to lowercase.\n",
    "2.  Tokenize it into words.\n",
    "3.  Remove all English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (you can reuse the cleaned_sentence from the cell above)\n",
    "lower_sentence = cleaned_sentence.lower()\n",
    "tokens = word_tokenize(lower_sentence)\n",
    "final_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ed4f4",
   "metadata": {},
   "source": [
    "### Task 3: Stem the Final Words\n",
    "\n",
    "Now, take the list of `final_words` you created in Task 2 and apply the Porter Stemmer to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "stemmed_final_words = [stemmer.stem(word) for word in final_words]\n",
    "print(stemmed_final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a16e0",
   "metadata": {},
   "source": [
    "### Task 4: Extract Information from a Bio\n",
    "\n",
    "You have a short biography. Your goal is to extract the person's email and the year they were born using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c338d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio = \"John Doe, born in 1995, is a data scientist. You can contact him at john.doe@email.com for work inquiries. His old email was j.doe@university.edu.\"\n",
    "\n",
    "# Find the year (4 digits)\n",
    "year_pattern = r'\\d{4}'\n",
    "year = re.search(year_pattern, bio)\n",
    "print(\"Year of birth:\", year.group(0) if year else \"Not found\")\n",
    "\n",
    "# Find all email addresses\n",
    "email_pattern = r'\\S+@\\S+'\n",
    "emails = re.findall(email_pattern, bio)\n",
    "print(\"Emails found:\", emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18b3db",
   "metadata": {},
   "source": [
    "### Task 5: Sentence Boundary Detection\n",
    "\n",
    "The following text has a tricky abbreviation. Use NLTK's `sent_tokenize` to see if it can correctly identify the two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_text = \"Dr. Smith lives in New York. He is a doctor.\"\n",
    "\n",
    "# Your code here\n",
    "sentences = sent_tokenize(tricky_text)\n",
    "print(f\"Found {len(sentences)} sentences:\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624e373",
   "metadata": {},
   "source": [
    "## ‚úÖ Well Done!\n",
    "\n",
    "You've successfully covered the core building blocks of Natural Language Processing. These pre-processing steps are crucial for almost any advanced AI task involving text, from building chatbots to analyzing customer sentiment. Keep practicing and exploring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
