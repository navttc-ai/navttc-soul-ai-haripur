{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eee095",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Comprehensive Guide to Building and Evaluating ML Models\n",
    "\n",
    "**Welcome to your 2-hour crash course on Machine Learning fundamentals!**\n",
    "\n",
    "In this session, we will explore the essential steps and concepts required to build and evaluate robust machine learning models. Get ready to dive into the world of data, models, and metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8a8cc",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this 2-hour session, you will be able to:\n",
    "\n",
    "1.  **Understand Testing in ML:** Grasp the importance of testing data, models, and pipelines.\n",
    "2.  **Use Evaluation Metrics:** Learn to choose and calculate the right metrics for classification and regression tasks.\n",
    "3.  **Differentiate Classification vs. Regression:** Identify the two main types of supervised learning problems and evaluate them.\n",
    "4.  **Handle Dataset Imbalance:** Recognize the problem of imbalanced data and know the basic techniques to fix it.\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8aa3d",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 1: Testing in Machine Learning (Approx. 20 mins)\n",
    "\n",
    "ðŸ“„ **Explanation: What is ML Testing?**\n",
    "\n",
    "Testing in machine learning is the process of checking every part of your ML system to make sure it works correctly, reliably, and fairly. It's different from normal software testing because we're not just checking code; we're also checking the **data quality** and the **model's performance**.\n",
    "\n",
    "**Key Types of Tests:**\n",
    "- **Unit Tests:** Testing small, individual pieces of code (like a data cleaning function).\n",
    "- **Integration Tests:** Testing if different parts work together (e.g., does data loading and preprocessing connect smoothly?).\n",
    "- **Data Testing:** Checking if your data is accurate, complete, and in the right format.\n",
    "- **Model Validation:** Seeing how well your trained model performs on new, unseen data.\n",
    "- **A/B Testing:** Comparing two different models in a live environment to see which one performs better in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5da2fa40",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:17\u001b[1;36m\u001b[0m\n\u001b[1;33m    assert 99 not in cleaned_data\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: A Unit Test\n",
    "# Let's say we have a function to remove extreme values (outliers) from a list of data.\n",
    "\n",
    "def remove_outliers(data):\n",
    "    \"\"\"For our purpose, let's just remove any number over 50.\"\"\"\n",
    "    cleaned_data = [point for point in data if point <= 50]\n",
    "    return cleaned_data\n",
    "\n",
    "# Now, we write a unit test to check if our function works as expected.\n",
    "def test_remove_outliers():\n",
    "    print(\"Running test...\")\n",
    "    test_data = [1, 2, 3, 100, 5, 4, 99]\n",
    "    cleaned_data = remove_outliers(test_data)\n",
    "    \n",
    "    # The 'assert' keyword checks if a condition is true. If not, it raises an error.\n",
    "    assert 100 not in cleaned_data\n",
    "    assert 99 not in cleaned_data\n",
    "    \n",
    "    print(\"âœ… Test Passed! The function correctly removed the outliers.\")\n",
    "\n",
    "# Run the test\n",
    "test_remove_outliers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7ecfd",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 1\n",
    "\n",
    "You have a function that **normalizes** data, scaling all numbers to be between 0 and 1. Write a simple unit test to verify its correctness.\n",
    "**Hint:** After normalizing, the minimum value should be 0 and the maximum value should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602711dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
      "\n",
      "âœ… Well done! Your test confirms the function works.\n"
     ]
    }
   ],
   "source": [
    "# The function to be tested\n",
    "def normalize_data(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    if max_val == min_val:\n",
    "        return [0.0 for x in data]\n",
    "    return [(x - min_val) / (max_val - min_val) for x in data]\n",
    "\n",
    "# Write your test function here\n",
    "def test_normalize_data():\n",
    "    my_data = [10, 20, 30, 40, 50]\n",
    "    normalized = normalize_data(my_data)\n",
    "    \n",
    "    # ðŸ§ª Your Assertions Go Here! \n",
    "    print(f\"Normalized data: {normalized}\")\n",
    "    assert min(normalized) == 0.0\n",
    "    assert max(normalized) == 1.0\n",
    "    \n",
    "    print(\"\\nâœ… Well done! Your test confirms the function works.\")\n",
    "\n",
    "# Run your test\n",
    "test_normalize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c2671",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 2: Evaluation Metrics (Approx. 20 mins)\n",
    "\n",
    "ðŸ“„ **Explanation: How Good Is Your Model?**\n",
    "\n",
    "Evaluation metrics are scores that tell us how well our model is performing. Just like you get a grade on a test, a model gets a score from a metric. The metric you choose depends on the problem you're solving.\n",
    "\n",
    "### ðŸ“Š Classification Metrics (Is it A or B?)\n",
    "A **Confusion Matrix** shows where the model was right and wrong.\n",
    "- **True Positives (TP):** Correctly predicted as Positive.\n",
    "- **True Negatives (TN):** Correctly predicted as Negative.\n",
    "- **False Positives (FP):** Wrongly predicted as Positive (Type I Error).\n",
    "- **False Negatives (FN):** Wrongly predicted as Negative (Type II Error).\n",
    "\n",
    "From this, we calculate:\n",
    "- **Accuracy:** Overall correctness. Can be misleading for imbalanced data!\n",
    "- *A value close to 1.0 indicates the model makes mostly correct predictions, while a value close to 0 indicates frequent misclassifications.*\n",
    "\n",
    "- **Precision:** Correctness of positive predictions. Use when FPs are costly.\n",
    "- A value close to 1.0 indicates few false positives, while a value close to 0 indicates many normal samples are incorrectly flagged as anomalies.*\n",
    "\n",
    "- **Recal**  Represents the model's ability to distinguish between normal and anomalous cases.  \n",
    "  *A value close to 1.0 indicates excellent separability and strong classification power, while a value close to 0 indicates poor discriminative ability.*\n",
    "- **F1-Score:** A balance between Precision and Recall.\n",
    "- Represents the harmonic mean of precision and recall, balancing both false positives and false negatives.\n",
    "- A value close to 1.0 indicates a strong balance between precision and recall, while a value close to 0 indicates poor performance in one or both metrics.*\n",
    "\n",
    "- **AUC-ROC**  \n",
    "  Represents the model's ability to distinguish between normal and anomalous cases.  \n",
    "  *A value close to 1.0 indicates excellent separability and strong classification power, while a value close to 0 indicates poor discriminative ability.*\n",
    "lity to find all actual positives. Use when FNs are costly.\n",
    "- A value close to 1.0 indicates most anomalies are detected, while a value close to 0 indicates many anomalies are missed.*\n",
    "\n",
    "\n",
    "### ðŸ“ˆ Regression Metrics (How much?)\n",
    "- **Mean Absolute Error (MAE):** The average error in the same units as the target.\n",
    "- **Mean Squared Error (MSE):** Averages the squared errors, heavily penalizing large mistakes.\n",
    "- **Root Mean Squared Error (RMSE):** The square root of MSE, putting the error back into the target's units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ff6e1b-f6f2-428c-b096-2a41fdb65ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3755515374.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Let's interpret the model's performance using actual metric values:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Understanding Evaluation Metrics with Example Values\n",
    "\n",
    "Let's interpret the model's performance using actual metric values:\n",
    "\n",
    "- **Accuracy = 0.92:**  \n",
    "  Represents the proportion of correctly predicted samples out of all samples.  \n",
    "  *A value close to 1.0 indicates the model makes mostly correct predictions, while a value close to 0 indicates frequent misclassifications.*\n",
    "\n",
    "- **Precision = 0.85:**  \n",
    "  Represents how many of the predicted positive (anomalous) cases are actually positive.  \n",
    "  *A value close to 1.0 indicates few false positives, while a value close to 0 indicates many normal samples are incorrectly flagged as anomalies.*\n",
    "\n",
    "- **Recall = 0.78:**  \n",
    "  Represents how many of the actual positive (anomalous) cases are correctly identified.  \n",
    "  *A value close to 1.0 indicates most anomalies are detected, while a value close to 0 indicates many anomalies are missed.*\n",
    "\n",
    "- **F1-Score = 0.81:**  \n",
    "  Represents the harmonic mean of precision and recall, balancing both false positives and false negatives.  \n",
    "  *A value close to 1.0 indicates a strong balance between precision and recall, while a value close to 0 indicates poor performance in one or both metrics.*\n",
    "\n",
    "- **AUC-ROC = 0.93:**  \n",
    "  Represents the model's ability to distinguish between normal and anomalous cases.  \n",
    "  *A value close to 1.0 indicates excellent separability and strong classification power, while a value close to 0 indicates poor discriminative ability.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06538744",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 3: Classification vs. Regression in Action (Approx. 30 mins)\n",
    "\n",
    "ðŸ“„ **Explanation: Predicting Categories vs. Numbers**\n",
    "\n",
    "Let's see the two main types of supervised learning in action and, more importantly, how we apply the metrics we just learned to evaluate them.\n",
    "\n",
    "### ðŸ”µ Classification Example & Evaluation\n",
    "- **Goal:** Predict a category (e.g., will a student pass or fail?).\n",
    "- **Evaluation:** We'll use Accuracy, Precision, Recall, and F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f922dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual values:    [0, 0, 0, 1, 1, 1]\n",
      "Predicted values: [0, 0, 0, 1, 1, 1]\n",
      "\n",
      "--- Classification Metrics ---\n",
      "Accuracy:  1.00\n",
      "Precision: 1.00\n",
      "Recall:    1.00\n",
      "F1-Score:  1.00\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 0]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: A Classification model with full evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Data: [hours_studied]\n",
    "X = [[1], [2], [4], [5], [7], [8]] \n",
    "# Target: [passed_exam] (0=Fail, 1=Pass)\n",
    "y_true = [0, 0, 0, 1, 1, 1]\n",
    "\n",
    "# 1. Create and train the model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X, y_true)\n",
    "\n",
    "# 2. Get predictions for the training data\n",
    "# (In a real project, you would use a separate test set!)\n",
    "y_pred = classifier.predict(X)\n",
    "\n",
    "print(f\"Actual values:    {y_true}\")\n",
    "print(f\"Predicted values: {list(y_pred)}\\n\")\n",
    "\n",
    "# 3. Evaluate the model using our metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"--- Classification Metrics ---\")\n",
    "print(f\"Accuracy:  {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall:    {recall:.2f}\")\n",
    "print(f\"F1-Score:  {f1:.2f}\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb13a5-5c9f-4bcb-8440-580c301e89a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b318bf18",
   "metadata": {},
   "source": [
    "### ðŸ”´ Regression Example & Evaluation\n",
    "- **Goal:** Predict a number (e.g., what is the salary?).\n",
    "- **Evaluation:** We'll use MAE, MSE, and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’» Example: A Regression model with full evaluation\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Data: [years_of_experience]\n",
    "X = [[1], [2], [3], [5], [8]]\n",
    "# Target: [salary]\n",
    "y_true = [45000, 55000, 60000, 85000, 110000]\n",
    "\n",
    "# 1. Create and train the model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y_true)\n",
    "\n",
    "# 2. Get predictions for the training data\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "print(\"--- Predictions vs Actuals ---\")\n",
    "for actual, pred in zip(y_true, y_pred):\n",
    "    print(f\"Actual: ${actual:<7,} | Predicted: ${pred:,.2f}\")\n",
    "\n",
    "# 3. Evaluate the model using our metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\n--- Regression Metrics ---\")\n",
    "print(f\"Mean Absolute Error (MAE):   ${mae:,.2f}\")\n",
    "print(f\"Mean Squared Error (MSE):    ${mse:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb71321",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 2\n",
    "\n",
    "For each scenario below, identify if it is a **Classification** or a **Regression** problem.\n",
    "\n",
    "1.  Predicting the number of likes a social media post will get.\n",
    "    - **Answer:** `Your Answer Here`\n",
    "2.  Determining if a credit card transaction is fraudulent.\n",
    "    - **Answer:** `Your Answer Here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec12958",
   "metadata": {},
   "source": [
    "*(Double-click here to see the solutions)*\n",
    "\n",
    "1. **Regression** (predicting a continuous number)\n",
    "2. **Classification** (predicting a discrete category: 'fraudulent' or 'not fraudulent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c1f6a",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 4: Dataset Imbalance (Approx. 20 mins)\n",
    "\n",
    "ðŸ“„ **Explanation: A Lopsided Dataset**\n",
    "\n",
    "A dataset is **imbalanced** when one class has far more examples than another (e.g., 99% non-fraud vs. 1% fraud). This is a problem because a lazy model can achieve high accuracy by just always predicting the majority class, making it useless.\n",
    "\n",
    "### Remedies for Imbalance\n",
    "1. **Oversampling (Add to the minority):** Create more examples of the minority class. A smart way to do this is with **SMOTE (Synthetic Minority Over-sampling Technique)**, which creates *new, synthetic* data points.\n",
    "\n",
    "2. **Undersampling (Remove from the majority):** Delete examples from the majority class. This is fast but can lead to information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e9b201-ec29-48ba-a9fe-c3243b1ff0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 943, 1: 57})\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’» Example: Using SMOTE to Balance a Dataset\n",
    "# You might need to install this library! Run: pip install imbalanced-learn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. Create a fake imbalanced dataset (95% vs 5%)\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, weights=[0.95, 0.05], random_state=1)\n",
    "print(f'Original dataset shape: {Counter(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad9577a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({1: 943, 0: 943})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Qasim\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "print(f'Resampled dataset shape: {Counter(y_resampled)}')\n",
    "\n",
    "# ðŸ’¡ Notice how the minority class (1) now has the same number of samples as the majority class (0)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd80a8b",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 3\n",
    "\n",
    "In your own words, why is SMOTE often a better choice than simply duplicating existing minority class data?\n",
    "\n",
    "*(Double-click this cell to write your answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28262425",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "SMOTE creates new, artificial data points that are similar to, but not identical to, existing ones. This provides the model with more varied examples to learn from. Simply duplicating data can lead to overfitting, where the model just memorizes the specific copied examples instead of learning the general pattern of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227e829",
   "metadata": {},
   "source": [
    "--- \n",
    "## ðŸŽ“ Final Revision Assignment (Approx. 30 mins)\n",
    "\n",
    "Congratulations on completing the core topics! Now it's time to put it all together. Use the cells below to solve these mini-problems.\n",
    "\n",
    "--- \n",
    "\n",
    "**Task 1: Problem Identification**\n",
    "\n",
    "A real estate company wants to build a model to predict the selling price of houses. Is this a Classification or a Regression problem? Why?\n",
    "\n",
    "*(Double-click to write your answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057f7aa",
   "metadata": {},
   "source": [
    "**Task 2: Choosing the Right Metric**\n",
    "\n",
    "For an airport security model that detects dangerous items, which metric is more important: **Precision** or **Recall**? Explain.\n",
    "\n",
    "*(Double-click to write your answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513208f8",
   "metadata": {},
   "source": [
    "**Task 3: The Imbalance Problem**\n",
    "\n",
    "An online service wants to predict user cancellations. Only 2% of users cancel each month. Why is a model with 98% accuracy potentially a very poor model?\n",
    "\n",
    "*(Double-click to write your answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f70dc8",
   "metadata": {},
   "source": [
    "**Task 4: Suggesting a Solution**\n",
    "\n",
    "Based on Task 3, name and briefly describe **one** technique to help the model learn better fromzm             the imbalanced dataset.\n",
    "\n",
    "*(Doubljhl.;/e-click to write your answer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9914689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Metric Calculation                               \n",
    "\n",
    "# A model's test results are: TP=120, FP=30, TN=800, FN=50\n",
    "# Calculate the Accuracy and Recall for this model.\n",
    "\n",
    "TP = 120\n",
    "FP = 30\n",
    "TN = 800\n",
    "FN = 50\n",
    "\n",
    "# Your code here\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "print(f\"Final Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Final Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Write a Unit Test\n",
    "# Below is a function that converts all text in a list to lowercase.\n",
    "# Write a simple unit test to make sure it works correctly.\n",
    "\n",
    "def to_lowercase(text_list):\n",
    "    return [text.lower() for text in text_list]\n",
    "\n",
    "def test_to_lowercase():\n",
    "    # Your code here\n",
    "    test_input = [\"Hello World\", \"PYTHON IS FUN\", \"MixedCase\"]\n",
    "    expected_output = [\"hello world\", \"python is fun\", \"mixedcase\"]\n",
    "    \n",
    "    actual_output = to_lowercase(test_input)\n",
    "    \n",
    "    assert actual_output == expected_output\n",
    "    print(\"âœ… Lowercase test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_to_lowercase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a3172",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully completed this crash course on building and evaluating machine learning models. You now have a foundational understanding of testing, metrics, model types, and handling imbalanced data. Keep practicing and exploring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
