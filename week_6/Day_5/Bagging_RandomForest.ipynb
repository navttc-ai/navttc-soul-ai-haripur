{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526ff4b3",
   "metadata": {},
   "source": [
    "# üå≥ Introduction to Decision Trees, Bagging, and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285623da",
   "metadata": {},
   "source": [
    "### üìò Welcome to Your First AI/ML Model Session!\n",
    "\n",
    "Welcome! In this 2-hour interactive session, we'll journey from a single predictive model to a powerful ensemble of models. We'll demystify some of the most fundamental and widely used algorithms in machine learning.\n",
    "\n",
    "**Our Learning Objectives for Today:**\n",
    "\n",
    "1.  **Understand Decision Trees:** Learn what a Decision Tree is, how it makes decisions, and how to build one in Python.\n",
    "2.  **Discover Ensemble Learning:** Grasp the idea of 'wisdom of the crowd' by learning about Bagging.\n",
    "3.  **Master Random Forests:** See how Random Forests improve upon Bagging to become one of the most effective ML models.\n",
    "4.  **Get Hands-On:** Apply these concepts with simple, fun coding exercises.\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262644f",
   "metadata": {},
   "source": [
    "## Topic 1: The Decision Tree üå≥\n",
    "\n",
    "A Decision Tree is just like a flowchart. It asks a series of questions about your data to arrive at a decision. It's one of the most intuitive and easy-to-understand models in machine learning!\n",
    "\n",
    "**How it works:**\n",
    "*   **Root Node:** The starting point, which contains all your data.\n",
    "*   **Decision Nodes:** The points where the tree asks a question (e.g., \"Is the outlook sunny?\").\n",
    "*   **Branches:** The paths that represent the answer to a question (e.g., 'Yes' or 'No').\n",
    "*   **Leaf Nodes:** The final endpoints, which give you the prediction (e.g., \"Play Tennis\").\n",
    "\n",
    "The tree learns the best questions to ask by finding splits that make the data in each branch as 'pure' or similar as possible. A common way to measure this is with **Gini Impurity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíª Code Example: Predicting Tennis Play\n",
    "\n",
    "# First, we need to import the necessary tools from scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Let's create the 'Play Tennis' dataset from our notes\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak'],\n",
    "    'Play Tennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Machine learning models need numbers, not text! So, we convert our text data into numbers.\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_encoded = df.apply(le.fit_transform)\n",
    "\n",
    "# Separate the features (X) from the target (y)\n",
    "X = df_encoded.drop('Play Tennis', axis=1)\n",
    "y = df_encoded['Play Tennis']\n",
    "\n",
    "# Create and train our Decision Tree model!\n",
    "tree_model = DecisionTreeClassifier(criterion='gini')\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Let's make a prediction! What if: Outlook=Sunny (2), Temp=Mild (2), Humidity=High (0), Wind=Weak (1)\n",
    "prediction_encoded = tree_model.predict([[2, 2, 0, 1]])\n",
    "\n",
    "# Convert the prediction back to text ('Yes' or 'No')\n",
    "prediction = le.inverse_transform(prediction_encoded)\n",
    "print(f\"Prediction for a Sunny, Mild, High Humidity, Weak Wind day: {prediction[0]} Tennis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1082d0e",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: Fruit Identifier\n",
    "\n",
    "Your turn! Let's create a model to guess if a fruit is an Apple or an Orange based on its weight and texture.\n",
    "\n",
    "1.  Create two lists: `features` (containing weight and texture) and `labels` (the fruit name).\n",
    "2.  Train a `DecisionTreeClassifier` on this data.\n",
    "3.  Predict what fruit has `weight=145` and `texture='Smooth' (1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Data: 0 for Bumpy, 1 for Smooth. 0 for Apple, 1 for Orange.\n",
    "features = [[140, 1], [130, 1], [150, 0], [170, 0]]\n",
    "labels = [0, 0, 1, 1] # Apple, Apple, Orange, Orange\n",
    "\n",
    "# 1. Create a Decision Tree Classifier\n",
    "fruit_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# 2. Train the classifier using the .fit() method\n",
    "# fruit_classifier.fit(..., ...)\n",
    "\n",
    "# 3. Predict for a fruit that weighs 145g and is Smooth (1)\n",
    "# prediction = fruit_classifier.predict([[145, 1]])\n",
    "# if prediction == 0:\n",
    "#     print(\"It's probably an Apple!\")\n",
    "# else:\n",
    "#     print(\"It's probably an Orange!\")\n",
    "\n",
    "print(\"Task not yet completed. Fill in the code above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb1325",
   "metadata": {},
   "source": [
    "## Topic 2: Bagging (Bootstrap Aggregating) üõçÔ∏è\n",
    "\n",
    "A single decision tree can sometimes learn the training data *too* well, a problem called **overfitting**. It might not perform well on new, unseen data.\n",
    "\n",
    "üí° **Idea:** What if we train *many* slightly different trees on slightly different versions of the data and let them vote on the final answer? This is the core idea of Bagging!\n",
    "\n",
    "**The Bagging Process:**\n",
    "1.  **Bootstrap:** Create many new datasets from the original one by sampling *with replacement*. Imagine you have a bag of marbles; you pick one, note its color, and *put it back*. You do this until your new bag is full. Some marbles will be picked multiple times, some not at all.\n",
    "2.  **Train:** Train a separate model (like a decision tree) on each of these new datasets in parallel.\n",
    "3.  **Aggregate (Vote):** For a new prediction, ask every model for its opinion. The final answer is the one that gets the most votes!\n",
    "\n",
    "The main goal of Bagging is to **reduce variance**, making the model more stable and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíª Code Example: Upgrading to a Bagging Classifier\n",
    "\n",
    "# We'll use the same tennis data as before.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X and y are already loaded and encoded from the first example.\n",
    "\n",
    "# We create a base decision tree - this is the model we will make copies of.\n",
    "base_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Now, we create the Bagging model.\n",
    "# n_estimators=100 means we will create 100 decision trees.\n",
    "bagging_model = BaggingClassifier(base_estimator=base_tree, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the bagging model\n",
    "bagging_model.fit(X, y)\n",
    "\n",
    "# Let's make the same prediction as before!\n",
    "# Outlook=Sunny (2), Temp=Mild (2), Humidity=High (0), Wind=Weak (1)\n",
    "prediction_encoded = bagging_model.predict([[2, 2, 0, 1]])\n",
    "prediction = le.inverse_transform(prediction_encoded)\n",
    "\n",
    "print(f\"Bagging Model Prediction: {prediction[0]} Tennis!\")\n",
    "print(\"By combining 100 trees, we get a more robust prediction.\")\n",
    "\n",
    "# üß™ Try changing the n_estimators value (e.g., to 10 or 500) and see if the prediction changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8ed19",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: Bagging the Fruits\n",
    "\n",
    "Now apply the Bagging technique to our fruit identifier problem. Will a committee of classifiers do better than a single one?\n",
    "\n",
    "Use `BaggingClassifier` with a `DecisionTreeClassifier` base to train on the fruit data. How does this compare to the single tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ba233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Data: 0 for Bumpy, 1 for Smooth. 0 for Apple, 1 for Orange.\n",
    "features = [[140, 1], [130, 1], [150, 0], [170, 0]]\n",
    "labels = [0, 0, 1, 1] # Apple, Apple, Orange, Orange\n",
    "\n",
    "# 1. Create the Bagging Classifier\n",
    "# Hint: Use n_estimators=10 to start\n",
    "# fruit_bagging_model = BaggingClassifier(..., n_estimators=10, random_state=42)\n",
    "\n",
    "# 2. Train the model\n",
    "# fruit_bagging_model.fit(..., ...)\n",
    "\n",
    "# 3. Predict for a fruit that weighs 145g and is Smooth (1)\n",
    "# prediction = fruit_bagging_model.predict([[145, 1]])\n",
    "# if prediction == 0:\n",
    "#     print(\"The committee of trees thinks it's an Apple!\")\n",
    "# else:\n",
    "#     print(\"The committee of trees thinks it's an Orange!\")\n",
    "    \n",
    "print(\"‚úÖ Well done on learning about Bagging! Just fill in the code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7117db",
   "metadata": {},
   "source": [
    "## Topic 3: The Random Forest üå≤üå≤üå≤\n",
    "\n",
    "The Random Forest is a tweaked, and often more powerful, version of Bagging. It's one of the most popular and effective off-the-shelf models you can use!\n",
    "\n",
    "It works just like Bagging, but with one extra trick:\n",
    "\n",
    "**‚ú® The Magic Ingredient: Feature Randomness ‚ú®**\n",
    "\n",
    "When a normal bagged tree decides to make a split (ask a question), it looks at *all* the available features (Outlook, Temperature, Humidity, Wind) to find the best one.\n",
    "\n",
    "In a Random Forest, when a tree wants to make a split, it is only allowed to look at a *random subset* of the features. For example, it might only be allowed to consider 'Humidity' and 'Wind' for one split, and 'Outlook' and 'Temperature' for another.\n",
    "\n",
    "**Why does this help?**\n",
    "It forces the trees to be more diverse and creative! If one feature is very strong (like 'Outlook'), Bagging might create 100 trees that all look very similar. By restricting the features, Random Forest ensures it builds many different kinds of trees, leading to a more robust and accurate final vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíª Code Example: Unleashing the Random Forest\n",
    "\n",
    "# scikit-learn makes this super easy with RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X and y are the same encoded tennis data\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "# n_estimators=100 means it will build 100 trees\n",
    "forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest_model.fit(X, y)\n",
    "\n",
    "# Let's make our prediction one last time\n",
    "# Outlook=Sunny (2), Temp=Mild (2), Humidity=High (0), Wind=Weak (1)\n",
    "prediction_encoded = forest_model.predict([[2, 2, 0, 1]])\n",
    "prediction = le.inverse_transform(prediction_encoded)\n",
    "\n",
    "print(f\"Random Forest Model Prediction: {prediction[0]} Tennis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89261a04",
   "metadata": {},
   "source": [
    "### üéØ Practice Task: A Forest for Fruits\n",
    "\n",
    "You know the drill! Apply the `RandomForestClassifier` to the fruit dataset. This is often the go-to model for classification tasks like this.\n",
    "\n",
    "Experiment with `n_estimators`. Does using 10 trees give a different result than using 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Data: 0 for Bumpy, 1 for Smooth. 0 for Apple, 1 for Orange.\n",
    "features = [[140, 1], [130, 1], [150, 0], [170, 0]]\n",
    "labels = [0, 0, 1, 1] # Apple, Apple, Orange, Orange\n",
    "\n",
    "# 1. Create the Random Forest Classifier with 100 estimators\n",
    "# fruit_forest_model = ...\n",
    "\n",
    "# 2. Train the model\n",
    "# fruit_forest_model.fit(..., ...)\n",
    "\n",
    "# 3. Predict for a fruit that weighs 145g and is Smooth (1)\n",
    "# prediction = ...\n",
    "# if prediction == 0:\n",
    "#     print(\"The Random Forest says it's an Apple!\")\n",
    "# else:\n",
    "#     print(\"The Random Forest says it's an Orange!\")\n",
    "    \n",
    "print(\"Task not yet completed. Give it a try!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6a8b1",
   "metadata": {},
   "source": [
    "## üß† Final Revision Assignment\n",
    "\n",
    "Congratulations on learning about three powerful machine learning models! Now it's time to put all your new knowledge to the test with a slightly bigger, more realistic dataset: the famous Iris dataset.\n",
    "\n",
    "This dataset contains measurements for 3 different species of Iris flowers. Your goal is to build models to predict the species based on the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load the dataset from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# We split the data so we can train on one part and test on another\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Dataset is loaded and split. Ready for your models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f102501",
   "metadata": {},
   "source": [
    "**Assignment Tasks:**\n",
    "\n",
    "1.  **Task 1: The Single Tree.** Train a `DecisionTreeClassifier` on the `X_train` and `y_train` data.\n",
    "2.  **Task 2: Evaluate the Tree.** Use your trained tree to make predictions on `X_test`. Calculate its accuracy using the `accuracy_score` function.\n",
    "3.  **Task 3: The Bagging Committee.** Train a `BaggingClassifier` (with a Decision Tree base) on the training data.\n",
    "4.  **Task 4: Evaluate Bagging.** Predict on `X_test` with the bagging model and calculate its accuracy.\n",
    "5.  **Task 5: The Mighty Forest.** Train a `RandomForestClassifier` on the training data.\n",
    "6.  **Task 6: Evaluate the Forest.** Predict on `X_test` with the random forest and calculate its accuracy. Which model performed the best?\n",
    "7.  **Task 7 (Conceptual):** A bank wants to predict loan defaults. Why might a Random Forest be a better choice than a single Decision Tree for this important task? (Write your answer in a markdown cell or as a comment in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Code for the Final Assignment ---\n",
    "\n",
    "# Task 1 & 2: Decision Tree\n",
    "print(\"--- Decision Tree ---\")\n",
    "# your_tree = DecisionTreeClassifier(random_state=42)\n",
    "# your_tree.fit(X_train, y_train)\n",
    "# tree_preds = your_tree.predict(X_test)\n",
    "# tree_accuracy = accuracy_score(y_test, tree_preds)\n",
    "# print(f\"Decision Tree Accuracy: {tree_accuracy:.2f}\")\n",
    "\n",
    "# Task 3 & 4: Bagging Classifier\n",
    "print(\"\\n--- Bagging Classifier ---\")\n",
    "# your_bagging = BaggingClassifier(n_estimators=100, random_state=42)\n",
    "# your_bagging.fit(X_train, y_train)\n",
    "# bagging_preds = your_bagging.predict(X_test)\n",
    "# bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
    "# print(f\"Bagging Accuracy: {bagging_accuracy:.2f}\")\n",
    "\n",
    "# Task 5 & 6: Random Forest Classifier\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "# your_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# your_forest.fit(X_train, y_train)\n",
    "# forest_preds = your_forest.predict(X_test)\n",
    "# forest_accuracy = accuracy_score(y_test, forest_preds)\n",
    "# print(f\"Random Forest Accuracy: {forest_accuracy:.2f}\")\n",
    "\n",
    "# Task 7 (Conceptual Answer as a comment):\n",
    "# A Random Forest is better for a bank because..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b737621",
   "metadata": {},
   "source": [
    "## üéâ You've Completed the Session!\n",
    "\n",
    "Fantastic work! You've gone from the basics of a single Decision Tree to understanding and implementing powerful ensemble methods like Bagging and Random Forests. These are essential tools for any data scientist or AI practitioner.\n",
    "\n",
    "Keep experimenting and happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
