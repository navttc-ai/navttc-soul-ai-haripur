{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739ef122",
   "metadata": {},
   "source": [
    "# üìò Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Welcome to your first class on **Natural Language Processing**! üöÄ\n",
    "\n",
    "In this 2-hour session, we will explore how computers understand human language. We will cover the linguistic basics, the history of NLP, and get hands-on with Python to clean and prepare text data.\n",
    "\n",
    "### üéØ Learning Objectives:\n",
    "1.  Understand what NLP is and its goals.\n",
    "2.  Learn the 4 pillars of linguistics: Syntax, Semantics, Pragmatics, and Discourse.\n",
    "3.  Explore the evolution of NLP (The 3 Curves).\n",
    "4.  Perform practical data pre-processing using Python libraries **NLTK** and **SpaCy**.\n",
    "\n",
    "Let's get started! üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90908d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e912341",
   "metadata": {},
   "source": [
    "## 1. What is Natural Language Processing? ü§ñüí¨\n",
    "\n",
    "**Natural Language Processing (NLP)** is a field of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a meaningful way.\n",
    "\n",
    "It bridges the gap between how humans communicate and how computers understand data.\n",
    "\n",
    "### The Goal\n",
    "The ultimate goal is to read, decipher, and make sense of human language to extract valuable insights from unstructured data like emails, social media, and news articles.\n",
    "\n",
    "### üìà Common Applications\n",
    "You use NLP every day! Here are some examples:\n",
    "*   **Search Engines:** Google uses NLP to understand what you are looking for.\n",
    "*   **Virtual Assistants:** Siri and Alexa process your voice commands.\n",
    "*   **Machine Translation:** Google Translate converts text between languages.\n",
    "*   **Chatbots:** Automated customer support agents.\n",
    "*   **Sentiment Analysis:** Determining if a review is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06842d",
   "metadata": {},
   "source": [
    "### üß† Practice Task 1\n",
    "\n",
    "Think of one technology you used today that might use NLP. Double-click this cell and write it down below:\n",
    "\n",
    "**My Answer:** [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecb9f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5413b",
   "metadata": {},
   "source": [
    "## 2. The Pillars of Language: How Computers Analyze Text üèõÔ∏è\n",
    "\n",
    "To process language like a human, a machine needs to understand it at different levels. In linguistics, we categorize these into four key areas:\n",
    "\n",
    "1.  **Syntax (Grammar):** The rules governing the structure of sentences.\n",
    "    *   *Example:* \"The cat sat on the mat\" is grammatically correct. \"Sat the mat on cat the\" is not.\n",
    "\n",
    "2.  **Semantics (Literal Meaning):** The meaning of words and sentences independent of context.\n",
    "    *   *Example:* \"The cat sat on the mat\" and \"On the mat sat the cat\" have different structures (syntax) but the same literal meaning (semantics).\n",
    "\n",
    "3.  **Pragmatics (Context & Intent):** How context influences meaning. It looks beyond the literal definition to understand sarcasm, irony, or requests.\n",
    "\n",
    "4.  **Discourse (The Bigger Picture):** How sentences connect to form a coherent conversation or text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4f9dd",
   "metadata": {},
   "source": [
    "### üí° Example in Action\n",
    "\n",
    "Consider this conversation:\n",
    "> **Person A:** \"Can you pass the salt?\"\n",
    "> **Person B:** \"Yeah, sure.\" *(Passes the salt)*\n",
    "\n",
    "Let's analyze this:\n",
    "*   **Syntax:** Both sentences follow English grammar rules.\n",
    "*   **Semantics:** Literally, Person A is asking if Person B has the physical ability to pass the salt.\n",
    "*   **Pragmatics:** Person B understands that this isn't a question about ability, but a **request** to perform an action.\n",
    "*   **Discourse:** Person B's response is directly related to Person A's question, creating a coherent exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad6b0a",
   "metadata": {},
   "source": [
    "### üß† Practice Task 2 (Quick Quiz)\n",
    "\n",
    "Which of the following best describes the role of **Pragmatics** in NLP? (Delete the wrong answers below)\n",
    "\n",
    "a) Analyzing the grammatical structure of sentences.\n",
    "b) Understanding the literal meaning of words.\n",
    "c) Interpreting meaning based on context and intent.\n",
    "d) Identifying the root form of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d12a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7f9db",
   "metadata": {},
   "source": [
    "## 3. The Evolution of NLP: The Three Curves üìâüìà\n",
    "\n",
    "NLP has evolved through three overlapping \"curves\" or eras:\n",
    "\n",
    "1.  **The Syntactics Curve (The Past):** Rule-based systems focused on grammar. They were rigid and struggled with the ambiguity of real-world language (e.g., \"bag-of-words\" models).\n",
    "\n",
    "2.  **The Semantics Curve (The Present):** The era of Machine Learning and Deep Learning. Models use statistics to learn the meaning of words from data. A hallmark of this is **Word Embeddings**, where mathematically similar words are grouped together.\n",
    "\n",
    "3.  **The Pragmatics Curve (The Future):** Focuses on context and intent. Large Language Models (LLMs) like GPT are pushing boundaries here, aiming for systems that can reason and engage in natural conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6af08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf4187",
   "metadata": {},
   "source": [
    "## 4. practical NLP: Data Pre-processing üßπ\n",
    "\n",
    "Raw text data is often \"noisy\" and unstructured. Before an AI model can understand it, we must clean it. This is called **Data Pre-processing**.\n",
    "\n",
    "We will use two popular Python libraries:\n",
    "1.  **NLTK (Natural Language Toolkit):** Great for teaching, research, and understanding the basics.\n",
    "2.  **SpaCy:** Modern, fast, and designed for production use.\n",
    "\n",
    "### ‚öôÔ∏è Setup (Run this cell once)\n",
    "First, let's prepare our environment by downloading necessary data for NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK library\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (only needs to be done once)\n",
    "# 'punkt' is for splitting sentences and words\n",
    "# 'stopwords' is a list of common words like 'the', 'is', 'a'\n",
    "print(\"‚¨áÔ∏è Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691b88b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac46ed",
   "metadata": {},
   "source": [
    "## 5. Noise Removal with NLTK üßº\n",
    "\n",
    "**Noise** in text includes:\n",
    "*   **Stopwords:** Common words (the, a, is) that carry little semantic meaning.\n",
    "*   **Punctuation:** Marks like commas and periods.\n",
    "\n",
    "Removing noise helps the AI focus on the important words.\n",
    "\n",
    "Let's look at the step-by-step process using NLTK:\n",
    "1.  **Tokenization:** Splitting text into individual words (tokens).\n",
    "2.  **Lowercasing:** Making everything lowercase so \"The\" and \"the\" are treated the same.\n",
    "3.  **Stopword & Punctuation Removal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ea37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Our raw text\n",
    "text = \"This is an example sentence, showing off the stop words filtration!\"\n",
    "print(\"ORIGINAL:\", text)\n",
    "\n",
    "# 1. Tokenization (splitting into words)\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# 2. Lowercasing\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# 3. Prepare Stopwords and Punctuation lists\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# 4. Filtering (keeping words NOT in stop_words AND NOT in punctuation)\n",
    "filtered_tokens = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words and word not in punct:\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "print(\"CLEANED: \", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efba7a",
   "metadata": {},
   "source": [
    "### üß† Practice Task 3\n",
    "\n",
    "In the cell below, change the `my_text` variable to a sentence of your choice, then run the cell to see how NLTK cleans it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2998c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Try changing this sentence!\n",
    "my_text = \"NLP is amazing, but it requires a lot of data processing.\"\n",
    "\n",
    "# --- Processing logic (same as above) ---\n",
    "tokens = word_tokenize(my_text)\n",
    "tokens = [word.lower() for word in tokens]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Using a 'list comprehension' for cleaner code\n",
    "final_clean_words = [word for word in tokens if word not in stop_words and word not in punct]\n",
    "\n",
    "print(f\"Original: {my_text}\")\n",
    "print(f\"Result:   {final_clean_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740808a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bcd0672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Qasim\\anaconda3\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "# Run this once to download the SpaCy English model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81021f71",
   "metadata": {},
   "source": [
    "Now, let's see how SpaCy handles noise removal. Notice how much shorter the code is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846080eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Load the English language model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence, showing off the stop words filtration!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORIGINAL:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     53\u001b[0m         name,\n\u001b[0;32m     54\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     55\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     56\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     57\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     58\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     59\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\util.py:484\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 1. Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence, showing off the stop words filtration!\"\n",
    "print(\"ORIGINAL:\", text)\n",
    "\n",
    "# 2. Process the text with SpaCy (it tokenizes automatically)\n",
    "doc = nlp(text)\n",
    "\n",
    "# 3. Noise Removal in one line using SpaCy's attributes\n",
    "# We check if a token is NOT a stopword AND is NOT punctuation\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "print(\"CLEANED: \", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf567cf",
   "metadata": {},
   "source": [
    "### üß† Practice Task 4\n",
    "\n",
    "Create a sentence with **lots** of punctuation and run it through SpaCy in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d05a3e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m messy_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello!!! Note: NLP is fun... #AI @Python.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process it\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(messy_text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Clean it\u001b[39;00m\n\u001b[0;32m      8\u001b[0m clean_list \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_punct]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Write a sentence with many symbols: ! @ # $ , .\n",
    "messy_text = \"Hello!!! Note: NLP is fun... #AI @Python.\"\n",
    "\n",
    "# Process it\n",
    "doc = nlp(messy_text)\n",
    "\n",
    "# Clean it\n",
    "clean_list = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "print(clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f392e5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab67c68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134a7d8",
   "metadata": {},
   "source": [
    "## üéì Final Revision Assignment\n",
    "\n",
    "Great job today! To wrap up the last 20 minutes, complete these tasks to reinforce what you've learned.\n",
    "\n",
    "### Task 1: Concept Check\n",
    "Explain the difference between **Stemming** and **Lemmatization**. How would each process the word \"better\"?\n",
    "> *Write your answer here:*\n",
    "\n",
    "### Task 2: Case Study\n",
    "Imagine you are building a customer service chatbot. Why is understanding **Pragmatics** crucial? Give an example of a customer query where literal meaning is different from intended meaning.\n",
    "> *Write your answer here:*\n",
    "\n",
    "### Task 3: NLTK Coding Challenge\n",
    "Write code below to process the sentence: `\"The quick brown fox jumps over the lazy dog.\"`\n",
    "1. Tokenize it.\n",
    "2. Convert to lowercase.\n",
    "3. Remove stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529db3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your NLTK solution here\n",
    "challenge_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Hint: Use the code from Section 5 as a reference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# üß™ Change this text to include different names and places\n",
    "text = \"Google was founded by Larry Page and Sergey Brin in California.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Analyzing: '{text}'\\n\")\n",
    "\n",
    "# Loop through recognized entities\n",
    "for entity in doc.ents:\n",
    "    print(f\"Word: {entity.text} | Label: {entity.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c60a7",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You have completed the **Introduction to NLP** class. You now understand the linguistic pillars, the history of the field, and how to use Python to clean text data.\n",
    "\n",
    "Keep practicing, and happy coding! üêç"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
