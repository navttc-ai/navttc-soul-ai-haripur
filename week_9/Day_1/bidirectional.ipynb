{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc107610",
   "metadata": {},
   "source": [
    "# üß† Introduction to Bidirectional LSTMs for AI Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dd9f9",
   "metadata": {},
   "source": [
    "### Welcome to Your 2-Hour Journey into Sequence Models! üöÄ\n",
    "\n",
    "Today, we're diving into one of the coolest advancements in AI for understanding sequences: **Bidirectional Recurrent Neural Networks (Bi-RNNs)** and **Bidirectional LSTMs (Bi-LSTMs)**.\n",
    "\n",
    "Imagine trying to understand a sentence by reading it only one word at a time, from left to right. Sometimes you need the whole sentence to get the full picture, right? That's exactly what Bi-LSTMs help an AI model do!\n",
    "\n",
    "#### üìò Learning Objectives for This Session:\n",
    "\n",
    "By the end of this 2-hour session, you will be able to:\n",
    "1.  **Understand** what sequential data is and how basic RNNs work.\n",
    "2.  **Identify** the limitations of looking at data in only one direction.\n",
    "3.  **Explain** how a Bidirectional RNN works by looking both forward and backward.\n",
    "4.  **Build** a simple Bi-LSTM model using Python and TensorFlow/Keras.\n",
    "5.  **Recognize** real-world applications where Bi-LSTMs are powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bf593",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7632bf",
   "metadata": {},
   "source": [
    "## Topic 1: What are RNNs? (A Quick Recap)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a special type of neural network designed for **sequential data**‚Äîdata where the order matters!\n",
    "\n",
    "Think about:\n",
    "- A sentence (a sequence of words)\n",
    "- A song (a sequence of notes)\n",
    "- Stock prices over time (a sequence of numbers)\n",
    "\n",
    "RNNs have a 'memory' (called a **hidden state**) that allows them to remember what came before. This memory helps them understand the context as they process each item in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b490664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing word: 'I' -> RNN memory is updated!\n",
      "Processing word: 'am' -> RNN memory is updated!\n",
      "Processing word: 'learning' -> RNN memory is updated!\n",
      "Processing word: 'AI' -> RNN memory is updated!\n"
     ]
    }
   ],
   "source": [
    "# This is a conceptual example. We won't run this code.\n",
    "# It just shows the idea of an RNN processing a sequence.\n",
    "\n",
    "sentence = [\"I\", \"am\", \"learning\", \"AI\"] \n",
    "memory = None # Start with no memory\n",
    "\n",
    "for word in sentence:\n",
    "    # The RNN processes the word and updates its memory\n",
    "    # memory = rnn_process(word, memory)\n",
    "    print(f\"Processing word: '{word}' -> RNN memory is updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3c3c3",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 1: Spot the Sequence!\n",
    "\n",
    "Think of three examples of sequential data you encounter in your daily life. Write them down in the cell below! (Double-click to edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a05da3",
   "metadata": {},
   "source": [
    "1.  *Your example 1 here (e.g., The steps in a recipe)*\n",
    "2.  *Your example 2 here (e.g., A conversation in a chat app)*\n",
    "3.  *Your example 3 here (e.g., The daily weather forecast for a week)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912abb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47692c0",
   "metadata": {},
   "source": [
    "## Topic 2: The Problem with Only Looking Forward ü§î\n",
    "\n",
    "A standard RNN (a *unidirectional* RNN) reads a sequence from start to finish. This is a problem because sometimes the meaning of a word depends on what comes *after* it.\n",
    "\n",
    "**Example:** Consider the sentence:\n",
    "> \"The athlete picked up the **bat** and swung.\"\n",
    "\n",
    "When a unidirectional RNN sees the word `bat`, it only knows \"The athlete picked up the...\". It doesn't know the word \"swung\" is coming up. It might incorrectly think of a flying animal instead of a piece of sports equipment!\n",
    "\n",
    "This is a major limitation. To truly understand context, we need to look both ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb05bf",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 2: Create Some Confusion!\n",
    "\n",
    "Your turn! In the markdown cell below, write a short sentence where the meaning of a word is unclear until you read the end of the sentence. This will show why looking forward is so important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435954c5",
   "metadata": {},
   "source": [
    "*Your sentence here... (e.g., He started to bank the plane sharply to the left.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c56f28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c56e7d",
   "metadata": {},
   "source": [
    "## Topic 3: The Bidirectional Solution! ‚ÜîÔ∏è\n",
    "\n",
    "A **Bidirectional RNN** solves this problem in a clever way. It uses two separate RNN layers:\n",
    "\n",
    "1.  **Forward Layer:** Processes the sequence from left-to-right (the normal way).\n",
    "2.  **Backward Layer:** Processes the sequence from right-to-left (in reverse!).\n",
    "\n",
    "At each word, the model combines the information from BOTH layers. This gives it a super-rich understanding of the context from the past and the future.\n",
    "\n",
    "üí° **Real-World Analogy:**\n",
    "Think about the sentence: \"After a long walk, he sat on the river **bank**.\"\n",
    "- The **forward** pass sees `...river bank` and might be unsure.\n",
    "- The **backward** pass sees `bank river...` and gets a strong clue.\n",
    "- By combining them, the model becomes very confident that `bank` means the side of a river, not a place for money!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dffc05",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 3: Think Like a Bi-RNN\n",
    "\n",
    "Imagine a Bi-RNN is processing this sentence: **\"Apple announced a new iPhone today.\"**\n",
    "\n",
    "When it's looking at the word `Apple`, what context does the **backward pass** give it? Write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916d1f7",
   "metadata": {},
   "source": [
    "*Your answer here... (Hint: The backward pass provides context like '...announced a new iPhone today', which helps the model know 'Apple' is a company, not a fruit!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639878c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1f471",
   "metadata": {},
   "source": [
    "## Topic 4: Adding a Better Memory: Bidirectional LSTMs (Bi-LSTMs)\n",
    "\n",
    "Standard RNNs have a problem: they can have a short memory. They sometimes forget information from the beginning of a long sequence. This is called the **vanishing gradient problem**.\n",
    "\n",
    "To fix this, researchers created **Long Short-Term Memory (LSTM)** cells. LSTMs are a special type of RNN with 'gates' that allow them to remember or forget information selectively, so they can capture long-term dependencies.\n",
    "\n",
    "A **Bi-LSTM** is simply a Bidirectional RNN that uses LSTM cells. It's the best of both worlds:\n",
    "- **LSTM:** Provides a powerful, long-term memory.\n",
    "- **Bidirectional:** Provides context from both the past and the future.\n",
    "\n",
    "This combination is extremely powerful for most sequence tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687063fa",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 4: Quick Quiz!\n",
    "\n",
    "What is the primary advantage of a Bidirectional RNN over a unidirectional RNN?\n",
    "a) It is computationally less expensive.\n",
    "b) It can process sequences of any length.\n",
    "c) It considers both past and future context to make predictions.\n",
    "d) It is less prone to the vanishing gradient problem.\n",
    "\n",
    "*Write your answer (a, b, c, or d) in the cell below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00562b",
   "metadata": {},
   "source": [
    "My Answer: c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a6025",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21959246",
   "metadata": {},
   "source": [
    "## Topic 5: üíª Let's Code! Building a Bi-LSTM in TensorFlow/Keras\n",
    "\n",
    "Now for the fun part! Let's see how easy it is to build a Bi-LSTM model for sentiment analysis (classifying text as positive or negative). We'll use the popular `TensorFlow` and `Keras` libraries.\n",
    "\n",
    "Don't worry if you don't understand every single detail. The goal is to see the main building blocks in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to install TensorFlow if you don't have it\n",
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f4d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 128)         98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,424,257\n",
      "Trainable params: 1,424,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # 1. Embedding Layer: Turns words (represented by numbers) into vectors.\n",
    "    # Think of this as a smart dictionary that learns word meanings.\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "\n",
    "    # 2. First Bi-LSTM Layer: This is the core of our model!\n",
    "    # We wrap an LSTM layer in 'Bidirectional()'. Keras handles the rest.\n",
    "    # 'return_sequences=True' is important because we want to pass the full sequence to the next layer.\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    \n",
    "    # 3. Second Bi-LSTM Layer: We can stack them for more power!\n",
    "    # This time, return_sequences is False (the default) because we are feeding into a flat Dense layer next.\n",
    "    Bidirectional(LSTM(32)),\n",
    "\n",
    "    # 4. Dense Layers: Standard neural network layers for final classification.\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') # Sigmoid is used for binary (0 or 1) classification.\n",
    "])\n",
    "\n",
    "# Compile the model so it's ready for training\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model's architecture. It's like an X-ray of our AI!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539d7bc",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 5: Experiment!\n",
    "\n",
    "üß™ **Your mission:** Modify the code above!\n",
    "\n",
    "1.  Copy the code from the cell above into the cell below.\n",
    "2.  Change the number of units in the second `Bidirectional(LSTM(32))` layer from `32` to `16`.\n",
    "3.  Re-run the `model.summary()`.\n",
    "\n",
    "**Question:** Look at the `Total params` in the summary. Did the total number of parameters in the model increase or decrease? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2911b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the code here and make your change!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502e242",
   "metadata": {},
   "source": [
    "‚úÖ **Well done!** You've just taken your first step in modifying a neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3c0c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec0eff",
   "metadata": {},
   "source": [
    "## Topic 6: Where are Bi-LSTMs Used? üåé\n",
    "\n",
    "Bi-LSTMs are superstars in tasks where the entire sequence context is crucial. Here are some key applications:\n",
    "\n",
    "- **Natural Language Processing (NLP):**\n",
    "  - **Sentiment Analysis:** Is a movie review positive or negative?\n",
    "  - **Named Entity Recognition (NER):** Finding names, places, and organizations in a text.\n",
    "  - **Machine Translation:** Translating a sentence from one language to another.\n",
    "  - **Part-of-Speech Tagging:** Identifying if a word is a noun, verb, adjective, etc.\n",
    "\n",
    "- **Speech Recognition:** Used in systems like Siri or Google Assistant to transcribe spoken words into text.\n",
    "\n",
    "- **Bioinformatics:** Analyzing sequences of DNA, RNA, or proteins.\n",
    "\n",
    "**Important Note:** Bi-LSTMs are great when you have the whole sequence available. They are **not** suitable for real-time prediction tasks (like predicting the next word as you type), because you don't have access to the 'future' data yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d5fd73",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 6: Match the Task!\n",
    "\n",
    "Why would a Bi-LSTM be a good choice for **Named Entity Recognition (NER)**? For example, in the sentence: \"Yesterday, **Paris** showed why she loves the city of **Paris**.\"\n",
    "\n",
    "Explain in the cell below how looking forward and backward helps the model tell the difference between the two `Paris` words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe084f8",
   "metadata": {},
   "source": [
    "*Your explanation here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559299c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ac111",
   "metadata": {},
   "source": [
    "## üèÜ Final Revision Assignment (Home Practice)\n",
    "\n",
    "Congratulations on making it through the session! Now it's time to put all your new knowledge together. These tasks are designed to help you revise and strengthen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaefdc",
   "metadata": {},
   "source": [
    "**Task 1: The Core Idea**\n",
    "In your own words, explain the main difference between a unidirectional LSTM and a bidirectional LSTM. Why is this difference so important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e61513",
   "metadata": {},
   "source": [
    "**Task 2: Code Comments**\n",
    "Copy the final model code from Topic 5 into the code cell below. Add a new comment `# Your comment here` to every single line of code, explaining what that line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the model code here and add your comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e4bbd",
   "metadata": {},
   "source": [
    "**Task 3: Simplify the Model**\n",
    "Modify the model from Task 2. Remove the first `Bidirectional(LSTM(64, ...))` layer, so that the model only has one Bi-LSTM layer. Then, print the model summary. How does this change the number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c33145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your simplified model code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211f22b",
   "metadata": {},
   "source": [
    "**Task 4: Explore a Related Concept (GRU)**\n",
    "A **Gated Recurrent Unit (GRU)** is a slightly simpler version of an LSTM. Search online for \"Keras GRU layer\". How would you change `Bidirectional(LSTM(32))` to use a `GRU` cell instead? Write the line of code in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf08542",
   "metadata": {},
   "source": [
    "*Your single line of code here... (e.g., `Bidirectional(GRU(32)) `)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617772f",
   "metadata": {},
   "source": [
    "**Task 5: When NOT to Use a Bi-LSTM**\n",
    "The notebook mentioned that Bi-LSTMs are not good for real-time tasks. Give an example of a real-time prediction task and explain why a Bi-LSTM would fail at it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee4b0e",
   "metadata": {},
   "source": [
    "**Task 6: Brainstorm a New Application**\n",
    "Think of a new, creative application where a Bi-LSTM could be useful. It could be anything from analyzing poetry to predicting player movements in sports. Describe the application and explain why context from both directions would be important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
