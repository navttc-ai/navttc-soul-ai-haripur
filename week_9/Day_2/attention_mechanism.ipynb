{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7069728a",
   "metadata": {},
   "source": [
    "# üöÄ Introduction to the Attention Mechanism for AI Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9849a0",
   "metadata": {},
   "source": [
    "### Welcome to Your 2-Hour Guide to Attention! üß†\n",
    "\n",
    "Hello and welcome! In the next two hours, we're going to explore one of the most powerful and exciting ideas in modern AI: the **Attention Mechanism**.\n",
    "\n",
    "Imagine reading a long book. To understand the story, you don't pay equal attention to every single word. Instead, you focus on the most important words and phrases that give context. That's exactly what the attention mechanism helps AI models do!\n",
    "\n",
    "This technique revolutionized fields like machine translation and text summarization and is the core building block of famous models like GPT and BERT.\n",
    "\n",
    "--- \n",
    "### üìò Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1.  **Understand** what the attention mechanism is and why it's so important.\n",
    "2.  **Identify** the key components of attention: Queries, Keys, and Values.\n",
    "3.  **Explore** a simplified code example of self-attention.\n",
    "4.  **Learn** about different types of attention like Multi-Head Attention.\n",
    "5.  **Recognize** the real-world applications of attention in NLP, computer vision, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d938b4",
   "metadata": {},
   "source": [
    "## Topic 1: What is Attention? The Big Idea üí°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e69df",
   "metadata": {},
   "source": [
    "The attention mechanism is a technique that allows a model to **focus on the most relevant parts of the input** when making a prediction. \n",
    "\n",
    "Older models, like Recurrent Neural Networks (RNNs), had to process information in a strict sequence, trying to cram the meaning of a whole sentence into a single memory state. This was like trying to remember the beginning of a very long story by the time you reach the end ‚Äì it's tough! They often forgot important early details.\n",
    "\n",
    "Attention solves this by giving the model the ability to \"look back\" at the entire input at every step. It dynamically decides which parts of the input are the most important for the current task.\n",
    "\n",
    "**Conceptual Example: Machine Translation**\n",
    "\n",
    "Imagine translating this sentence:\n",
    "*   **English:** \"The cat sat on the mat.\"\n",
    "*   **French:** \"Le chat s'est assis sur le tapis.\"\n",
    "\n",
    "When the model is generating the French word `chat` (cat), the attention mechanism would assign a **high weight** (high importance) to the English word `cat`. Similarly, when generating `tapis` (mat), it would focus heavily on the word `mat`. This allows for a much more accurate and context-aware translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cedbd",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 1: Your Own Words\n",
    "\n",
    "In the cell below, write a short explanation (1-2 sentences) of why a model translating a long document would benefit from an attention mechanism. Think about the 'forgetting' problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53be95",
   "metadata": {},
   "source": [
    "# Double-click here and write your answer! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a5eea",
   "metadata": {},
   "source": [
    "## Topic 2: How Attention Works - Queries, Keys & Values (Q, K, V) üîë"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db313e",
   "metadata": {},
   "source": [
    "The magic of attention comes from three special components that are created from our input data:\n",
    "\n",
    "1.  **Queries (Q):** Think of this as the **current question** or what the model is currently focused on. For example, if we are translating a sentence, the query could represent the word we are about to generate.\n",
    "\n",
    "2.  **Keys (K):** These are like **labels or keywords** for all the words in the input. Each word has a key that describes what kind of information it holds.\n",
    "\n",
    "3.  **Values (V):** These contain the **actual information** or meaning of each input word. They are the substance we want to draw from.\n",
    "\n",
    "The process works like searching for a video online:\n",
    "*   You have a **Query** (e.g., \"funny cat videos\").\n",
    "*   The search engine matches your query against the **Keys** (the titles and descriptions of all videos).\n",
    "*   It finds the best matches and returns the **Values** (the actual videos!).\n",
    "\n",
    "Attention does something similar: it uses the Query to score how well it matches each Key. These scores are then used to create a weighted sum of all the Values, giving us a **context vector** that is perfectly tailored to our query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0c8a6",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 2: Match the Concepts\n",
    "\n",
    "Match the component with its description:\n",
    "\n",
    "**Components:** `Query`, `Key`, `Value`\n",
    "\n",
    "**Descriptions:**\n",
    "A. The actual information or meaning of an input element.\n",
    "B. The current focus or 'question' being asked.\n",
    "C. A 'label' used to identify and match the input element.\n",
    "\n",
    "*Write your answers (e.g., Query = B) in the cell below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a813867",
   "metadata": {},
   "source": [
    "# Double-click here and write your answer!\n",
    "# Query = ?\n",
    "# Key = ?\n",
    "# Value = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd805d10",
   "metadata": {},
   "source": [
    "## Topic 3: Self-Attention in Action (with Code!) üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b16d44",
   "metadata": {},
   "source": [
    "A very powerful and popular type of attention is **self-attention**. Here, the input sequence attends *to itself*! \n",
    "\n",
    "This means the queries, keys, and values all come from the **same input sequence**. It allows the model to understand the internal relationships within a sentence. For example, in the sentence \"The robot picked up the ball because **it** was heavy,\" self-attention helps the model figure out that **\"it\"** refers to the **\"ball\"** and not the **\"robot\"**.\n",
    "\n",
    "Let's see a simplified version of this in code using Python's `numpy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"A function to convert scores into probabilities.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(x, W_q, W_k, W_v):\n",
    "    \"\"\"A simplified implementation of self-attention.\"\"\"\n",
    "    # Step 1: Create Queries, Keys, and Values from the input 'x'\n",
    "    # The '@' symbol is for matrix multiplication in numpy\n",
    "    Q = x @ W_q\n",
    "    K = x @ W_k\n",
    "    V = x @ W_v\n",
    "\n",
    "    # Step 2: Calculate attention scores by matching Queries and Keys\n",
    "    # We scale the scores to make training more stable\n",
    "    d_k = K.shape[-1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "\n",
    "    # Step 3: Apply softmax to turn scores into attention weights (probabilities)\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # Step 4: Compute the final output by taking a weighted sum of the Values\n",
    "    output = attention_weights @ V\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Let's imagine we have a sentence with 4 words\n",
    "sequence_length = 4\n",
    "input_dim = 3 # Each word is represented by a vector of size 3\n",
    "d_k = 2  # Dimension of keys/queries\n",
    "d_v = 2  # Dimension of values\n",
    "\n",
    "# Create a random input sequence (in a real model, these would be word embeddings)\n",
    "x = np.random.randn(sequence_length, input_dim)\n",
    "\n",
    "# Create random weight matrices (these are learned during model training)\n",
    "W_q = np.random.randn(input_dim, d_k)\n",
    "W_k = np.random.randn(input_dim, d_k)\n",
    "W_v = np.random.randn(input_dim, d_v)\n",
    "\n",
    "# Get the output and the attention weights!\n",
    "output, attention_weights = self_attention(x, W_q, W_k, W_v)\n",
    "\n",
    "print(\"Input Shape:\", x.shape)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"\\n--- Attention Weights Matrix ---\")\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70a8ec",
   "metadata": {},
   "source": [
    "The `Attention Weights Matrix` above is the most interesting part! The value at `[row, column]` shows how much word `row` pays attention to word `column`. A high value means high importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de557fe5",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 3: Experiment with the Code!\n",
    "\n",
    "üß™ In the code cell above, change the `sequence_length` from `4` to `6` (as if we had a 6-word sentence) and re-run it. \n",
    "\n",
    "**Question:** What is the new shape of the `Attention Weights Matrix`? Why do you think it changed to that shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631ecd1",
   "metadata": {},
   "source": [
    "## Topic 4: Multi-Head Attention üêô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b471b",
   "metadata": {},
   "source": [
    "Single-head attention is great, but what if we could let the model focus on different things at the same time? That's the idea behind **Multi-Head Attention**.\n",
    "\n",
    "Instead of doing the attention calculation once, we do it multiple times in parallel with different weight matrices. Each parallel calculation is called an **\"attention head.\"**\n",
    "\n",
    "**Analogy:** Imagine you're analyzing a sentence. \n",
    "*   **Head 1** might focus on the grammatical structure (e.g., subject-verb relationships).\n",
    "*   **Head 2** might focus on the meaning and semantics (e.g., which words are synonyms).\n",
    "*   **Head 3** might focus on positional relationships (e.g., which words are close to each other).\n",
    "\n",
    "Finally, the outputs of all these heads are combined. This gives the model a much richer and more nuanced understanding of the input data, because it can learn different types of relationships simultaneously.\n",
    "\n",
    "Multi-head attention is a core component of the famous **Transformer architecture**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7143dd",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 4: Quick Question\n",
    "\n",
    "What is the main advantage of multi-head attention over single-head attention?\n",
    "\n",
    "a) It's faster to compute.\n",
    "b) It uses less memory.\n",
    "c) It allows the model to focus on different types of relationships in the data at the same time.\n",
    "d) It only works for short sentences.\n",
    "\n",
    "*Write your answer (a, b, c, or d) in the cell below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0daeb2d",
   "metadata": {},
   "source": [
    "# Double-click here and write your answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f33cda",
   "metadata": {},
   "source": [
    "## Topic 5: Where is Attention Used? üåç Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff5c0d",
   "metadata": {},
   "source": [
    "Attention isn't just a cool theoretical idea; it's used everywhere in modern AI!\n",
    "\n",
    "#### üó£Ô∏è Natural Language Processing (NLP)\n",
    "*   **Machine Translation:** To align words between source and target languages.\n",
    "*   **Text Summarization:** To identify the most important sentences in a long document.\n",
    "*   **Question Answering:** To find the part of a text that contains the answer to a question.\n",
    "*   **Sentiment Analysis:** To pinpoint words that carry strong positive or negative emotion.\n",
    "\n",
    "#### üñºÔ∏è Computer Vision\n",
    "*   **Image Captioning:** To focus on different parts of an image while generating the descriptive text.\n",
    "*   **Object Detection:** To highlight important regions in an image to find objects more accurately.\n",
    "\n",
    "#### üé§ Speech Recognition\n",
    "*   **Transcription:** To focus on the most critical parts of an audio signal to convert speech to text accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91660bd3",
   "metadata": {},
   "source": [
    "## üéâ Congratulations & Summary!\n",
    "\n",
    "You've made it through the core concepts of the attention mechanism! \n",
    "\n",
    "### ‚úÖ Key Takeaways\n",
    "\n",
    "*   **Core Idea:** Attention allows a model to selectively focus on relevant parts of the input.\n",
    "*   **Key Components:** It works using **Queries, Keys, and Values**.\n",
    "*   **Self-Attention:** A powerful variant where an input sequence attends to itself to learn internal relationships.\n",
    "*   **Multi-Head Attention:** Enhances the model's ability by running multiple attention layers in parallel, each learning different patterns.\n",
    "*   **Impact:** It is the foundation of the **Transformer architecture** and modern Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0c39e",
   "metadata": {},
   "source": [
    "# üìù Final Revision Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded80105",
   "metadata": {},
   "source": [
    "Time to put your new knowledge to the test! These questions cover everything we've discussed. Take your time and use the notes above if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c1f9b",
   "metadata": {},
   "source": [
    "### Task 1: Multiple Choice Question\n",
    "\n",
    "What is the primary purpose of the **softmax function** in the attention mechanism?\n",
    "\n",
    "a) To scale the dot-product scores.\n",
    "b) To convert the attention scores into a probability distribution.\n",
    "c) To compute the query, key, and value vectors.\n",
    "d) To concatenate the outputs of multiple attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4703f",
   "metadata": {},
   "source": [
    "Double-click here to write your answer (a, b, c, or d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338fd56",
   "metadata": {},
   "source": [
    "### Task 2: Short Question\n",
    "\n",
    "Explain the role of the scaling factor `sqrt(d_k)` in the scaled dot-product attention formula. Why is it important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d7c60",
   "metadata": {},
   "source": [
    "Double-click here to write your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfac1e",
   "metadata": {},
   "source": [
    "### Task 3: Another Short Question\n",
    "\n",
    "How does the attention mechanism help in making an AI model more **interpretable** (i.e., easier for humans to understand why it made a certain decision)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def10add",
   "metadata": {},
   "source": [
    "Double-click here to write your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6735a093",
   "metadata": {},
   "source": [
    "### Task 4: Problem-Solving with Code\n",
    "\n",
    "Given the following vectors, calculate the final context vector using dot-product attention (without scaling and without softmax for simplicity). Fill in the code below to perform the calculation.\n",
    "\n",
    "*   Query: `Q = [1, 0]`\n",
    "*   Keys: `K1 = [1, 1]`, `K2 = [0, 1]`\n",
    "*   Values: `V1 = [0.5, 0.5]`, `V2 = [0.2, 0.8]`\n",
    "\n",
    "**Steps:**\n",
    "1. Calculate the score for Key 1: `score1 = Q ‚Ä¢ K1` (dot product)\n",
    "2. Calculate the score for Key 2: `score2 = Q ‚Ä¢ K2`\n",
    "3. Calculate the context vector: `Context = (score1 * V1) + (score2 * V2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0351aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given vectors\n",
    "Q = np.array([1, 0])\n",
    "K1 = np.array([1, 1])\n",
    "K2 = np.array([0, 1])\n",
    "V1 = np.array([0.5, 0.5])\n",
    "V2 = np.array([0.2, 0.8])\n",
    "\n",
    "# 1. Calculate the scores (dot products)\n",
    "# HINT: Use np.dot(vector1, vector2)\n",
    "score1 = # YOUR CODE HERE\n",
    "score2 = # YOUR CODE HERE\n",
    "\n",
    "print(f\"Score 1 (Q with K1): {score1}\")\n",
    "print(f\"Score 2 (Q with K2): {score2}\")\n",
    "\n",
    "# 2. Calculate the final context vector\n",
    "context_vector = # YOUR CODE HERE\n",
    "\n",
    "print(f\"\\nFinal Context Vector: {context_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d0d7c",
   "metadata": {},
   "source": [
    "### Task 5: Case Study\n",
    "\n",
    "A team is building a text summarization model for long legal documents. They are considering using an older RNN-based model versus a modern Transformer-based model (which uses attention). \n",
    "\n",
    "Explain why the Transformer model with its attention mechanism is likely a much better choice for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ecbbd",
   "metadata": {},
   "source": [
    "Double-click here to write your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a399fb",
   "metadata": {},
   "source": [
    "--- \n",
    "### Great work today! You've taken a huge step in understanding the engines behind modern AI. Keep exploring! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
