{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2dd7434",
   "metadata": {},
   "source": [
    "# üß† Introduction to Convolutional Neural Networks (CNNs) for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342877b",
   "metadata": {},
   "source": [
    "## üëã Welcome to Your First Look at CNNs!\n",
    "\n",
    "Welcome! In this 2-hour session, we'll dive into the exciting world of **Convolutional Neural Networks (CNNs)**. These are the powerful AI models behind many things you see every day, like your phone's facial recognition or how social media tags photos!\n",
    "\n",
    "### üìò Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1.  **Understand** what a CNN is and why it's so important.\n",
    "2.  **Identify** the core components of a CNN (Convolution, Pooling, etc.).\n",
    "3.  **Differentiate** between a 2D CNN for images and a 1D CNN for text.\n",
    "4.  **Read** simple code that defines a basic CNN model.\n",
    "5.  **Explore** real-world applications of this amazing technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2a404",
   "metadata": {},
   "source": [
    "## Topic 1: What is a CNN?\n",
    "\n",
    "A **Convolutional Neural Network** (CNN or ConvNet) is a special type of AI model inspired by how the human brain's visual cortex works. It's incredibly good at processing data that has a grid-like structure, like an image.\n",
    "\n",
    "**Why does it matter?** Before CNNs, teaching a computer to recognize objects in pictures was very difficult and required a lot of manual work to extract features (like edges, corners, and colors). CNNs learn to do this **automatically**! They learn to see patterns, starting with simple edges and building up to complex objects like faces or cars.\n",
    "\n",
    "This automated feature learning is what makes CNNs the backbone of modern AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b954d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our simple image: (5, 5)\n",
      "\n",
      "A real-world color image might have a shape like (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# An image is just a grid of numbers (pixels)!\n",
    "# Let's imagine a small 5x5 grayscale image.\n",
    "# 0 = black, 255 = white\n",
    "import numpy as np\n",
    "\n",
    "# This numpy array represents a simple image with a bright cross in the middle.\n",
    "simple_image = np.array([\n",
    "    [0, 0, 255, 0, 0],\n",
    "    [0, 0, 255, 0, 0],\n",
    "    [255, 255, 255, 255, 255],\n",
    "    [0, 0, 255, 0, 0],\n",
    "    [0, 0, 255, 0, 0]\n",
    "])\n",
    "\n",
    "# In Python, we can check its dimensions or 'shape'.\n",
    "# A real color image would have a 3rd dimension for color channels (Red, Green, Blue).\n",
    "print(\"Shape of our simple image:\", simple_image.shape)\n",
    "print(\"\\nA real-world color image might have a shape like (224, 224, 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc138a8e",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Think about your smartphone. Can you name one feature that likely uses a CNN to understand images or video? Write your answer in the code cell below as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here. For example:\n",
    "# My answer: The feature that unlocks my phone with my face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098d53c",
   "metadata": {},
   "source": [
    "## Topic 2: The Core Building Blocks of a CNN\n",
    "\n",
    "CNNs are built from a few key layers that work together. Let's look at the three most important ones.\n",
    "\n",
    "### 1. The Convolutional Layer: The Feature Detector üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "This is the main workhorse. It uses a small window called a **filter** (or kernel) that slides over the image. This filter is designed to detect a specific pattern, like an edge, a corner, or a patch of color. The network *learns* the best filters for the job.\n",
    "\n",
    "After the convolution, we usually apply a **ReLU** (Rectified Linear Unit) activation function. It's a simple rule: if a pixel's value is negative, change it to zero. This helps the model learn more complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5158cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Patch:\n",
      " [[10 10 10]\n",
      " [10 10 10]\n",
      " [90 90 90]]\n",
      "\n",
      "Filter:\n",
      " [[ 1  1  1]\n",
      " [ 0  0  0]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "Detection Score: -240 (A high score indicates a match!)\n"
     ]
    }
   ],
   "source": [
    "# Let's see a conceptual example of a filter in action.\n",
    "# This isn't real TensorFlow code, but it shows the idea!\n",
    "\n",
    "image_patch = np.array([\n",
    "    [10, 10, 10],\n",
    "    [10, 10, 10],\n",
    "    [90, 90, 90] # A horizontal edge\n",
    "])\n",
    "\n",
    "horizontal_edge_filter = np.array([\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [-1, -1, -1]\n",
    "])\n",
    "\n",
    "# The 'convolution' is a dot product of the patch and filter.\n",
    "# A high value means the filter found the pattern it was looking for!\n",
    "detection_score = np.sum(image_patch * horizontal_edge_filter)\n",
    "\n",
    "print(\"Image Patch:\\n\", image_patch)\n",
    "print(\"\\nFilter:\\n\", horizontal_edge_filter)\n",
    "print(\"\\nDetection Score:\", detection_score, \"(A high score indicates a match!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fca5d",
   "metadata": {},
   "source": [
    "### 2. The Pooling Layer: The Shrinker üìâ\n",
    "\n",
    "After detecting features, the network needs to make the data smaller and more manageable. The Pooling Layer does this by downsampling, or shrinking, the feature map. \n",
    "\n",
    "The most common type is **Max Pooling**. It looks at a small window of pixels (e.g., 2x2) and keeps only the *maximum* value. This reduces the size of the data but keeps the most important information (the strongest feature signals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b9274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2x2 Patch:\n",
      " [[10 50]\n",
      " [90 30]]\n",
      "\n",
      "Value after Max Pooling: 90\n"
     ]
    }
   ],
   "source": [
    "# Conceptual example of Max Pooling\n",
    "\n",
    "feature_map_patch = np.array([\n",
    "    [10, 50],\n",
    "    [90, 30]\n",
    "])\n",
    "\n",
    "# Find the maximum value in this 2x2 patch\n",
    "max_pooled_value = np.max(feature_map_patch)\n",
    "\n",
    "print(\"Original 2x2 Patch:\\n\", feature_map_patch)\n",
    "print(\"\\nValue after Max Pooling:\", max_pooled_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbc525",
   "metadata": {},
   "source": [
    "### 3. The Fully Connected Layer: The Decision Maker üß†\n",
    "\n",
    "After several rounds of convolution and pooling, the network has a rich set of high-level features. The data is then \"flattened\" from a 2D grid into a single long list. This list is fed into a **Fully Connected Layer**, which is a classic neural network that looks at all the features and makes the final decision, like \"This image is 95% a cat\" or \"This review is 88% positive.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf12e9",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Why is the Pooling layer useful? Choose the best answer:\n",
    "a) It adds more features to the image.\n",
    "b) It reduces the size of the data, making the network faster and more efficient.\n",
    "c) It makes the final classification decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1579f5c",
   "metadata": {},
   "source": [
    "## Topic 3: 2D CNNs in Action - Image Classification üñºÔ∏è\n",
    "\n",
    "Now let's put it all together for images! When we use a CNN on an image, the convolutions happen in two dimensions (height and width). This is a **2D CNN**.\n",
    "\n",
    "**The Logic:**\n",
    "1.  **Input:** The image is fed in as a grid of pixels.\n",
    "2.  **Early Layers:** The first few convolutional layers learn to detect simple features like edges, corners, and colors.\n",
    "3.  **Deeper Layers:** These simple features are combined in deeper layers to detect more complex shapes, like eyes, noses, or wheels.\n",
    "4.  **Final Layers:** The deepest layers can recognize entire objects, like a face or a car.\n",
    "5.  **Classification:** The fully connected layers take this high-level understanding and classify the image.\n",
    "\n",
    "Below is a real (but simple) CNN model defined using the popular Python library TensorFlow/Keras. This model is designed to classify handwritten digits (0-9) from the famous MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf8425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 5408)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                54090     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,410\n",
      "Trainable params: 54,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# You need to have tensorflow installed for this to run: pip install tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define a simple 2D CNN model\n",
    "model_2d = Sequential([\n",
    "    # Input Layer: We need to specify the shape of our images (28x28 pixels, 1 color channel for grayscale)\n",
    "    # 1. Convolutional Layer: Finds initial features. \n",
    "    # 32 filters, each 3x3 in size. 'relu' is our activation function.\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "\n",
    "    # 2. Pooling Layer: Shrinks the data.\n",
    "    # It will look at 2x2 windows and take the max value.\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    # 3. Flatten Layer: Prepares the data for the decision-making layers.\n",
    "    # It unrolls the 2D feature maps into one long vector.\n",
    "    Flatten(),\n",
    "\n",
    "    # 4. Fully Connected (Dense) Layer: The Decision Maker.\n",
    "    # 10 output neurons, one for each digit (0-9). 'softmax' gives us probabilities for each class.\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print a summary of our model architecture!\n",
    "model_2d.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec65c1d",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Look at the `Conv2D` layer in the code above. It has `32` filters. What do you think would happen if you changed this number to `16`? Would the model learn more patterns or fewer patterns?\n",
    "\n",
    "*(Hint: Each filter learns one pattern. So fewer filters means the model learns fewer patterns!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c0845",
   "metadata": {},
   "source": [
    "## Topic 4: 1D CNNs in Action - Text Classification üìù\n",
    "\n",
    "CNNs aren't just for images! They can also be used for sequential data, like text or time-series data. For this, we use a **1D CNN**.\n",
    "\n",
    "**How does it work with text?**\n",
    "1.  **Text to Numbers:** First, we can't feed words directly to a neural network. We convert each word into a number.\n",
    "2.  **Word Embeddings:** Then, each number is mapped to a meaningful vector of numbers (an **embedding**). This vector captures the word's meaning, so words like \"happy\" and \"joyful\" will have similar vectors.\n",
    "3.  **1D Convolution:** The CNN filter now slides over this sequence of word vectors in **one dimension**. A filter of size 3 would look at 3 words at a time (a trigram), searching for meaningful patterns like \"not very good\" that indicate negative sentiment.\n",
    "4.  **Pooling & Classification:** Just like with images, we use pooling (usually `GlobalMaxPooling1D`, which just takes the most important signal from the whole sentence) and fully connected layers to make the final classification (e.g., \"positive\" or \"negative\" review).\n",
    "\n",
    "Below is a simple 1D CNN for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e471de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,362,177\n",
      "Trainable params: 1,362,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# You need to have tensorflow installed for this to run: pip install tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Let's assume we have 10,000 unique words in our vocabulary\n",
    "vocab_size = 10000\n",
    "# Let's assume we make all our sentences 100 words long (by padding or trimming)\n",
    "max_length = 100\n",
    "# Each word will be represented by a vector of size 128\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define a simple 1D CNN model\n",
    "model_1d = Sequential([\n",
    "    # 1. Embedding Layer: Turns word numbers into dense vectors.\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "\n",
    "    # 2. 1D Convolutional Layer: Slides along the sentence to find patterns (n-grams).\n",
    "    # Here the filter size is 5, so it looks at 5 words at a time.\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "\n",
    "    # 3. Pooling Layer: Takes the most important signal from the convolution.\n",
    "    GlobalMaxPooling1D(),\n",
    "\n",
    "    # 4. Fully Connected Layer: Makes the final decision.\n",
    "    # 1 output neuron with 'sigmoid' activation for binary classification (e.g., positive/negative).\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print a summary of our model!\n",
    "model_1d.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f731fe3",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Our 1D CNN example uses `kernel_size=5`. This means the filter looks at patterns of 5 consecutive words. If you were trying to classify text based on legal documents, would you want a smaller or larger `kernel_size`? Why?\n",
    "\n",
    "*(Hint: Legal language often has long, complex phrases. A larger kernel size might be better to capture these!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4c8cb",
   "metadata": {},
   "source": [
    "## üöÄ Final Revision Assignment\n",
    "\n",
    "Time to test your knowledge! These questions cover everything we've discussed. Try to answer them based on what you've learned. This is great practice for you to do at home."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad3f54",
   "metadata": {},
   "source": [
    "### Task 1 (Multiple Choice)\n",
    "\n",
    "What is the **primary purpose** of a convolutional layer in a CNN?\n",
    "a) To classify the input data.\n",
    "b) To reduce the dimensionality of the input.\n",
    "c) To extract features from the input data.\n",
    "d) To introduce non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99fd64",
   "metadata": {},
   "source": [
    "### Task 2 (Multiple Choice)\n",
    "\n",
    "In a 2D CNN for image classification, what does **\"translation invariance\"** refer to?\n",
    "a) The network can handle images of different sizes.\n",
    "b) The network can recognize an object even if its position changes in the image.\n",
    "c) The network can classify multiple objects in the same image.\n",
    "d) The network is invariant to the color of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1446cc5",
   "metadata": {},
   "source": [
    "### Task 3 (Multiple Choice)\n",
    "\n",
    "What is the role of the **embedding layer** in a 1D CNN for text classification?\n",
    "a) To increase the length of the text sequences.\n",
    "b) To convert words into meaningful vector representations.\n",
    "c) To perform the final classification.\n",
    "d) To reduce the number of parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71502684",
   "metadata": {},
   "source": [
    "### Task 4 (Short Question)\n",
    "\n",
    "In simple terms, explain the main difference between a **2D convolution** (for images) and a **1D convolution** (for text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a330808",
   "metadata": {},
   "source": [
    "### Task 5 (Problem Solving)\n",
    "\n",
    "You have a color image that is 32 pixels wide, 32 pixels high, and has 3 color channels (RGB). You apply a single convolutional layer with 16 filters. What is the **depth** of the output feature map? (i.e., how many channels will it have?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b764b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Fill in the Blanks\n",
    "# You are designing a simple 1D CNN to classify SMS messages as \"spam\" or \"not spam\".\n",
    "# List the layers you would use in the correct order.\n",
    "\n",
    "# 1. __________ Layer (To convert words to vectors)\n",
    "# 2. __________ Layer (To find patterns in the text)\n",
    "# 3. __________ Layer (To reduce the data and keep important signals)\n",
    "# 4. __________ Layer (To make the final 'spam' or 'not spam' decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f97ff",
   "metadata": {},
   "source": [
    "### Task 7 (Case Study)\n",
    "\n",
    "A startup wants to build an app that automatically categorizes user photos into \"food\", \"animals\", or \"landscapes\". They have a small dataset of 10,000 images. Should they train a huge CNN from scratch or use **transfer learning** (using a pre-trained model like VGG16)? Why?\n",
    "\n",
    "*(Hint: Training a big model from scratch requires a massive amount of data. Is 10,000 images a lot?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16a0c3",
   "metadata": {},
   "source": [
    "## ‚úÖ Well Done & Next Steps!\n",
    "\n",
    "Congratulations on completing this introduction to CNNs! You've learned the fundamental concepts that power much of modern artificial intelligence.\n",
    "\n",
    "### üìö Extra Learning Resources\n",
    "\n",
    "If you want to continue your journey, here are some excellent resources:\n",
    "\n",
    "*   **Online Courses:**\n",
    "    *   Coursera - Deep Learning Specialization by Andrew Ng\n",
    "    *   edX - Deep Learning Fundamentals with Keras\n",
    "    *   Udacity - Intro to Deep Learning with PyTorch\n",
    "\n",
    "*   **Tutorials & Documentation:**\n",
    "    *   [TensorFlow CNN Tutorial](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "    *   [PyTorch CNN Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "    *   [Keras Text Classification with 1D CNN](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
    "\n",
    "*   **Key Research Papers:**\n",
    "    *   \"ImageNet Classification with Deep Convolutional Neural Networks\" (AlexNet)\n",
    "    *   \"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
