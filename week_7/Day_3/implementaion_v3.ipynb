{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46ceb16",
   "metadata": {},
   "source": [
    "# 🧠 Classifying Handwritten Digits with TensorFlow, PyTorch, and Keras\n",
    "\n",
    "Welcome to this hands-on session where we'll teach a computer to read handwritten numbers! This is a classic first project in AI called image classification.\n",
    "\n",
    "We will build the *exact same* neural network using three of the most popular deep learning toolkits: **TensorFlow**, **PyTorch**, and standalone **Keras**. This will give you a great feel for how each one works.\n",
    "\n",
    "--- \n",
    "\n",
    "### 📘 Learning Objectives for Today:\n",
    "\n",
    "By the end of this 2-hour session, you will be able to:\n",
    "1.  **Understand** the task of classifying the MNIST handwritten digit dataset.\n",
    "2.  **Load and prepare** image data for a neural network.\n",
    "3.  **Implement** the same neural network model using TensorFlow/Keras, PyTorch, and standalone Keras.\n",
    "4.  **Recognize** the key similarities and differences between these powerful frameworks.\n",
    "5.  **Train and evaluate** each model to see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9eca2",
   "metadata": {},
   "source": [
    "## The Task: The MNIST Dataset ✍️\n",
    "\n",
    "Our goal is to classify images from the famous **MNIST dataset**. \n",
    "- It contains 60,000 images for training and 10,000 for testing.\n",
    "- Each image is a small, 28x28 pixel grayscale picture of a handwritten digit (0 through 9).\n",
    "- We will train a neural network to look at an image and predict which digit it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc187065",
   "metadata": {},
   "source": [
    "# Topic 1: TensorFlow (with the integrated Keras API)\n",
    "\n",
    "**TensorFlow** is a powerful and flexible open-source library for machine learning developed by Google. It's often used with its user-friendly high-level API, **Keras**, which is now fully integrated into TensorFlow. This combination gives us both power and simplicity!\n",
    "\n",
    "Let's build our digit classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4eea9-b521-4c13-b316-43e8cd47bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tensorflow\n",
    "# pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# for pytorch\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b389b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "#frist download tensorflow using this code in Anaconda Prompt\n",
    "# pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# Step 1: Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Step 2: Load and preprocess the data\n",
    "# The data is already included in TensorFlow!\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values from 0-255 to 0-1. This helps the network learn better.\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(\"Data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c5e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Qasim\\anaconda3\\envs\\dl\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow model built successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build the model architecture\n",
    "model_tf = Sequential([\n",
    "    # This layer flattens the 28x28 image into a single 784-pixel line.\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    \n",
    "    # This is our hidden layer with 128 neurons. 'relu' is a common activation function.\n",
    "    Dense(128, activation='relu'),   \n",
    "    \n",
    "    # This is the output layer. It has 10 neurons (one for each digit 0-9).\n",
    "    # 'softmax' gives a probability for each digit.\n",
    "    Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "print(\"TensorFlow model built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eccbcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9257 - loss: 0.2582\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.1134\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9763 - loss: 0.0784\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9817 - loss: 0.0588\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9857 - loss: 0.0461\n",
      "Training finished.\n",
      "\n",
      "Evaluating on test data:\n",
      "313/313 - 2s - 6ms/step - accuracy: 0.9742 - loss: 0.0821\n",
      "\n",
      "Test Accuracy: 97.42%\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Compile the model\n",
    "# Here we define the optimizer, how to measure loss, and what metric to track.\n",
    "model_tf.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the model\n",
    "# We show the model the training data for 5 'epochs' (5 full passes).\n",
    "print(\"Starting training...\")\n",
    "model_tf.fit(x_train, y_train, epochs=5)\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "# Let's see how well it performs on the test data it has never seen before.\n",
    "print(\"\\nEvaluating on test data:\")\n",
    "loss, accuracy = model_tf.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d03a0f",
   "metadata": {},
   "source": [
    "### 🎯 Practice Task (TensorFlow/Keras)\n",
    "\n",
    "Our model has one hidden layer with 128 neurons.\n",
    "\n",
    "🧪 **Try this:** In the cell below, create a new model (you can call it `model_tf_2`) with **256 neurons** in the hidden layer. Then compile, train, and evaluate it. Does the accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1b1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to build and train a model with 256 neurons in the Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955aabf0",
   "metadata": {},
   "source": [
    "# Topic 2: PyTorch 🔥\n",
    "\n",
    "**PyTorch** is another hugely popular open-source machine learning library, developed by Facebook's AI Research lab. It's known for its flexibility and more 'Python-like' feel, which makes it a favorite in the research community. \n",
    "\n",
    "You'll notice the process is a bit more manual, giving you more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d40e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 2: Define transformations and load the data\n",
    "# We define how to prepare the data: convert it to a tensor and normalize it.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# PyTorch has built-in data loaders for MNIST.\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"PyTorch data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e459cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model built successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build the model architecture using a class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128) # Input is 784, output is 128\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10) # Input is 128, output is 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model_pytorch = Net()\n",
    "print(\"PyTorch model built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3656bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1, Loss: 0.39683890608010264\n",
      "Epoch 2, Loss: 0.20958272183437082\n",
      "Epoch 3, Loss: 0.1521850965702648\n",
      "Epoch 4, Loss: 0.12384487968335338\n",
      "Epoch 5, Loss: 0.10275054311077954\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Train the model (The Training Loop)\n",
    "# In PyTorch, you write the training loop yourself.\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()    # Reset the gradients\n",
    "        outputs = model_pytorch(images) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate loss\n",
    "        loss.backward()          # Backward pass (calculate gradients)\n",
    "        optimizer.step()         # Update weights\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d1bb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test data:\n",
      "Test Accuracy: 96.57%\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model\n",
    "print(\"\\nEvaluating on test data:\")\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # We don't need to calculate gradients during evaluation\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model_pytorch(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf2d77",
   "metadata": {},
   "source": [
    "### 🎯 Practice Task (PyTorch)\n",
    "\n",
    "The training loop ran for 5 epochs.\n",
    "\n",
    "🧪 **Try this:** Copy the training and evaluation code cells below. Change `range(5)` to `range(3)` to train for only 3 epochs. How does this affect the final accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0964f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to train and evaluate the PyTorch model for 3 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ceb72",
   "metadata": {},
   "source": [
    "# Topic 3: Keras (Standalone) ✨\n",
    "\n",
    "**Keras** started as a high-level, user-friendly API that could run on top of different backends like TensorFlow, Theano, or CNTK. It's famous for allowing for fast and easy model building. \n",
    "\n",
    "While it is now integrated into TensorFlow, you can still use it as a separate library. The code will look *very* similar to our first TensorFlow example, with some minor differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5868ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone Keras data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Keras libraries\n",
    "# Note: You might need to install keras separately (`pip install keras`)\n",
    "# if it's not already part of your TensorFlow installation.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Step 2: Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels. (e.g., 5 -> [0,0,0,0,0,1,0,0,0,0])\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"Standalone Keras data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e791fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Qasim\\anaconda3\\envs\\dl\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone Keras model built and compiled!\n"
     ]
    }
   ],
   "source": [
    "# Step 3 & 4: Build and Compile the model\n",
    "# This part is almost identical to the tf.keras example!\n",
    "model_keras = Sequential()\n",
    "model_keras.add(Flatten(input_shape=(28, 28)))\n",
    "model_keras.add(Dense(128, activation='relu'))\n",
    "model_keras.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# A key difference from the first example is the loss function.\n",
    "# Since we one-hot encoded the labels, we use 'categorical_crossentropy'.\n",
    "model_keras.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Standalone Keras model built and compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f784dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9009 - loss: 0.3578\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9518 - loss: 0.1696\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9659 - loss: 0.1196\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9740 - loss: 0.0910\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0734\n",
      "\n",
      "Evaluating on test data:\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0889\n",
      "Test loss: 0.0889\n",
      "Test accuracy: 97.25%\n"
     ]
    }
   ],
   "source": [
    "# Step 5 & 6: Train and Evaluate the model\n",
    "print(\"Starting training...\")\n",
    "model_keras.fit(x_train, y_train, epochs=5, batch_size=128)\n",
    "\n",
    "print(\"\\nEvaluating on test data:\")\n",
    "score = model_keras.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {score[0]:.4f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c6f00",
   "metadata": {},
   "source": [
    "### 🎯 Practice Task (Standalone Keras)\n",
    "\n",
    "The `batch_size` determines how many images the model looks at before updating its weights. We used 128.\n",
    "\n",
    "🧪 **Try this:** Change the `batch_size` in the `model.fit()` call to `32`. How does this affect the training time per epoch and the final accuracy? (Smaller batches often lead to longer training time but can sometimes improve accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to train the Keras model with a batch_size of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b805468",
   "metadata": {},
   "source": [
    "## 🚀 Final Revision Assignment\n",
    "\n",
    "Congratulations! You've successfully built and trained an image classifier in three different ways. You've seen that the core ideas are universal:\n",
    "\n",
    "**Load Data -> Define Model -> Train Model -> Evaluate Model**\n",
    "\n",
    "Here are a few challenges to solidify your understanding. Try to solve at least three!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8252a",
   "metadata": {},
   "source": [
    "1.  **Deeper Model:** Go back to the first **TensorFlow/Keras** model. Add a *second* hidden `Dense` layer with 64 neurons (and a `'relu'` activation) between the existing 128-neuron layer and the final output layer. Does making the model 'deeper' improve performance?\n",
    "\n",
    "2.  **Optimizer Choice:** In all three models, we used the 'Adam' optimizer. Research another optimizer like `'sgd'` (Stochastic Gradient Descent). Try swapping `optimizer='adam'` with `optimizer='sgd'` in one of the Keras models. How does it affect the training and final accuracy?\n",
    "\n",
    "3.  **PyTorch Learning Rate:** The `lr=0.001` in the PyTorch optimizer is the learning rate. This controls how big of a step the optimizer takes. Change it to a much larger value like `lr=0.1`. What happens? Does the model fail to learn? \n",
    "\n",
    "4.  **Data Normalization Impact:** We divided the pixel values by `255.0` to scale them between 0 and 1. Comment out this line in one of the Keras models and retrain it. Does the model still work? (Hint: It will likely struggle a lot!).\n",
    "\n",
    "5.  **Predict a Single Image:** After training one of the models, write code to select just *one* image from the `x_test` dataset, make a prediction on it, and print both the predicted digit and the actual correct digit (`y_test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffa6bc",
   "metadata": {},
   "source": [
    "## ✅ Well Done!\n",
    "\n",
    "You've taken a massive step in your AI journey. Understanding how to implement models in different frameworks is a critical skill. The best way to learn is by experimenting, so feel free to change parameters and see what happens.\n",
    "\n",
    "Keep learning and stay curious!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
