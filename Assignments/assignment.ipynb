{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Fake News Detection using NLP & Machine Learning\n",
    "\n",
    "Welcome dear students 👋!  \n",
    "In this fun mini-project, you will build a **Fake News Detector** using **NLP and Machine Learning**.\n",
    "\n",
    "We'll learn step-by-step how text is processed, understood, and classified by AI systems 💡.\n",
    "\n",
    "Let's begin our happy learning journey 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📘 Step 1: Linguistic Foundation\n",
    "\n",
    "Before coding, let's recall the **four linguistic levels** of NLP:\n",
    "- **Syntax** 🧩 — sentence structure (grammar rules)\n",
    "- **Semantics** 💬 — meaning of words/sentences\n",
    "- **Pragmatics** 🧠 — meaning based on context\n",
    "- **Discourse** 📖 — how sentences connect to form a meaningful paragraph\n",
    "\n",
    "👉 We'll explore them using **POS Tagging** and **Chunking** soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentence for linguistic analysis\n",
    "example_sentence = \"The intelligent system learns new patterns efficiently.\"\n",
    "\n",
    "# TODO: Tokenize the sentence and perform POS tagging using nltk\n",
    "# HINT: use nltk.word_tokenize() and nltk.pos_tag()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# --- Write your code below ---\n",
    "# tokens = ...\n",
    "# pos_tags = ...\n",
    "# print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Step 2: Data Preprocessing with NLTK\n",
    "\n",
    "We'll clean text data to remove unnecessary symbols, numbers, and stopwords.\n",
    "\n",
    "### Concepts Covered:\n",
    "- Lowercasing\n",
    "- Removing numbers & punctuation\n",
    "- Stopword removal\n",
    "- Tokenization\n",
    "- Stemming & Lemmatization\n",
    "- Normalization\n",
    "\n",
    "Let's define our preprocessing function 🧼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifici intellig revolution industri\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(t)) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"Artificial Intelligence in 2025 will revolutionize industries!\"\n",
    "print(preprocess(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 3: Load Dataset\n",
    "\n",
    "We'll use a small Fake News dataset. Each row contains a news article and a label (real/fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00301ba3433741e5a1418c22ce890b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/487 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Qasim\\anaconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Qasim\\.cache\\huggingface\\hub\\datasets--ErfanMoosaviMonazzah--fake-news-detection-dataset-English. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa2ca4016ea43728089cb0912e301f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.tsv:   0%|          | 0.00/78.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b19b58d1d3c4ba7a2d8958fb84faf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.tsv:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792f182ba18348aea4b04673c6cfe3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English/resolve/2f7e828658d33a0e8aca4b2f9f4ccb26e27ac32f/test.tsv: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd5b39959ff4e78b5a08409f278aaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:  32%|###2      | 10.5M/32.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c638946de54ed3aba7d236255e6e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03fcfb15eeb46269be226dbef7fae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cab6579b77440d6b5aa8750172fea05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>The U.S. Senate on Friday backed a plan to nam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>What a difference a year makes. A year ago, Pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>The US Supreme Court is set to decide the firs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>U.S. President Donald Trump is expected to mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>U.S. Supreme Court Justice Antonin Scalia’s ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "2308   The U.S. Senate on Friday backed a plan to nam...      1\n",
       "22404  What a difference a year makes. A year ago, Pr...      1\n",
       "23397  The US Supreme Court is set to decide the firs...      0\n",
       "25058  U.S. President Donald Trump is expected to mak...      1\n",
       "2664   U.S. Supreme Court Justice Antonin Scalia’s ca...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "ds = load_dataset(\"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(ds['train'])  # or ds['test'] depending on partition\n",
    "\n",
    "# Assume the columns include e.g. 'text' and 'label'\n",
    "data = df[['text', 'label']].dropna().sample(1000, random_state=42)\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>The U.S. Senate on Friday backed a plan to nam...</td>\n",
       "      <td>1</td>\n",
       "      <td>u senat friday back plan name plaza front chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>What a difference a year makes. A year ago, Pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>differ year make year ago prime minist theresa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>The US Supreme Court is set to decide the firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>u suprem court set decid first major abort cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>U.S. President Donald Trump is expected to mak...</td>\n",
       "      <td>1</td>\n",
       "      <td>u presid donald trump expect make announc week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>U.S. Supreme Court Justice Antonin Scalia’s ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>u suprem court justic antonin scalia ’ caus de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "2308   The U.S. Senate on Friday backed a plan to nam...      1   \n",
       "22404  What a difference a year makes. A year ago, Pr...      1   \n",
       "23397  The US Supreme Court is set to decide the firs...      0   \n",
       "25058  U.S. President Donald Trump is expected to mak...      1   \n",
       "2664   U.S. Supreme Court Justice Antonin Scalia’s ca...      1   \n",
       "\n",
       "                                              clean_text  \n",
       "2308   u senat friday back plan name plaza front chin...  \n",
       "22404  differ year make year ago prime minist theresa...  \n",
       "23397  u suprem court set decid first major abort cas...  \n",
       "25058  u presid donald trump expect make announc week...  \n",
       "2664   u suprem court justic antonin scalia ’ caus de...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing to all text\n",
    "data['clean_text'] = data['text'].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Step 4: POS Tagging, NER, and Chunking\n",
    "\n",
    "Let's analyze sentence structure using NLTK’s tools.\n",
    "\n",
    "👉 Complete missing code lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data['clean_text'].iloc[0]\n",
    "tokens = word_tokenize(sample)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags[:10])  # show first few tagged words\n",
    "\n",
    "# Define a chunk grammar (e.g., noun phrase)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(pos_tags)\n",
    "\n",
    "# TODO: Visualize or print tree\n",
    "# HINT: result.draw()  # (optional if you run locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Step 5: Explore WordNet\n",
    "WordNet helps understand word meanings and relations.\n",
    "Try exploring synonyms, definitions, and examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = 'intelligence'\n",
    "syns = wordnet.synsets(word)\n",
    "\n",
    "print(f\"Definition: {syns[0].definition()}\")\n",
    "print(f\"Examples: {syns[0].examples()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 Step 6: Feature Extraction (BoW + TF-IDF)\n",
    "Let's convert text into numerical features that ML models can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Create BoW and TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data['clean_text'])\n",
    "y = data['label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 Step 7: Document Similarity\n",
    "Let's calculate how similar two documents are using cosine similarity 🧮."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(X[0:2])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Step 8: Classification Models (SVM, Decision Tree, Random Forest)\n",
    "Now, let's train and test our models!\n",
    "\n",
    "We'll compare how each performs using accuracy and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚖️ Step 9: Handling Imbalanced Dataset\n",
    "Sometimes data isn't balanced — let's oversample minority classes using **RandomOverSampler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "print('Before:', X.shape, 'After:', X_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Step 10: Evaluation Metrics & Confusion Matrix\n",
    "Let's visualize model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(models['Random Forest'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔮 Step 11: Future Directions in NLP\n",
    "NLP is evolving rapidly! ✨\n",
    "- **Transformers** like BERT, GPT, and T5 dominate current research.\n",
    "- **Few-shot & zero-shot learning** are reducing labeled data needs.\n",
    "- **Ethical AI** ensures fairness and transparency.\n",
    "\n",
    "Keep exploring, learning, and experimenting — the future is yours 🌍!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 **Congratulations!** You've completed the mini-project successfully! 💪\n",
    "\n",
    "You now understand the NLP pipeline — from syntax to semantics, and from preprocessing to machine learning. 👏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
