{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¬ Movie Review Sentiment Analysis Project\n",
    "Welcome to your NLP mini-project! In this task, youâ€™ll use real movie review data to learn core Natural Language Processing concepts â€” from preprocessing to classification.\n",
    "\n",
    "Youâ€™ll complete some code cells (marked with **TODO**) to practice important NLP steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 2: Load Simple Text Dataset\n",
    "Weâ€™ll use the built-in **20 Newsgroups dataset** (itâ€™s small and automatically downloads). We'll only pick two categories to make it binary â€” like fake vs real sentiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data from sklearn\n",
    "categories = ['rec.autos', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'text': newsgroups.data, 'label': newsgroups.target})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Step 3: Text Preprocessing\n",
    "Here, youâ€™ll:\n",
    "- Lowercase text\n",
    "- Remove punctuation and stopwords\n",
    "- Apply stemming\n",
    "\n",
    "ðŸ‘‰ **Task:** Fill in the missing lines marked as `# TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
    "    words = text.split()\n",
    "    # TODO: remove stopwords\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    # TODO: apply stemming\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "data['clean_text'] = data['text'].apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§® Step 4: Feature Extraction\n",
    "Convert text into numerical vectors using **TF-IDF** (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data['clean_text'])\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Step 5: Train Models\n",
    "Weâ€™ll train both **SVM** and **Decision Tree** classifiers to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Train Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 6: Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Model Evaluation & Comparison\n",
    "# Weâ€™ll compare the performance of 4 models:\n",
    "# 1. Support Vector Machine (SVM)\n",
    "# 2. Decision Tree\n",
    "# 3. Bagging Classifier\n",
    "# 4. Random Forest\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'SVM': svm_model,\n",
    "    'Decision Tree': dt_model,\n",
    "    'Bagging': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train & evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nðŸ”¹ Training and Evaluating Model: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§­ Step 7: Discussion & Extension Ideas\n",
    "- Which model performed better? Why?\n",
    "- Try using `CountVectorizer` instead of `TfidfVectorizer`.\n",
    "- Add `WordNet Lemmatizer` instead of stemmer.\n",
    "- Plot confusion matrix using seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Congratulations!\n",
    "Youâ€™ve completed your mini-project covering:\n",
    "- Data loading\n",
    "- Text preprocessing\n",
    "- Feature extraction\n",
    "- Model training & evaluation\n",
    "\n",
    "Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
