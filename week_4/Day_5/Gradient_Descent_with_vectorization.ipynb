{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7759d9c",
   "metadata": {},
   "source": [
    "# üìò Introduction to Univariate Linear Regression & Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afc4e7",
   "metadata": {},
   "source": [
    "## Welcome, Future AI Expert! üëã\n",
    "\n",
    "In this 2-hour interactive session, we will dive into one of the most fundamental algorithms in machine learning: **Univariate Linear Regression**. We'll learn how a computer can find relationships in data and make predictions. We'll also uncover the magic behind how it 'learns' using an algorithm called **Gradient Descent**.\n",
    "\n",
    "### üéØ Our Learning Objectives:\n",
    "\n",
    "1.  **Understand Linear Regression**: What it is and why it's useful.\n",
    "2.  **The Hypothesis & Cost Function**: Learn how the model makes predictions and measures its own error.\n",
    "3.  **Grasp Gradient Descent**: Discover the iterative process of learning from data.\n",
    "4.  **Code with Loops**: Implement Gradient Descent the intuitive, step-by-step way.\n",
    "5.  **Code with Vectorization**: Learn the fast and efficient way using the NumPy library.\n",
    "6.  **Apply Your Knowledge**: Solve a final assignment to solidify your new skills!\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1244acb",
   "metadata": {},
   "source": [
    "## Topic 1: What is Univariate Linear Regression?\n",
    "\n",
    "Imagine you have data and you want to predict a value. For example, predicting a student's exam score based on the hours they studied. Univariate Linear Regression helps us find a **straight line** that best fits our data.\n",
    "\n",
    "This line is represented by a simple equation:\n",
    "\n",
    "`y = mx + c`\n",
    "\n",
    "In Machine Learning, we often write it like this:\n",
    "\n",
    "**`hŒ∏(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx`**\n",
    "\n",
    "*   `hŒ∏(x)` is our **prediction** (hypothesis).\n",
    "*   `x` is our **input feature** (e.g., hours studied).\n",
    "*   `Œ∏‚ÇÄ` (theta-zero) is the **y-intercept**.\n",
    "*   `Œ∏‚ÇÅ` (theta-one) is the **slope** of the line.\n",
    "\n",
    "Our goal is to find the best values for `Œ∏‚ÇÄ` and `Œ∏‚ÇÅ` that make our line fit the data as closely as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fabaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we'll need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Here's our simple dataset: Hours Studied vs. Exam Score\n",
    "hours_studied = np.array([1, 2, 3, 4, 5])\n",
    "exam_score = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Let's visualize the data!\n",
    "plt.scatter(hours_studied, exam_score)\n",
    "plt.xlabel(\"Hours Studied\")\n",
    "plt.ylabel(\"Exam Score\")\n",
    "plt.title(\"Student Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07c439",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 1: Observe the Data\n",
    "\n",
    "Look at the plot above. Do you see a general trend? As the 'Hours Studied' increase, what tends to happen to the 'Exam Score'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938d568",
   "metadata": {},
   "source": [
    "## Topic 2: How Good is Our Prediction? - The Cost Function (MSE)\n",
    "\n",
    "Before we can find the 'best' line, we need a way to measure how 'bad' a line is. We do this with a **Cost Function**.\n",
    "\n",
    "The cost function calculates the difference between our model's predictions and the actual data points. We use the **Mean Squared Error (MSE)**, which sounds complex but is quite simple:\n",
    "\n",
    "1.  For each data point, calculate the **error** (the distance between the predicted value and the actual value).\n",
    "2.  **Square** each error (this makes them positive).\n",
    "3.  Calculate the **average** of all the squared errors.\n",
    "\n",
    "**Our goal is to find `Œ∏‚ÇÄ` and `Œ∏‚ÇÅ` that make this cost as low as possible!** A lower cost means our line is a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the cost function in Python\n",
    "def calculate_cost(theta0, theta1, x, y):\n",
    "    m = len(y) # Number of data points\n",
    "    predictions = theta0 + theta1 * x\n",
    "    squared_errors = (predictions - y) ** 2\n",
    "    cost = (1 / (2 * m)) * np.sum(squared_errors)\n",
    "    return cost\n",
    "\n",
    "# Let's test it with a guess: theta0 = 0 and theta1 = 0\n",
    "initial_cost = calculate_cost(0, 0, hours_studied, exam_score)\n",
    "print(f\"The initial cost with our first guess is: {initial_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f125e9",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 2: Experiment with Cost\n",
    "\n",
    "Let's try a better guess for `theta1`. We can see from the plot that the slope should be positive. \n",
    "\n",
    "Copy the code from the cell above and calculate the cost for `theta1 = 1`. Is the cost higher or lower? What does this tell you about your new guess?\n",
    "\n",
    "üß™ **Try changing the values of `theta0` and `theta1` to see if you can get the cost even lower!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bad595",
   "metadata": {},
   "source": [
    "## Topic 3: The Learning Algorithm - Gradient Descent\n",
    "\n",
    "Guessing `Œ∏‚ÇÄ` and `Œ∏‚ÇÅ` randomly is not very efficient. We need an algorithm that can find the best values for us. That algorithm is **Gradient Descent**!\n",
    "\n",
    "**üí° Analogy:** Imagine you are standing on a foggy mountain and want to get to the lowest point. You can't see the bottom, but you can feel the slope under your feet. You would take a small step in the steepest downhill direction. You repeat this process until you reach the valley floor.\n",
    "\n",
    "That's exactly what Gradient Descent does!\n",
    "*   The **'mountain'** is our cost function.\n",
    "*   The **'lowest point'** is the minimum cost.\n",
    "*   The **'step size'** is called the **Learning Rate (alpha, `Œ±`)**.\n",
    "\n",
    "It iteratively adjusts `Œ∏‚ÇÄ` and `Œ∏‚ÇÅ` in small steps to minimize the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e34517",
   "metadata": {},
   "source": [
    "## Topic 4: The Step-by-Step Way - Gradient Descent with Loops (Non-Vectorized)\n",
    "\n",
    "One way to implement Gradient Descent is by using `for` loops. We loop through our dataset many times (called 'iterations' or 'epochs'), and in each iteration, we update our `theta` values slightly.\n",
    "\n",
    "This approach is very intuitive and easy to understand because it follows the process one data point at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77dda6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training Complete!\n",
      "Final Theta0 (intercept): 1.8521278749603411\n",
      "Final Theta1 (slope): 0.6963550004885218\n"
     ]
    }
   ],
   "source": [
    "# Data (same as before)\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.01          # Learning Rate\n",
    "iterations = 1000     # Number of steps to take\n",
    "m = len(y)            # Number of data points\n",
    "theta0 = 0            # Initial guess for intercept\n",
    "theta1 = 0            # Initial guess for slope\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for _ in range(iterations):\n",
    "    sum_error0 = 0\n",
    "    sum_error1 = 0\n",
    "    \n",
    "    # Inner loop through each data point\n",
    "    for i in range(m):\n",
    "        prediction = theta0 + theta1 * x[i]\n",
    "        error = prediction - y[i]\n",
    "        sum_error0 += error\n",
    "        sum_error1 += error * x[i]\n",
    "    \n",
    "    # Calculate the gradients (the direction of the step)\n",
    "    grad0 = (1/m) * sum_error0\n",
    "    grad1 = (1/m) * sum_error1\n",
    "    \n",
    "    # Update the parameters (take the step downhill)\n",
    "    theta0 = theta0 - alpha * grad0\n",
    "    theta1 = theta1 - alpha * grad1\n",
    "\n",
    "print(f\"‚úÖ Training Complete!\")\n",
    "print(f\"Final Theta0 (intercept): {theta0}\")\n",
    "print(f\"Final Theta1 (slope): {theta1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61b7b4",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 3: The Learning Rate\n",
    "\n",
    "The learning rate `alpha` is a very important parameter.\n",
    "\n",
    "*   **Too small:** The algorithm will learn very slowly.\n",
    "*   **Too large:** It might overshoot the minimum and never find it!\n",
    "\n",
    "üß™ **Experiment:** Copy the code above. Change `alpha` to `0.5` and re-run. What happens to the final `theta` values? (You might see `inf` or `NaN`, which means it failed!). Now, try a very small value like `0.0001`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c14e84",
   "metadata": {},
   "source": [
    "## Topic 5: The Speedy Way - Vectorized Gradient Descent\n",
    "\n",
    "Using loops in Python can be slow, especially with large datasets. **Vectorization** is the process of using libraries like **NumPy** to perform operations on entire arrays (or 'vectors') at once.\n",
    "\n",
    "It's like calculating the predictions for all students at the same time, instead of one by one. This is much faster because NumPy's functions are written in low-level languages like C and are highly optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eff9494-f69e-4df4-bf1e-94790313ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "m = len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72088250-4b7c-4682-ae35-8e1e08e4755a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588f8f7d-6696-4fe3-927a-25f7524e403b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 2.],\n",
       "       [1., 3.],\n",
       "       [1., 4.],\n",
       "       [1., 5.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data for vectorization\n",
    "# We add a column of ones to 'x' to handle the theta0 term. This is a common trick!\n",
    "X = np.c_[np.ones(m), x]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89df441-2fac-4eda-bcd3-e0943f2a212a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34479240-6b5a-4b40-a729-cfa43ca8c8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa49cc5-a924-4362-ba80-c689aa0491ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(2) # theta is now a vector [theta0, theta1]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b669c70f-d6ce-45a0-ab21-18aadf93dbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e963bae-2f07-4661-b4a7-2fc8a50546c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232c179-278f-4c1b-b5c1-45ce1d635fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a81c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vectorized Gradient Descent\n",
    "for _ in range(iterations):\n",
    "    # Predictions for ALL examples at once!\n",
    "    predictions = X.dot(theta)\n",
    "    \n",
    "    # Errors for ALL examples at once!\n",
    "    errors = predictions - y\n",
    "    \n",
    "    # Gradients for BOTH thetas at once!\n",
    "    gradient = (1/m) * X.T.dot(errors)\n",
    "    \n",
    "    # Update BOTH thetas at once!\n",
    "    theta = theta - alpha * gradient\n",
    "\n",
    "print(f\"‚úÖ Vectorized Training Complete!\")\n",
    "print(f\"Final Theta vector [theta0, theta1]: {theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58009b18",
   "metadata": {},
   "source": [
    "### üéØ Practice Task 4: Understand the Shapes\n",
    "\n",
    "In vectorization, the shape (or dimensions) of your matrices is very important. Add a `print()` statement in the code cell above to check the `.shape` of `X`, `theta`, and `y`. Understanding these dimensions is key to working with matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e8eae",
   "metadata": {},
   "source": [
    "## üöÄ Final Revision Assignment (Time for Practice!)\n",
    "\n",
    "Congratulations on making it this far! Now it's time to put everything you've learned into practice with a final assignment. These tasks combine all the topics we've covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a64ab",
   "metadata": {},
   "source": [
    "### Task 1: Short Question (Theory)\n",
    "\n",
    "In your own words, explain the role of the learning rate (`Œ±`) in gradient descent. What happens if it's too large or too small?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5aa2d",
   "metadata": {},
   "source": [
    "### Task 2: New Dataset - Ice Cream Sales!\n",
    "\n",
    "Let's use a new dataset! We want to predict ice cream sales based on the temperature.\n",
    "\n",
    "**Data:**\n",
    "*   `temperature` = [20, 22, 25, 30, 33, 35]\n",
    "*   `sales` = [15, 20, 28, 40, 45, 52]\n",
    "\n",
    "Complete the code below to train a linear regression model on this new data using the **vectorized** approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the assignment\n",
    "temperature = np.array([20, 22, 25, 30, 33, 35])\n",
    "sales = np.array([15, 20, 28, 40, 45, 52])\n",
    "\n",
    "# --- YOUR CODE HERE --- #\n",
    "\n",
    "# 1. Set Parameters (alpha, iterations)\n",
    "alpha = 0.001 # Hint: You might need a smaller alpha for this data!\n",
    "iterations = 10000\n",
    "m = len(sales)\n",
    "\n",
    "# 2. Prepare the feature matrix 'X' with a column of ones\n",
    "# X = ...\n",
    "\n",
    "# 3. Initialize the theta vector with zeros\n",
    "# theta = ...\n",
    "\n",
    "# 4. Write the gradient descent loop\n",
    "# for _ in range(iterations):\n",
    "    # ... (calculate predictions, errors, gradient, and update theta)\n",
    "\n",
    "# 5. Print the final trained theta values\n",
    "# print(f\"Final Theta for ice cream sales: {theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988824d",
   "metadata": {},
   "source": [
    "### Task 3: Visualize the Cost Function\n",
    "\n",
    "It's very useful to see if our cost is actually decreasing. Modify your vectorized code from Task 2 to store the cost at each iteration and then plot it.\n",
    "\n",
    "You will need to:\n",
    "1. Create an empty list `cost_history = []` before the loop.\n",
    "2. Inside the loop, calculate the cost using the `calculate_cost` function we wrote earlier and append it to the list: `cost_history.append(cost)`.\n",
    "3. After the loop, use `matplotlib.pyplot` to plot the `cost_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this code to plot the cost history.\n",
    "# You will need to combine your vectorized gradient descent code\n",
    "# with the cost calculation at each step.\n",
    "\n",
    "cost_history = []\n",
    "\n",
    "# --- YOUR GRADIENT DESCENT LOOP HERE --- #\n",
    "\n",
    "\n",
    "# --- PLOTTING CODE --- #\n",
    "# plt.plot(cost_history)\n",
    "# plt.xlabel(\"Iterations\")\n",
    "# plt.ylabel(\"Cost\")\n",
    "# plt.title(\"Cost Decrease Over Time\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2e442",
   "metadata": {},
   "source": [
    "### Task 4: Make a Prediction!\n",
    "\n",
    "Using your final, trained `theta` values from the ice cream sales model, write a line of code to predict the number of sales on a day when the temperature is **28** degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22793ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final theta values (replace with your trained values)\n",
    "final_theta0 = 0 # Replace with your value\n",
    "final_theta1 = 0 # Replace with your value\n",
    "\n",
    "temperature_to_predict = 28\n",
    "\n",
    "# Calculate the predicted sales\n",
    "predicted_sales = final_theta0 + final_theta1 * temperature_to_predict\n",
    "\n",
    "print(f\"Predicted sales for {temperature_to_predict} degrees: {predicted_sales}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bd5aa",
   "metadata": {},
   "source": [
    "## üéâ You Did It! üéâ\n",
    "\n",
    "Excellent work! You have successfully built, trained, and used your very first machine learning model from scratch. You've learned about linear regression, how to measure its performance with a cost function, and how to train it with gradient descent using two different methods.\n",
    "\n",
    "This is a huge step in your AI journey. Keep experimenting and happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
