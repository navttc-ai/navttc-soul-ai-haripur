{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49fa385b",
   "metadata": {},
   "source": [
    "# ðŸ§  Introduction to Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc073b66",
   "metadata": {},
   "source": [
    "### Welcome to Your First Look at RNNs! ðŸ‘‹\n",
    "\n",
    "Welcome! In this 2-hour session, we'll dive into the exciting world of Recurrent Neural Networks (RNNs). These are special types of neural networks that have a 'memory,' making them perfect for understanding sequences like text, speech, and time-series data.\n",
    "\n",
    "Think about how you read a sentence: you understand each word based on the words that came before it. RNNs work in a similar way! \n",
    "\n",
    "#### ðŸ“˜ Learning Objectives for Today:\n",
    "\n",
    "1.  **What is an RNN?** Understand the core idea of 'memory' in neural networks.\n",
    "2.  **How do they work?** Learn about the simple math behind an RNN's feedback loop.\n",
    "3.  **Types of RNNs:** Discover different RNN architectures for different problems (e.g., sentiment analysis, text generation).\n",
    "4.  **Common Challenges:** Learn about the famous 'vanishing gradient' problem.\n",
    "5.  **Practice:** Get hands-on with simple coding tasks to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e890da4",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 1: The Core Idea - A Network with Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef9798",
   "metadata": {},
   "source": [
    "A regular neural network processes each input independently. If you show it two pictures, it doesn't remember the first picture when it sees the second one.\n",
    "\n",
    "**Recurrent Neural Networks (RNNs) are different.** They have a **loop** that allows them to pass information from one step to the next. This loop acts as a form of memory, called the **'hidden state'**.\n",
    "\n",
    "This memory allows an RNN to understand context in sequential data. For example, to understand the sentence \"The weather was great!\", the network needs to remember the word \"weather\" when it gets to the word \"great!\".\n",
    "\n",
    "![RNN Loop](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/1024px-Recurrent_neural_network_unfold.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84967fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal function with input 5: 10\n",
      "Normal function with input 10: 20\n",
      "\n",
      "---------------------\n",
      "\n",
      "RNN-like function with input 5: 5\n",
      "RNN-like function with input 10: 15\n"
     ]
    }
   ],
   "source": [
    "# Let's see a simple comparison with code.\n",
    "\n",
    "# A normal function has no memory of past calls\n",
    "def normal_function(input_value):\n",
    "    # It only knows about the current input\n",
    "    return input_value * 2\n",
    "\n",
    "print(f\"Normal function with input 5: {normal_function(5)}\")\n",
    "print(f\"Normal function with input 10: {normal_function(10)}\") # This call knows nothing about the previous one\n",
    "\n",
    "print(\"\\n---------------------\\n\")\n",
    "\n",
    "# An RNN-like function that keeps a 'memory' (or state)\n",
    "memory = 0\n",
    "def rnn_like_function(input_value):\n",
    "    global memory # Use the global memory variable\n",
    "    # The new output depends on the new input AND the old memory\n",
    "    new_output = input_value + memory \n",
    "    # Update the memory for the next call\n",
    "    memory = new_output\n",
    "    return new_output\n",
    "\n",
    "print(f\"RNN-like function with input 5: {rnn_like_function(5)}\")\n",
    "print(f\"RNN-like function with input 10: {rnn_like_function(10)}\") # This output is influenced by the previous call!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70239d8",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 1\n",
    "\n",
    "Think about your daily life. Can you describe one real-world task where remembering previous steps is crucial? (e.g., following a recipe). Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc93d91",
   "metadata": {},
   "source": [
    "*(Double-click here to write your answer)*\n",
    "\n",
    "**My Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841ce6d",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 2: How RNNs Work - The Recurrent Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f05384",
   "metadata": {},
   "source": [
    "So how does this 'memory' actually work? It's all about a simple formula that gets repeated at each step (or *time step*) in the sequence.\n",
    "\n",
    "The core of an RNN is the calculation of the **hidden state**, `h_t`, at a time `t`.\n",
    "\n",
    "The formula looks like this:\n",
    "\n",
    "**`h_t = f(W_xh * x_t + W_hh * h_{t-1} + b_h)`**\n",
    "\n",
    "Let's break that down:\n",
    "- **`h_t`**: The **new hidden state** (the memory for this step).\n",
    "- **`x_t`**: The **input** at the current time step (e.g., a vector for a word).\n",
    "- **`h_{t-1}`**: The **previous hidden state** (the memory from the last step).\n",
    "- **`W_xh`, `W_hh`**: These are the **weight matrices**. Think of them as knobs that the RNN learns to tune. `W_xh` adjusts the importance of the new input, and `W_hh` adjusts the importance of the old memory.\n",
    "- **`b_h`**: The **bias**, just another learned parameter.\n",
    "- **`f`**: An **activation function** (like `tanh` or `ReLU`) that adds non-linearity, helping the network learn complex patterns.\n",
    "![RNN Loop](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/1024px-Recurrent_neural_network_unfold.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacd1309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Hidden State (h_0): 0\n",
      "Current Input (x_1): 10\n",
      "New Hidden State (h_1): 0.9999\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate one step of an RNN with Python!\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume our inputs and states are just single numbers for simplicity.\n",
    "# In a real RNN, these would be vectors!\n",
    "\n",
    "# Input at time t=1\n",
    "x_1 = 10\n",
    "\n",
    "# Hidden state from the previous step (t=0). Let's start with 0.\n",
    "h_0 = 0\n",
    "\n",
    "# Let's define some fixed weights and bias (in a real RNN, these are learned)\n",
    "W_xh = 0.5\n",
    "W_hh = 0.2\n",
    "b_h = 0.1\n",
    "\n",
    "# We will use the tanh activation function\n",
    "# In numpy, it's np.tanh()\n",
    "\n",
    "# Calculate the new hidden state h_1\n",
    "h_1 = np.tanh(W_xh * x_1 + W_hh * h_0 + b_h)\n",
    "\n",
    "print(f\"Previous Hidden State (h_0): {h_0}\")\n",
    "print(f\"Current Input (x_1): {x_1}\")\n",
    "print(f\"New Hidden State (h_1): {h_1:.4f}\") # .4f formats to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1248af",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 2\n",
    "\n",
    "Now it's your turn! Using the code cell below, calculate the **next hidden state, `h_2`**. \n",
    "\n",
    "Use the `h_1` we just calculated as your new `h_{t-1}` and assume the next input in the sequence, `x_2`, is `20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Calculate h_2\n",
    "\n",
    "# The new input at time t=2\n",
    "x_2 = 20\n",
    "\n",
    "# The previous hidden state is the h_1 we calculated above\n",
    "h_1 = 0.9999 # Let's use the rounded value from the previous output\n",
    "\n",
    "# The weights and bias are the same\n",
    "W_xh = 0.5\n",
    "W_hh = 0.2\n",
    "b_h = 0.1\n",
    "\n",
    "# Calculate h_2 using the formula. Replace '...' with your calculation.\n",
    "h_2 = ...\n",
    "\n",
    "print(f\"New Hidden State (h_2): {h_2:.4f}\")\n",
    "\n",
    "# ðŸ§ª Try changing the values of x_2 or the weights and see how h_2 changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a0b50",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 3: Training an RNN - Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc992cb",
   "metadata": {},
   "source": [
    "So we have this formula, but how does the network *learn* the right values for the weights (`W_xh`, `W_hh`)?\n",
    "\n",
    "It uses a special version of backpropagation called **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "**The Idea:**\n",
    "1.  **Unroll the Network:** Imagine the sequence has 3 time steps. We can 'unroll' the RNN loop into a chain of 3 identical networks. The first network passes its hidden state to the second, the second to the third, and so on.\n",
    "2.  **Calculate Error:** The network makes predictions at each step, and we calculate the total error for the whole sequence.\n",
    "3.  **Backpropagate:** The error is sent backward through this unrolled chain, from the last step all the way to the first.\n",
    "4.  **Update Weights:** As the error travels back, it tells each weight how to adjust itself to reduce the error. Since the weights are shared across all time steps, the updates are combined.\n",
    "\n",
    "This allows the network to learn from its mistakes across the entire sequence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634131e1",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Fun Fact\n",
    "BPTT is the reason training RNNs can be computationally expensive and slow, especially for very long sequences. You are essentially training one very deep neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c931ff",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 4: A Common Challenge - Vanishing & Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a9f3f",
   "metadata": {},
   "source": [
    "During BPTT, we repeatedly multiply gradients by the `W_hh` weight matrix. This can lead to two major problems:\n",
    "\n",
    "1.  **ðŸ’¥ Exploding Gradients:** If the weights in `W_hh` are large (e.g., > 1.0), the gradients can get bigger and bigger as they flow backward in time. This is like a snowball rolling down a hill. The training becomes unstable, and the model can't learn.\n",
    "    *   **Solution:** *Gradient Clipping* - if a gradient gets too big, we just chop it off at a maximum value.\n",
    "\n",
    "2.  **ðŸ’§ Vanishing Gradients:** If the weights in `W_hh` are small (e.g., < 1.0), the gradients can get smaller and smaller until they effectively disappear. This is like a whisper in a long line of people â€“ by the end, the message is gone. This makes it very hard for the network to learn connections between distant events in a sequence (long-term dependencies).\n",
    "    *   **Solution:** Use more advanced RNN architectures like **LSTMs** and **GRUs**, which have special 'gates' to control the flow of information and prevent gradients from vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab889865",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 3\n",
    "\n",
    "Imagine you are training an RNN, and after a few iterations, the weights suddenly become `NaN` (Not a Number). Which problem are you most likely facing?\n",
    "\n",
    "a) Vanishing Gradients\n",
    "b) Exploding Gradients\n",
    "c) Overfitting\n",
    "\n",
    "*(Double-click here to write your answer, 'a' or 'b' or 'c')*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83509b87",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 5: Types of RNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc27f80",
   "metadata": {},
   "source": [
    "RNNs are flexible and can be built in different ways depending on the task. Here are the most common architectures:\n",
    "\n",
    "1.  **Many-to-One:**\n",
    "    - **Input:** A sequence (many)\n",
    "    - **Output:** A single value (one)\n",
    "    - **Example:** Sentiment analysis. You read a whole movie review (sequence) and classify it as 'positive' or 'negative' (single output).\n",
    "\n",
    "2.  **One-to-Many:**\n",
    "    - **Input:** A single value (one)\n",
    "    - **Output:** A sequence (many)\n",
    "    - **Example:** Image captioning. You give it one image and it generates a descriptive sentence (sequence of words).\n",
    "\n",
    "3.  **Many-to-Many (Equal Length):**\n",
    "    - **Input:** A sequence (many)\n",
    "    - **Output:** A sequence of the same length (many)\n",
    "    - **Example:** Named Entity Recognition. For each word in a sentence, you label it as a 'Person', 'Place', etc.\n",
    "\n",
    "4.  **Many-to-Many (Unequal Length):**\n",
    "    - **Input:** A sequence (many)\n",
    "    - **Output:** A sequence of a different length (many)\n",
    "    - **Example:** Machine translation. You translate a sentence from English to French, and the output sentence will likely have a different number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa9deb",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task 4\n",
    "\n",
    "Match the problem to the correct RNN architecture. Write your answers in the code cell below.\n",
    "\n",
    "**Problems:**\n",
    "1.  Predicting the next word in a sentence while you type.\n",
    "2.  Classifying an audio clip of a spoken digit (0-9).\n",
    "3.  Generating a piece of music from a single starting note.\n",
    "\n",
    "**Architectures:**\n",
    "- Many-to-One\n",
    "- One-to-Many\n",
    "- Many-to-Many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your answers here!\n",
    "answer_1 = \"...\" # e.g., \"Many-to-One\"\n",
    "answer_2 = \"...\"\n",
    "answer_3 = \"...\"\n",
    "\n",
    "print(f\"1. Text prediction is: {answer_1}\")\n",
    "print(f\"2. Audio classification is: {answer_2}\")\n",
    "print(f\"3. Music generation from a note is: {answer_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e5152",
   "metadata": {},
   "source": [
    "âœ… Well done! Understanding these architectures is key to applying RNNs correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebbe23",
   "metadata": {},
   "source": [
    "--- \n",
    "## Topic 6: Example in Action - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3939b",
   "metadata": {},
   "source": [
    "Let's walk through how a **Many-to-One** RNN would classify the sentiment of the sentence: **\"The movie was great!\"**\n",
    "\n",
    "1.  **Tokenize:** The sentence is split into words (tokens): `[\"The\", \"movie\", \"was\", \"great!\"]`\n",
    "2.  **Embed:** Each word is converted into a numerical vector (a word embedding). Let's pretend we have a simple embedding:\n",
    "    - `The` -> `[0.1, 0.2]`\n",
    "    - `movie` -> `[0.3, 0.5]`\n",
    "    - `was` -> `[0.1, 0.1]`\n",
    "    - `great!` -> `[0.9, 0.8]` (This vector might represent positivity)\n",
    "3.  **Process Sequentially:** The RNN processes one word vector at a time.\n",
    "    - **Step 1:** Input is `[0.1, 0.2]` (`The`). The RNN calculates a hidden state `h_1`.\n",
    "    - **Step 2:** Input is `[0.3, 0.5]` (`movie`). The RNN uses this input and `h_1` to calculate `h_2`.\n",
    "    - **Step 3:** Input is `[0.1, 0.1]` (`was`). The RNN uses this and `h_2` to calculate `h_3`.\n",
    "    - **Step 4:** Input is `[0.9, 0.8]` (`great!`). The RNN uses this and `h_3` to calculate the final hidden state `h_4`.\n",
    "4.  **Final Output:** The final hidden state `h_4` now contains information about the entire sentence. It is fed into a final classification layer, which outputs a single number (e.g., `0.95`), indicating a 95% probability that the review is positive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32cca0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: 'The movie was great'\n",
      "Processed 'the', New Hidden State is: 0.0\n",
      "Processed 'movie', New Hidden State is: 0.1\n",
      "Processed 'was', New Hidden State is: 0.1\n",
      "Processed 'great', New Hidden State is: 1.1\n",
      "Final Sentiment: Positive\n",
      "\n",
      "Analyzing: 'The movie was bad'\n",
      "Processed 'the', New Hidden State is: 0.0\n",
      "Processed 'movie', New Hidden State is: 0.1\n",
      "Processed 'was', New Hidden State is: 0.1\n",
      "Processed 'bad', New Hidden State is: -0.9\n",
      "Final Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# This is a pseudo-code example to show the logic.\n",
    "# We won't build a full RNN, but we can simulate the flow.\n",
    "\n",
    "def simple_rnn_sentiment(sentence):\n",
    "    # Step 1 & 2: Tokenize and Embed (let's use a simple score)\n",
    "    word_scores = {\"the\": 0.0, \"movie\": 0.1, \"was\": 0.0, \"great\": 1.0, \"bad\": -1.0}\n",
    "    tokens = sentence.lower().replace(\"!\", \"\").split()\n",
    "    \n",
    "    # Step 3: Process Sequentially (we'll just sum the scores)\n",
    "    hidden_state = 0.0\n",
    "    for token in tokens:\n",
    "        # The new hidden state is influenced by the old state and the new word\n",
    "        # This is a massive simplification of the real RNN formula!\n",
    "        hidden_state += word_scores.get(token, 0.0)\n",
    "        print(f\"Processed '{token}', New Hidden State is: {hidden_state:.1f}\")\n",
    "    \n",
    "    # Step 4: Final Output\n",
    "    if hidden_state > 0.5:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "\n",
    "# Let's test it!\n",
    "review_1 = \"The movie was great\"\n",
    "print(f\"Analyzing: '{review_1}'\")\n",
    "print(f\"Final Sentiment: {simple_rnn_sentiment(review_1)}\\n\")\n",
    "\n",
    "review_2 = \"The movie was bad\"\n",
    "print(f\"Analyzing: '{review_2}'\")\n",
    "print(f\"Final Sentiment: {simple_rnn_sentiment(review_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84318147",
   "metadata": {},
   "source": [
    "--- \n",
    "## ðŸ§  Final Revision Assignment\n",
    "\n",
    "Time to put it all together! These tasks are designed for you to practice at home to reinforce what you've learned. Try to solve them on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90716a",
   "metadata": {},
   "source": [
    "### Task 1: Calculate a Full Sequence\n",
    "\n",
    "You are given an input sequence of numbers: `[5, 10, 15]`. \n",
    "Your simple RNN has the following parameters:\n",
    "- Initial hidden state `h_0 = 0`.\n",
    "- Weights: `W_xh = 0.6`, `W_hh = 0.3`\n",
    "- Bias: `b_h = 0.0`\n",
    "- Activation function: `ReLU` (which is `max(0, x)`)\n",
    "\n",
    "In the code cell below, calculate `h_1`, `h_2`, and `h_3`. The formula is `h_t = max(0, W_xh * x_t + W_hh * h_{t-1} + b_h)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f66231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 Code\n",
    "import numpy as np\n",
    "\n",
    "sequence = [5, 10, 15]\n",
    "h_0 = 0\n",
    "W_xh = 0.6\n",
    "W_hh = 0.3\n",
    "b_h = 0.0\n",
    "\n",
    "# Define the ReLU function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Calculate h_1 (for x_1 = 5)\n",
    "x_1 = sequence[0]\n",
    "h_1 = ... # Your calculation here\n",
    "print(f\"h_1 is: {h_1}\")\n",
    "\n",
    "# Calculate h_2 (for x_2 = 10, using h_1)\n",
    "x_2 = sequence[1]\n",
    "h_2 = ... # Your calculation here\n",
    "print(f\"h_2 is: {h_2}\")\n",
    "\n",
    "# Calculate h_3 (for x_3 = 15, using h_2)\n",
    "x_3 = sequence[2]\n",
    "h_3 = ... # Your calculation here\n",
    "print(f\"h_3 is: {h_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9183b7",
   "metadata": {},
   "source": [
    "### Task 2: Explain in Your Own Words\n",
    "\n",
    "In the markdown cell below, explain the **Vanishing Gradient Problem**. Why does it make it hard for an RNN to learn from words at the beginning of a very long sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae984a",
   "metadata": {},
   "source": [
    "*(Double-click to write your explanation here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0736e0",
   "metadata": {},
   "source": [
    "### Task 3: Multiple Choice Review\n",
    "\n",
    "1. What is the primary advantage of RNNs over traditional feedforward neural networks?\n",
    "    a) They are faster to train.\n",
    "    b) They can handle sequential data by maintaining a memory of past inputs.\n",
    "    c) They require less data to train.\n",
    "\n",
    "2. More advanced RNNs like LSTMs use 'gates' primarily to solve what problem?\n",
    "    a) Overfitting\n",
    "    b) The need for more data\n",
    "    c) The vanishing and exploding gradient problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f12552",
   "metadata": {},
   "source": [
    "### Task 4: Propose an Architecture\n",
    "\n",
    "A company wants to build a chatbot that answers customer questions. The chatbot will read a customer's question (a sequence of words) and generate a relevant answer (another sequence of words).\n",
    "\n",
    "What RNN architecture would be most suitable for this, and why? \n",
    "\n",
    "*(Write your answer in the markdown cell below.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41d109",
   "metadata": {},
   "source": [
    "*(Double-click to write your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b25ec4",
   "metadata": {},
   "source": [
    "### Task 5: What's the Role of the Hidden State?\n",
    "\n",
    "Briefly explain the role of the hidden state in a Recurrent Neural Network. What information does it carry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384b4eb",
   "metadata": {},
   "source": [
    "*(Double-click to write your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea40980",
   "metadata": {},
   "source": [
    "--- \n",
    "## Summary & Further Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57661a87",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Key Takeaways\n",
    "\n",
    "- **RNNs are for Sequential Data:** Their defining feature is the ability to process sequences by maintaining a 'memory' or **hidden state**.\n",
    "- **The Power of the Loop:** The recurrent connection allows information to be passed from one time step to the next.\n",
    "- **Training with BPTT:** RNNs are trained using Backpropagation Through Time.\n",
    "- **Beware of Gradients:** Simple RNNs are susceptible to the vanishing and exploding gradient problems.\n",
    "- **Architectural Diversity:** RNNs come in various forms (many-to-one, one-to-many, etc.) to suit different problems.\n",
    "\n",
    "Congratulations on completing this introduction to RNNs! This is a foundational concept in modern AI, and understanding it well will help you learn more advanced models like LSTMs, GRUs, and Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1f371",
   "metadata": {},
   "source": [
    "### ðŸ”— Related Study Resources\n",
    "\n",
    "*   **[Understanding LSTM Networks by Christopher Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)**: A classic and highly intuitive blog post explaining the inner workings of LSTMs.\n",
    "*   **[Coursera: \"Sequence Models\" by Andrew Ng](https://www.coursera.org/learn/nlp-sequence-models)**: A comprehensive online course covering RNNs, LSTMs, GRUs, and their applications.\n",
    "*   **[TensorFlow RNN Guide](https://www.tensorflow.org/guide/keras/rnn)**: Official tutorial for implementing RNNs in TensorFlow.\n",
    "*   **[PyTorch RNN Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)**: Official tutorial for implementing RNNs in PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
