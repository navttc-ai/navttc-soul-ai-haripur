{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbb9e5b",
   "metadata": {},
   "source": [
    "# ðŸŽ¬ Introduction to Recurrent Neural Networks (RNNs) for Movie Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc3847",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Welcome to Your First AI Text Analysis Project!\n",
    "\n",
    "Over the next 2 hours, we'll embark on an exciting journey to teach a machine how to understand human sentiment in movie reviews. We'll be using a special type of neural network called a **Recurrent Neural Network (RNN)**, which is fantastic for understanding sequences of data, like text!\n",
    "\n",
    "**ðŸŽ¯ Learning Objectives:**\n",
    "1.  Understand what an RNN is and why it's used for text.\n",
    "2.  Load and prepare the famous IMDB movie review dataset.\n",
    "3.  Build a simple RNN model using TensorFlow and Keras.\n",
    "4.  Train the model to classify reviews as 'Positive' ðŸ‘ or 'Negative' ðŸ‘Ž.\n",
    "5.  Test our model's accuracy.\n",
    "6.  Use the trained model to predict the sentiment of your own movie review!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0f24",
   "metadata": {},
   "source": [
    "## Topic 1: What is an RNN? ðŸ¤”\n",
    "\n",
    "Imagine reading a sentence. You don't just read each word in isolation; you remember the words that came before it to understand the full meaning. An RNN works in a similar way! It has a 'memory' that allows it to retain information from previous steps in a sequence.\n",
    "\n",
    "This makes RNNs perfect for tasks like:\n",
    "- **Sentiment Analysis:** (What we're doing today!) Is a review positive or negative?\n",
    "- **Machine Translation:** Translating a sentence from one language to another.\n",
    "- **Text Generation:** Writing new text that looks like it was written by a human.\n",
    "\n",
    "ðŸ’¡ **Fun Fact:** Your phone's autocomplete feature is likely powered by a type of RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c7eca",
   "metadata": {},
   "source": [
    "## Topic 2: Setting Up Our Workspace and Loading Data ðŸ“‚\n",
    "\n",
    "First, we need to import the tools we'll be using from TensorFlow and Keras. We'll then load the IMDB movie review dataset, which is conveniently built into Keras. The dataset contains 50,000 movie reviews, already pre-processed into sequences of numbers where each number represents a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f75ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25000\n",
      "Number of test samples: 25000\n",
      "\n",
      "Here's what a single review looks like (it's a list of numbers):\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Load the IMDB dataset\n",
    "# We'll only keep the top 10,000 most frequently occurring words\n",
    "top_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "print(f\"Number of training samples: {len(x_train)}\")\n",
    "print(f\"Number of test samples: {len(x_test)}\")\n",
    "print(\"\\nHere's what a single review looks like (it's a list of numbers):\")\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89285fae",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 1\n",
    "\n",
    "Check the label of the first training review we printed above. The labels are either `1` (for positive) or `0` (for negative). Print the label of the first training sample (`y_train[0]`) to see if it was a positive or negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7af57e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for the first review: 1\n"
     ]
    }
   ],
   "source": [
    "# Your code here: Print the label of the first training review\n",
    "print(f\"Label for the first review: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3cbc1",
   "metadata": {},
   "source": [
    "## Topic 3: Preparing the Data (Padding Sequences) ðŸ“\n",
    "\n",
    "Neural networks require inputs to be of the same size. However, movie reviews have different lengths! To solve this, we'll use a technique called **padding**. We'll set a maximum review length, and any review shorter than that will be filled with zeros at the end. Longer reviews will be truncated.\n",
    "\n",
    "This ensures that every review we feed into our RNN has the exact same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f504d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data after padding: (25000, 500)\n",
      "Shape of testing data after padding: (25000, 500)\n",
      "\n",
      "Here's the first review after padding (notice the zeros at the beginning):\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
      "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      " 4472  113  103   32   15   16 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum review length\n",
    "max_review_length = 500\n",
    "\n",
    "# Pad the training and testing sequences\n",
    "x_train = pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "print(\"Shape of training data after padding:\", x_train.shape)\n",
    "print(\"Shape of testing data after padding:\", x_test.shape)\n",
    "print(\"\\nHere's the first review after padding (notice the zeros at the beginning):\")\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1304021",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 2\n",
    "\n",
    "What if we wanted to allow for longer reviews? Change `max_review_length` to `600` in the code cell above and re-run it. Observe how the shape of the training data changes. Then, change it back to `500`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116054c6",
   "metadata": {},
   "source": [
    "## Topic 4: Building the RNN Model ðŸ—ï¸\n",
    "\n",
    "Now for the fun part! We'll build our RNN model layer by layer using Keras's `Sequential` API.\n",
    "\n",
    "1.  **Embedding Layer:** This layer is very clever. It takes the integer-encoded words (like `[1, 24, 18, ...]`) and turns them into dense vectors of a fixed size. This helps the network learn the meaning and context of words.\n",
    "2.  **SimpleRNN Layer:** This is the heart of our model. It processes the sequence of word vectors one by one, remembering what it has seen.\n",
    "3.  **Dense Layer:** A standard fully connected neural network layer that will take the output from the RNN and make a final prediction (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09bf7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# 1. Add an Embedding layer\n",
    "# It will create 32-dimensional vectors for each of our 10,000 words.\n",
    "model.add(Embedding(top_words, 32, input_length=max_review_length))\n",
    "\n",
    "# 2. Add a SimpleRNN layer with 32 memory units\n",
    "model.add(SimpleRNN(32))\n",
    "\n",
    "# 3. Add the output Dense layer with a 'sigmoid' activation function\n",
    "# Sigmoid is great for binary (0 or 1) classification.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "# We define the loss function, the optimizer, and the metric we want to track.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of our model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a120c",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 3\n",
    "\n",
    "In the `model.summary()`, can you identify how many parameters the `simple_rnn` layer has to learn? Why do you think it's that many? (Hint: It relates to the input from the embedding layer, its own internal state, and a bias.) This is just a thought exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794731a",
   "metadata": {},
   "source": [
    "## Topic 5: Training the Model ðŸ‹ï¸â€â™‚ï¸\n",
    "\n",
    "It's time to train our model! We'll use the `.fit()` method, feeding it our prepared training data (`x_train`, `y_train`). The model will learn by comparing its predictions to the actual labels and adjusting its internal parameters to get better.\n",
    "\n",
    "We'll train for a few `epochs`. One epoch is one complete pass through the entire training dataset. \n",
    "\n",
    "ðŸ§ª **Note:** This step will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5116b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "313/313 [==============================] - 62s 187ms/step - loss: 0.5906 - accuracy: 0.6797 - val_loss: 0.4690 - val_accuracy: 0.7854\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 59s 187ms/step - loss: 0.5069 - accuracy: 0.7606 - val_loss: 0.6146 - val_accuracy: 0.6490\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 47s 149ms/step - loss: 0.4008 - accuracy: 0.8285 - val_loss: 0.4489 - val_accuracy: 0.8026\n"
     ]
    }
   ],
   "source": [
    "# Train the model!\n",
    "# We'll use 3 epochs and a batch size of 64.\n",
    "history = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b87a0",
   "metadata": {},
   "source": [
    "## Topic 6: Evaluating the Model ðŸ“Š\n",
    "\n",
    "Once training is complete, we need to see how well our model performs on data it has never seen before. That's what our test set (`x_test`, `y_test`) is for. We'll check the final accuracy.\n",
    "\n",
    "A good accuracy means our model can generalize well to new, unseen movie reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f017fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model Evaluation Complete!\n",
      "Accuracy on test data: 80.86%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nâœ… Model Evaluation Complete!\")\n",
    "print(f\"Accuracy on test data: {scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a295a9",
   "metadata": {},
   "source": [
    "## Topic 7: Making Predictions on Your Own Review! âœï¸\n",
    "\n",
    "This is the most exciting part! Let's write our own movie review and see what our AI model thinks about it.\n",
    "\n",
    "To do this, we need to perform the same preprocessing steps on our text as we did for the training data:\n",
    "1.  Convert the words to their corresponding integer IDs.\n",
    "2.  Pad the sequence to ensure it's 500 elements long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "819276a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n",
      "Your review: 'this movie was not fantastic and brilliant the acting was bad'\n",
      "Prediction Score: 0.24649077653884888\n",
      "Predicted Sentiment: Negative ðŸ‘\n"
     ]
    }
   ],
   "source": [
    "# Get the word-to-index mapping from the IMDB dataset\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Function to preprocess a new review\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    # Convert words to their integer index\n",
    "    # We add 3 to the index because 0, 1, and 2 are reserved for padding, start, etc.\n",
    "    encoded_review = [word_index.get(word, 2) + 3 for word in words]\n",
    "    # Pad the sequence\n",
    "    padded_review = pad_sequences([encoded_review], maxlen=max_review_length)\n",
    "    return padded_review\n",
    "\n",
    "# Let's test it with a new review\n",
    "my_review = \"this movie was not fantastic and brilliant the acting was bad\"\n",
    "processed_review = preprocess_text(my_review)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(processed_review)\n",
    "sentiment = \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
    "\n",
    "print(f\"Your review: '{my_review}'\")\n",
    "print(f\"Prediction Score: {prediction[0][0]}\")\n",
    "print(f\"Predicted Sentiment: {sentiment} ðŸ‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8549799",
   "metadata": {},
   "source": [
    "### ðŸ§  Practice Task 4\n",
    "\n",
    "Now it's your turn! In the code cell below, create your own review (positive or negative) and see what the model predicts. \n",
    "\n",
    "1. Create a variable `my_new_review` with your text.\n",
    "2. Use the `preprocess_text` function on it.\n",
    "3. Use `model.predict` to get the score.\n",
    "4. Print the final sentiment. \n",
    "\n",
    "ðŸ§ª **Experiment:** Try a review with mixed sentiment like \"The movie was long and boring but the ending was a surprise\" and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1661582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n",
      "Your review: 'it was a complete waste of time and the plot was terrible'\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# 1. Write your own review here!\n",
    "my_new_review = \"it was a complete waste of time and the plot was terrible\"\n",
    "\n",
    "# 2. Preprocess the review\n",
    "processed_new_review = preprocess_text(my_new_review)\n",
    "\n",
    "# 3. Make a prediction\n",
    "new_prediction = model.predict(processed_new_review)\n",
    "new_sentiment = \"Positive\" if new_prediction[0][0] > 0.5 else \"Negative\"\n",
    "\n",
    "# 4. Print the results\n",
    "print(f\"Your review: '{my_new_review}'\")\n",
    "print(f\"Predicted Sentiment: {new_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ce983",
   "metadata": {},
   "source": [
    "## ðŸš€ Final Revision Assignment\n",
    "\n",
    "Congratulations on building your first sentiment analysis model! To reinforce what you've learned, here are a few tasks to practice at home.\n",
    "\n",
    "**Task 1:** Find a real movie review online (from a site like Rotten Tomatoes or IMDb) and test it with the prediction code.\n",
    "\n",
    "**Task 2:** Re-run the model training (`model.fit(...)`) but this time for `5` epochs instead of `3`. Does the final accuracy on the test set improve or get worse?\n",
    "\n",
    "**Task 3:** In the model building step, change the number of memory units in the `SimpleRNN` layer from `32` to `64`. Re-train and evaluate the model. How does this change affect the number of trainable parameters and the final accuracy?\n",
    "\n",
    "**Task 4:** Write a very short, one-word review like \"amazing\" or \"bad\". Does the model correctly predict the sentiment?\n",
    "\n",
    "**Task 5:** The `preprocess_text` function uses `word_index.get(word, 2)`. The `2` is a default value for words not found in the vocabulary. Try writing a review with lots of made-up words (e.g., \"The flibbertigibbet was truly scrumdiddlyumptious\") and see how the model responds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
