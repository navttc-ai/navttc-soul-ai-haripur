{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26295bd6",
   "metadata": {},
   "source": [
    "# ðŸŽ¬ LSTM for IMDb Movie Review Sentiment Analysis\n",
    "\n",
    "Welcome to this 2-hour session on using Long Short-Term Memory (LSTM) networks for sentiment analysis! We'll build a model to predict whether a movie review is positive or negative. \n",
    "\n",
    "### ðŸ“˜ Learning Objectives:\n",
    "\n",
    "1.  **Understand Sentiment Analysis:** Learn what it is and why it's important.\n",
    "2.  **Prepare Text Data:** Preprocess the IMDb movie review dataset for our AI model.\n",
    "3.  **Build an LSTM Model:** Create a neural network using LSTMs, a special type of Recurrent Neural Network (RNN).\n",
    "4.  **Train & Evaluate:** Train our model and see how well it performs.\n",
    "5.  **Test with Your Own Reviews:** Predict the sentiment of any review you type!\n",
    "6.  **Why LSTMs are better than RNNs for text:** Understand the improvement LSTMs offer for handling long sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542c95b",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Topic 1: What is Sentiment Analysis?\n",
    "\n",
    "Sentiment analysis is the process of determining the emotional tone behind a series of words. In simple terms, it's figuring out if a piece of text is positive, negative, or neutral. \n",
    "\n",
    "For example:\n",
    "-   `\"This movie was fantastic!\"` -> **Positive**\n",
    "-   `\"I did not enjoy the film at all.\"` -> **Negative**\n",
    "\n",
    "We'll be using the famous IMDb dataset, which contains 50,000 movie reviews labeled as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b48ca5",
   "metadata": {},
   "source": [
    "## âš™ï¸ Topic 2: Setting Up Our Environment\n",
    "\n",
    "First, let's import the necessary libraries from TensorFlow and Keras. These tools help us build and train our AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa3a74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5987e",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Topic 3: Loading and Preparing the Data\n",
    "\n",
    "We will load the IMDb dataset. The dataset comes pre-processed where words are already converted into numbers (integers). We'll limit our vocabulary to the top 10,000 most frequent words.\n",
    "\n",
    "Then, we need to make sure all our reviews are the same length. We use `pad_sequences` to make every review 200 words long. Shorter reviews get padded with zeros, and longer ones are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da590b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 25000\n",
      "Test examples: 25000\n",
      "\n",
      "Shape of padded training data: (25000, 200)\n",
      "Shape of padded test data: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = 10000  # Number of words to consider as features\n",
    "maxlen = 200       # Max length of reviews (in words)\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(f\"Training examples: {len(x_train)}\")\n",
    "print(f\"Test examples: {len(x_test)}\")\n",
    "\n",
    "# --- Padding Sequences ---\n",
    "# Make all sequences (reviews) have the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(f\"\\nShape of padded training data: {x_train.shape}\")\n",
    "print(f\"Shape of padded test data: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ef9bf",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task: Check the Data\n",
    "\n",
    "Let's inspect one of the processed reviews to see what it looks like. It's just a list of numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df24910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review (as numbers):\n",
      "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
      "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
      "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
      "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
      "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
      "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
      " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
      "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
      "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
      "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
      " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
      "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
      "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
      "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
      " 5345   19  178   32]\n",
      "\n",
      "Label of first review: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the first review from the training set\n",
    "print(\"First review (as numbers):\")\n",
    "print(x_train[0])\n",
    "\n",
    "# The label '1' means positive, '0' means negative\n",
    "print(\"\\nLabel of first review:\", y_train[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ea73a",
   "metadata": {},
   "source": [
    "## ðŸ§  Topic 4: Why LSTMs are Better than Simple RNNs\n",
    "\n",
    "A simple Recurrent Neural Network (RNN) has a major problem: the **vanishing gradient problem**. \n",
    "\n",
    "Imagine you're reading a very long review:\n",
    "\n",
    "`\"I went to see this film last week with my friends, and while the popcorn was a bit stale, the cinematography reminded me of the classic movies from the 1960s, which I adore, but the plot was so confusing and poorly written that by the end, I have to say it was a **terrible** experience.\"`\n",
    "\n",
    "A simple RNN might forget the important early parts of the sentence (like `\"...popcorn was stale...\"` or `\"...cinematography reminded me of classic movies...\"`) by the time it reaches the end word (`\"terrible\"`). It struggles to maintain context over long sequences.\n",
    "\n",
    "**LSTMs solve this!** They have a special memory cell and gates (input, output, forget gates) that allow them to remember or forget information selectively. This makes them excellent at understanding the context in long sentences, leading to much better performance on tasks like sentiment analysis of detailed reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51dcc46",
   "metadata": {},
   "source": [
    "## ðŸ¤– Topic 5: Building the LSTM Model\n",
    "\n",
    "Now for the exciting part! We'll build our neural network layer by layer using Keras `Sequential` API.\n",
    "\n",
    "1.  **Embedding Layer:** This turns the integer numbers (representing words) into dense vectors of a fixed size. It helps the model learn the relationships between words.\n",
    "2.  **LSTM Layer:** This is the core of our model. We use a `Bidirectional` wrapper, which means it reads the review from start-to-end and end-to-start, capturing even more context!\n",
    "3.  **Dense Layer:** A final, fully connected layer that outputs a single value, which will tell us if the review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542f511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential([\n",
    "    # 1. Embedding Layer: Turns word indices into dense vectors of 128 dimensions\n",
    "    Embedding(vocab_size, 128, input_length=maxlen),\n",
    "    \n",
    "    # 2. LSTM Layer: Processes the sequence of vectors. Bidirectional helps learn from both directions.\n",
    "    Bidirectional(LSTM(64, return_sequences=False)), # return_sequences=False as we need output only at the end\n",
    "    \n",
    "    # 3. Dense Layer: The final output layer with a sigmoid activation for binary classification (0 or 1)\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9ba92",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸â€â™€ï¸ Topic 6: Training the Model\n",
    "\n",
    "Now, we feed our training data (`x_train`, `y_train`) to the model. The model will learn to associate sequences of words with their sentiment (positive/negative).\n",
    "\n",
    "This step will take a few minutes. Watch the `accuracy` and `val_accuracy` metrics increase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e8ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model... this might take a few minutes.\n",
      "Epoch 1/3\n",
      "157/157 [==============================] - 67s 392ms/step - loss: 0.4694 - accuracy: 0.7596 - val_loss: 0.3176 - val_accuracy: 0.8686\n",
      "Epoch 2/3\n",
      "157/157 [==============================] - 60s 385ms/step - loss: 0.2400 - accuracy: 0.9080 - val_loss: 0.3057 - val_accuracy: 0.8774\n",
      "Epoch 3/3\n",
      "157/157 [==============================] - 60s 384ms/step - loss: 0.1638 - accuracy: 0.9406 - val_loss: 0.3403 - val_accuracy: 0.8732\n",
      "\n",
      "âœ… Model training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model... this might take a few minutes.\")\n",
    "\n",
    "# We'll use a portion of training data for validation during training\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=3, # We'll train for 3 full cycles\n",
    "                    batch_size=128, # Process 128 reviews at a time\n",
    "                    validation_split=0.2) # Use 20% of training data for validation\n",
    "\n",
    "print(\"\\nâœ… Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7dffe",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Topic 7: Evaluating the Model\n",
    "\n",
    "Let's see how well our trained model performs on the test data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d787f309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3687 - accuracy: 0.8602\n",
      "\n",
      "Test Accuracy: 86.02%\n",
      "Test Loss: 0.3687\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80648df6",
   "metadata": {},
   "source": [
    "## ðŸ§ª Topic 8: Testing with Your Own Reviews!\n",
    "\n",
    "This is the most fun part. Let's write a function that takes your review, processes it, and predicts the sentiment using our trained model.\n",
    "\n",
    "We need to convert your text into the same numerical format the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0faa75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the word-to-index mapping from the IMDb dataset\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Create a reverse word index (index -> word)\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "# The first few indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # UNK for unknown words\n",
    "\n",
    "def predict_sentiment(review_text):\n",
    "    \"\"\"Preprocesses a user's review and predicts its sentiment.\"\"\"\n",
    "    # Convert review to lowercase and split into words\n",
    "    words = tf.keras.preprocessing.text.text_to_word_sequence(review_text)\n",
    "    \n",
    "    # Convert words to their integer indices\n",
    "    encoded_review = [word_index.get(word, 2) for word in words] # Use 2 for unknown words\n",
    "    \n",
    "    # Pad the sequence to the same length (200)\n",
    "    padded_review = pad_sequences([encoded_review], maxlen=maxlen)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(padded_review)\n",
    "    \n",
    "    # Interpret the prediction\n",
    "    if prediction[0][0] > 0.5:\n",
    "        print(f\"Prediction Score: {prediction[0][0]:.4f} -> ðŸ˜Š Positive Review\")\n",
    "    else:\n",
    "        print(f\"Prediction Score: {prediction[0][0]:.4f} -> ðŸ˜ž Negative Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96877def",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Practice Task: Write Your Own Reviews!\n",
    "\n",
    "Now it's your turn! Try different reviews in the cell below and see what the model predicts. Test it with clearly positive, clearly negative, and even more complex, longer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4585e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing a positive review:\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Prediction Score: 0.8839 -> ðŸ˜Š Positive Review\n",
      "--------------------------------------------------\n",
      "Testing a negative review:\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Prediction Score: 0.0114 -> ðŸ˜ž Negative Review\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ§ª Now, write your own review inside the quotes and run the cell!\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Prediction Score: 0.4573 -> ðŸ˜ž Negative Review\n"
     ]
    }
   ],
   "source": [
    "# --- Positive Review Example ---\n",
    "my_review_1 = \"This film was absolutely brilliant. The acting was superb and the storyline was gripping from start to finish. I would recommend it to everyone.\"\n",
    "print(\"Testing a positive review:\")\n",
    "predict_sentiment(my_review_1)\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- Negative Review Example ---\n",
    "my_review_2 = \"I was so disappointed with this movie. The plot was incredibly boring and the characters were not believable at all. I almost walked out of the theater.\"\n",
    "print(\"Testing a negative review:\")\n",
    "predict_sentiment(my_review_2)\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"\\nðŸ§ª Now, write your own review inside the quotes and run the cell!\")\n",
    "your_own_review = \"Replace this text with your movie review.\"\n",
    "predict_sentiment(your_own_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07491acb",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Final Revision Assignment\n",
    "\n",
    "Congratulations on building your first sentiment analysis model with LSTMs! To practice and reinforce what you've learned, here are a few tasks you can try at home.\n",
    "\n",
    "**Task 1: The Sarcastic Review**\n",
    "-   Write a sarcastic review like `\"Wow, another two hours of my life I'll never get back. Truly a masterpiece of boring cinema.\"` and see if the model can correctly classify it as negative.\n",
    "\n",
    "**Task 2: The Mixed Feelings Review**\n",
    "-   Write a review with both positive and negative points, like `\"The special effects were incredible, but the story was weak and predictable.\"` See which way the model leans.\n",
    "\n",
    "**Task 3: Change Model Parameters**\n",
    "-   Go back to the model building cell (`Topic 5`). Try changing the number of LSTM units from `64` to `32` or `128`. Re-train the model and see how the accuracy changes.\n",
    "\n",
    "**Task 4: Change `maxlen`**\n",
    "-   In `Topic 3`, change the `maxlen` parameter from `200` to `300`. How does this affect training time and accuracy? Do you think it would help or hurt the model's performance?\n",
    "\n",
    "**Task 5: Explore the Vocabulary**\n",
    "-   Write a short script to see what word corresponds to a specific number. For example, what word is represented by the number `50`? (Hint: Use the `reverse_word_index` we created).\n",
    "\n",
    "**Task 6 (Advanced): Try a Different Optimizer**\n",
    "-   In the `model.compile()` step, change the optimizer from `'adam'` to `'rmsprop'`. Re-train and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
