{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed14754",
   "metadata": {},
   "source": [
    "# ü§ñ Introduction to Gated Recurrent Units (GRUs) for AI Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131ba5a",
   "metadata": {},
   "source": [
    "### üìò Welcome to Your 2-Hour Guide to GRUs!\n",
    "\n",
    "Hello and welcome! In this session, we're going to explore a powerful tool in AI called the **Gated Recurrent Unit (GRU)**. Think of it as a special type of neural network with a great memory, perfect for understanding sequences like text, speech, or stock prices.\n",
    "\n",
    "A regular Recurrent Neural Network (RNN) can sometimes forget important information from the beginning of a long sequence. GRUs were invented to solve this exact problem using a clever 'gating' mechanism.\n",
    "\n",
    "--- \n",
    "\n",
    "### üéØ Learning Objectives for Today:\n",
    "\n",
    "By the end of this 2-hour session, you will be able to:\n",
    "1.  **Understand** what a GRU is and why it's useful.\n",
    "2.  **Explain** the role of the `Update Gate` and `Reset Gate`.\n",
    "3.  **Follow** a step-by-step mathematical example of a GRU at work.\n",
    "4.  **Build and train** your own simple GRU model in Python using TensorFlow/Keras.\n",
    "5.  **Compare** GRUs with their famous cousin, LSTMs.\n",
    "6.  **Identify** real-world applications where GRUs shine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a357538",
   "metadata": {},
   "source": [
    "## Topic 1: What is a GRU and Why Do We Need It? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e550dc",
   "metadata": {},
   "source": [
    "A Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN). Standard RNNs are great for processing sequences, but they suffer from the **vanishing gradient problem**. \n",
    "\n",
    "**What does that mean in simple terms?** Imagine you're reading a very long book. A simple RNN might forget important details from the first chapter by the time it reaches the last one. The 'memory' fades over time.\n",
    "\n",
    "GRUs solve this by using special gates that control the flow of information. These gates allow the network to decide what information is important to **keep** and what to **forget**, enabling it to remember context over very long sequences. This makes them amazing for tasks like:\n",
    "\n",
    "- üó£Ô∏è Natural Language Processing\n",
    "- üìà Time Series Analysis\n",
    "- üé§ Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d157f",
   "metadata": {},
   "source": [
    "## Topic 2: The Architecture of a GRU Cell üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90844e29",
   "metadata": {},
   "source": [
    "The magic of a GRU happens inside its 'cell'. At each step in a sequence, the cell takes two things:\n",
    "1. The current input (`x_t`)\n",
    "2. The memory from the previous step (`h_{t-1}`)\n",
    "\n",
    "It then uses two special gates to produce the new memory (`h_t`).\n",
    "\n",
    "###  Gate 1: The Reset Gate (r_t) gate\n",
    "**Job:** Decides how much of the *past memory* to forget.\n",
    "\n",
    "It looks at the current input and the previous memory to decide which parts of the old memory are irrelevant now. For example, if a new sentence starts, it might decide to 'reset' the memory of the previous sentence.\n",
    "\n",
    "`r_t = œÉ(W_r * [h_{t-1}, x_t])`\n",
    "\n",
    "### Gate 2: The Update Gate (z_t) üß†\n",
    "**Job:** Decides how much of the *new information* to add and how much of the *old memory* to keep.\n",
    "\n",
    "This is the most important gate! It balances between keeping the old memory and updating it with new information. If the update gate is set to 'keep', it can pass important information along for many, many steps.\n",
    "\n",
    "`z_t = œÉ(W_z * [h_{t-1}, x_t])`\n",
    "\n",
    "--- \n",
    "\n",
    "These gates work together to create a **final hidden state** (`h_t`), which is a smart combination of the previous memory and new candidate memory.\n",
    "\n",
    "`h_t = (1 - z_t) * h_{t-1} + z_t * hÃÉ_t`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87546b20",
   "metadata": {},
   "source": [
    "## Topic 3: A Step-by-Step Mathematical Example üî¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd06091",
   "metadata": {},
   "source": [
    "Let's see how these gates work with some numbers. Don't worry about the complex math, we'll use Python to do the calculations. The goal is to see how an input and a previous memory state combine to create a new memory state.\n",
    "\n",
    "**Assumptions:**\n",
    "- Current Input `x_t` = `[0.8]`\n",
    "- Previous Hidden State `h_{t-1}` = `[0.2]`\n",
    "- All weights are `1` and biases are `0` for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab10112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reset Gate (r_t) output: 0.73\n",
      "2. Update Gate (z_t) output: 0.73\n",
      "3. Candidate Hidden State (hÃÉ_t) output: 0.74\n",
      "\n",
      "4. üèÜ Final Hidden State (h_t) is: 0.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# --- Inputs ---\n",
    "x_t = 0.8\n",
    "h_t_minus_1 = 0.2\n",
    "\n",
    "# --- Simplified weights and biases ---\n",
    "W_r, W_z, W_h = 1, 1, 1\n",
    "\n",
    "# --- Let's calculate! ---\n",
    "\n",
    "# 1. Reset Gate: How much of the past to forget?\n",
    "# We combine the input and previous hidden state\n",
    "combined_input = h_t_minus_1 + x_t # Simplified from concatenation for this example\n",
    "r_t = sigmoid(W_r * combined_input)\n",
    "print(f\"1. Reset Gate (r_t) output: {r_t:.2f}\")\n",
    "\n",
    "# 2. Update Gate: How much new info to let in?\n",
    "z_t = sigmoid(W_z * combined_input)\n",
    "print(f\"2. Update Gate (z_t) output: {z_t:.2f}\")\n",
    "\n",
    "# 3. Candidate Hidden State: What's the 'new' potential memory?\n",
    "# Note: The reset gate (r_t) influences this calculation!\n",
    "h_tilde_t = tanh(W_h * (r_t * h_t_minus_1 + x_t))\n",
    "print(f\"3. Candidate Hidden State (hÃÉ_t) output: {h_tilde_t:.2f}\")\n",
    "\n",
    "# 4. Final Hidden State: The final, updated memory!\n",
    "# This is a mix of the old memory and the candidate memory, controlled by the update gate.\n",
    "h_t = (1 - z_t) * h_t_minus_1 + z_t * h_tilde_t\n",
    "print(f\"\\n4. üèÜ Final Hidden State (h_t) is: {h_t:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1fb03",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "üß™ **Experiment Time!**\n",
    "\n",
    "Go back to the code cell above and try changing the initial values for `x_t` and `h_t_minus_1`. \n",
    "\n",
    "- What happens to the final hidden state if `x_t` is very small (e.g., `0.1`)?\n",
    "- What if the previous memory `h_t_minus_1` was much stronger (e.g., `0.9`)?\n",
    "\n",
    "Re-run the cell with your new values and observe the outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec1268",
   "metadata": {},
   "source": [
    "## Topic 4: Building a GRU Model with Code üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66a33b",
   "metadata": {},
   "source": [
    "Now for the fun part! Let's build a real GRU model to perform a simple **sentiment analysis** task. \n",
    "\n",
    "**Our Goal:** Train a model to guess if a sentence is 'Positive' or 'Negative' based on the numbers in it. We'll imagine that higher numbers represent positive words and lower numbers represent negative words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926ac3f",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8722dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[1, 2, 3, 4]\n",
      "\n",
      "Padded data:\n",
      "[1 2 3 4 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# --- Sample Data ---\n",
    "# Imagine these number sequences represent sentences.\n",
    "# Labels: 1 = Positive, 0 = Negative\n",
    "X_train_raw = [\n",
    "    [8, 6, 7, 5, 3, 0, 9], # Positive\n",
    "    [9, 8, 7, 6, 5],       # Positive\n",
    "    [1, 2, 3, 4],          # Negative\n",
    "    [3, 2, 1],             # Negative\n",
    "    [10, 11, 12, 13]       # Positive\n",
    "]\n",
    "y_train = np.array([1, 1, 0, 0, 1])\n",
    "\n",
    "# --- Preprocessing ---\n",
    "# Neural networks need inputs of the same length.\n",
    "# We 'pad' the shorter sentences with zeros so they all have length 10.\n",
    "X_train = pad_sequences(X_train_raw, maxlen=10, padding='post')\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(X_train_raw[2])\n",
    "print(\"\\nPadded data:\")\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557790e0",
   "metadata": {},
   "source": [
    "### Step 2: Build the GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763e3fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 8)             120       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                4032      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,185\n",
      "Trainable params: 4,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Model Parameters ---\n",
    "vocab_size = 15     # How many unique 'words' (numbers) we have\n",
    "embedding_dim = 8   # The size of the vector for each word\n",
    "hidden_units = 32   # The number of memory units in our GRU layer\n",
    "\n",
    "# --- Let's build the model step-by-step ---\n",
    "model = Sequential([\n",
    "    # 1. Embedding Layer: Turns our numbers into meaningful vectors.\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=10),\n",
    "\n",
    "    # 2. GRU Layer: This is the brain! It processes the sequence of vectors.\n",
    "    GRU(units=hidden_units),\n",
    "\n",
    "    # 3. Output Layer: A single neuron that gives a prediction between 0 and 1.\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26627638",
   "metadata": {},
   "source": [
    "### Step 3: Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a18369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6725 - accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6721 - accuracy: 0.6000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6718 - accuracy: 0.6000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6714 - accuracy: 0.6000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6709 - accuracy: 0.6000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6703 - accuracy: 0.6000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6697 - accuracy: 0.6000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6690 - accuracy: 0.6000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6683 - accuracy: 0.6000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6674 - accuracy: 0.6000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6665 - accuracy: 0.6000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6654 - accuracy: 0.6000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6641 - accuracy: 0.6000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6626 - accuracy: 0.6000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6610 - accuracy: 0.6000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6591 - accuracy: 0.6000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6569 - accuracy: 0.6000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6545 - accuracy: 0.6000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6516 - accuracy: 0.6000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6484 - accuracy: 0.6000\n",
      "‚úÖ Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model: This sets up the learning process\n",
    "model.compile(\n",
    "    optimizer='adam',             # A popular and effective optimizer\n",
    "    loss='binary_crossentropy',   # A good loss function for two-class problems (Positive/Negative)\n",
    "    metrics=['accuracy']          # We want to see the accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model!\n",
    "print(\"\\nTraining the model...\")\n",
    "# We'll train for 20 'epochs', meaning we go through the data 20 times.\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1) # verbose=0 keeps the output clean\n",
    "print(\"‚úÖ Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8034fa0",
   "metadata": {},
   "source": [
    "### Step 4: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49d5bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "\n",
      "Predictions:\n",
      "Sequence: [9, 9, 8, 7] -> Predicted Sentiment: Positive (Raw score: 0.6144)\n",
      "Sequence: [1, 1, 2, 3] -> Predicted Sentiment: Positive (Raw score: 0.5881)\n"
     ]
    }
   ],
   "source": [
    "# Let's test our trained model on some new sentences!\n",
    "X_test_raw = [\n",
    "    [9, 9, 8, 7],  # Should be predicted as Positive\n",
    "    [1, 1, 2, 3]   # Should be predicted as Negative\n",
    "]\n",
    "\n",
    "# Remember to pad the test data just like we did with the training data\n",
    "X_test = pad_sequences(X_test_raw, maxlen=10, padding='post')\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for i, text in enumerate(X_test_raw):\n",
    "    # The model outputs a score. If it's > 0.5, we'll call it Positive.\n",
    "    sentiment = \"Positive\" if predictions[i][0] > 0.5 else \"Negative\"\n",
    "    print(f\"Sequence: {text} -> Predicted Sentiment: {sentiment} (Raw score: {predictions[i][0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f476aea",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Go to the **\"Build the GRU Model\"** code cell (Step 2).\n",
    "\n",
    "1.  Change the number of `hidden_units` from `32` to `64`. \n",
    "2.  Re-run that cell and all the cells after it.\n",
    "\n",
    "**Question:** Does increasing the `hidden_units` (giving the GRU more memory capacity) change the final predictions? Why might a larger number be better or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac920ef",
   "metadata": {},
   "source": [
    "## Topic 5: GRU vs. LSTM ü•ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab56cb",
   "metadata": {},
   "source": [
    "You'll often hear GRUs mentioned alongside another popular model: **Long Short-Term Memory (LSTM)**. Both were designed to solve the same memory problem in RNNs, but they do it slightly differently.\n",
    "\n",
    "Here's a quick comparison:\n",
    "\n",
    "| Feature                | GRU (Gated Recurrent Unit)                        | LSTM (Long Short-Term Memory)                  |\n",
    "|------------------------|---------------------------------------------------|------------------------------------------------|\n",
    "| **Gates**              | 2 Gates: Update & Reset                           | 3 Gates: Input, Forget & Output                |\n",
    "| **Internal State**     | Only a single 'hidden state'                      | A separate 'cell state' and a 'hidden state'   |\n",
    "| **Complexity**         | Simpler, fewer parameters                         | More complex, more parameters                  |\n",
    "| **Speed**              | Generally faster to train, less computation       | Slower due to more calculations                |\n",
    "| **Performance**        | Often performs just as well as LSTM               | May perform better on very complex, long sequences |\n",
    "\n",
    "üí° **When to choose a GRU?** A GRU is a great first choice! Since it's simpler and faster, it's perfect for many tasks. If you find your model isn't performing well enough, you can then try an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50bb4f",
   "metadata": {},
   "source": [
    "### üéØ Practice Task (Multiple Choice)\n",
    "\n",
    "**What is the primary purpose of the update gate (`z_t`) in a GRU?**\n",
    "\n",
    "a) To determine how much of the past information to forget.\n",
    "\n",
    "b) To calculate the candidate hidden state.\n",
    "\n",
    "c) To decide how much of the past information to carry forward.\n",
    "\n",
    "d) To apply a non-linear activation function to the output.\n",
    "\n",
    "*(Think about it! The answer is in Topic 2.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726396c",
   "metadata": {},
   "source": [
    "## Topic 6: Real-World Applications üåç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833f44a",
   "metadata": {},
   "source": [
    "GRUs are not just theoretical concepts; they power many applications you might use every day!\n",
    "\n",
    "- üí¨ **Machine Translation:** In tools like Google Translate, GRUs help understand the context of a sentence to provide accurate translations.\n",
    "- üìà **Stock Price Prediction:** By analyzing historical stock data, GRUs can forecast future price movements.\n",
    "- üéµ **Music Generation:** AI music composers can use GRUs to learn musical patterns and generate new melodies.\n",
    "- üó£Ô∏è **Speech Recognition:** When you talk to Siri or Alexa, GRU-like models are working to transcribe your speech into text.\n",
    "- üïµÔ∏è **Anomaly Detection:** They can monitor sequences of data (like network traffic) to detect unusual patterns that might signal a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754dce2",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Can you think of one more application where a model that understands sequences could be useful? Describe it in one sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e4f22",
   "metadata": {},
   "source": [
    "## üéì Final Revision Assignment\n",
    "\n",
    "Congratulations on making it through the session! Here are a few tasks to help you revise and strengthen your understanding. Try to complete these at home.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7e562",
   "metadata": {},
   "source": [
    "**1. Short Answer:** In your own words, explain how the reset gate (`r_t`) and the candidate hidden state (`hÃÉ_t`) work together. What is the reset gate's role in creating the 'new' potential memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe8654",
   "metadata": {},
   "source": [
    "**2. Problem-Solving:** \n",
    "Let's do another calculation! Given:\n",
    "- `h_{t-1}` = 0.5\n",
    "- `x_t` = 1.0\n",
    "- All weights are 0.5 and all biases are 0.\n",
    "\n",
    "Calculate the final hidden state `h_t`. You can use the Python code cell from Topic 3 as a template to help you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7792c92",
   "metadata": {},
   "source": [
    "**3. Coding Task 1: Stacking GRUs**\n",
    "\n",
    "Modify the Python code from Topic 4 to build a **stacked GRU model** with two GRU layers. \n",
    "\n",
    "**Hint:** To pass a sequence from one GRU layer to the next, you need to add an argument to the first GRU layer: `return_sequences=True`. \n",
    "\n",
    "Your new model architecture should look something like this:\n",
    "```python\n",
    "model = Sequential([\n",
    "    Embedding(...),\n",
    "    GRU(units=hidden_units, return_sequences=True), # First GRU layer\n",
    "    GRU(units=hidden_units), # Second GRU layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "Try running the full training and prediction process with this new, deeper model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b9b3",
   "metadata": {},
   "source": [
    "**4. Coding Task 2: Add More Test Data**\n",
    "\n",
    "In the \"Make Predictions\" cell (Step 4 of Topic 4), add two new sentences to the `X_test_raw` list and see what the model predicts for them. \n",
    "\n",
    "For example:\n",
    "- `[10, 0, 12, 5]` (a mix of high and low numbers)\n",
    "- `[4, 3, 2, 1, 0]` (a clearly 'negative' sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17311f2e",
   "metadata": {},
   "source": [
    "**5. Case Study:**\n",
    "You are asked to build a model to predict the next word in a sentence (like the autocomplete on your phone). Would you choose a simple RNN, a GRU, or an LSTM? Justify your choice by discussing the advantages and disadvantages of each for this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5adcc",
   "metadata": {},
   "source": [
    "## üéâ You've completed the session! Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
