{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e456d0c",
   "metadata": {},
   "source": [
    "# ü§ñ Building an Emotion Classifier with GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ca7bb",
   "metadata": {},
   "source": [
    "### üìò Welcome to Your 2-Hour Practical Guide!\n",
    "\n",
    "In this session, we'll apply our knowledge of Gated Recurrent Units (GRUs) to a real-world problem: **classifying emotions in tweets**. We will build a model that can read a tweet and predict whether the emotion is joy, sadness, anger, fear, love, or surprise.\n",
    "\n",
    "This is a multi-class text classification task, and it's a perfect job for a GRU because understanding emotion requires understanding the sequence and context of words.\n",
    "\n",
    "--- \n",
    "\n",
    "### üéØ Learning Objectives for Today:\n",
    "\n",
    "By the end of this 2-hour session, you will be able to:\n",
    "1.  **Load and prepare** a real-world text dataset using Pandas.\n",
    "2.  **Preprocess text data** for a neural network using a Tokenizer.\n",
    "3.  **Convert text labels** into a format the model can understand (one-hot encoding).\n",
    "4.  **Build a GRU-based model** for multi-class classification using TensorFlow/Keras.\n",
    "5.  **Train your model** on the emotions dataset.\n",
    "6.  **Evaluate its performance** and use it to predict emotions on new sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66213c",
   "metadata": {},
   "source": [
    "## Topic 1: Setup and Importing Libraries üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac42930",
   "metadata": {},
   "source": [
    "First things first, let's import all the tools we'll need for our project. We'll be using:\n",
    "- **TensorFlow & Keras:** For building and training our GRU model.\n",
    "- **Pandas:** For loading and manipulating our data.\n",
    "- **Numpy:** For numerical operations.\n",
    "- **Scikit-learn:** For processing our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "773b8f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4933b5",
   "metadata": {},
   "source": [
    "## Topic 2: Loading and Exploring the Dataset üìÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c168bd0",
   "metadata": {},
   "source": [
    "We will use a popular public dataset of tweets, each labeled with one of six emotions. We can load it directly from a URL into a pandas DataFrame. This makes our notebook easy to run anywhere!\n",
    "\n",
    "The data is in a `.txt` file, but we can treat it like a CSV with a semicolon (`;`) separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a376f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  emotion\n",
      "0                            i didnt feel humiliated        0\n",
      "1  i can go from feeling so hopeless to so damned...        0\n",
      "2   im grabbing a minute to post i feel greedy wrong        3\n",
      "3  i am ever feeling nostalgic about the fireplac...        2\n",
      "4                               i am feeling grouchy        3\n",
      "emotion\n",
      "1    5362\n",
      "0    4666\n",
      "3    2159\n",
      "4    1937\n",
      "2    1304\n",
      "5     572\n",
      "Name: count, dtype: int64\n",
      "Train shape: (16000, 2), Test shape: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd  # Optional: for DataFrame inspection\n",
    "\n",
    "# Load the full dataset (includes train, validation, test splits automatically)\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# Convert to DataFrames (matches your df_train/df_test structure)\n",
    "# Note: Column is 'label' (integers 0-5), not 'emotion'‚Äîeasy to rename/map\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Optional: Rename 'label' to 'emotion' for consistency with your code\n",
    "df_train = df_train.rename(columns={'label': 'emotion'})\n",
    "df_test = df_test.rename(columns={'label': 'emotion'})\n",
    "\n",
    "# Quick inspection (should match what you expected)\n",
    "print(df_train.head())\n",
    "print(df_train['emotion'].value_counts())\n",
    "print(f\"Train shape: {df_train.shape}, Test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469375e",
   "metadata": {},
   "source": [
    "Let's check the distribution of emotions in our training data to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3370b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Distribution:\n",
      "emotion\n",
      "1    5362\n",
      "0    4666\n",
      "3    2159\n",
      "4    1937\n",
      "2    1304\n",
      "5     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Emotion Distribution:\")\n",
    "print(df_train['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbaae35-1802-48f6-9706-4e814d822690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Codes:\n",
      "  0 ‚Üí sadness\n",
      "  1 ‚Üí joy\n",
      "  2 ‚Üí love\n",
      "  3 ‚Üí anger\n",
      "  4 ‚Üí fear\n",
      "  5 ‚Üí surprise\n",
      "\n",
      "Reverse mapping (label2id): {'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "# Emotion label mapping for the dair-ai/emotion dataset\n",
    "id2label = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Emotion Codes:\")\n",
    "for code, emotion in id2label.items():\n",
    "    print(f\"  {code} ‚Üí {emotion}\")\n",
    "\n",
    "print(\"\\nReverse mapping (label2id):\", label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac180db",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Write one line of code to get the first tweet in the training data that has the emotion 'fear'. \n",
    "\n",
    "**Hint:** You can filter a DataFrame like this: `df_train[df_train['emotion'] == 'fear']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dba1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to find and print the first 'fear' tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371871ef",
   "metadata": {},
   "source": [
    "## Topic 3: Text Preprocessing & Tokenization üßπ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c79fb",
   "metadata": {},
   "source": [
    "A neural network can't understand words directly. We need to convert our text into numbers. This process involves two main steps:\n",
    "\n",
    "1.  **Tokenization:** We'll create a vocabulary of all the unique words in our training data. Then, we'll assign a unique integer to each word. For example, `{'the': 1, 'love': 2, 'i': 3, ...}`.\n",
    "2.  **Sequencing:** We'll convert each tweet into a sequence of these integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a09f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:\n",
      "i didnt feel humiliated\n",
      "\n",
      "Tweet after Tokenization (converted to numbers):\n",
      "[2, 139, 3, 679]\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for tokenization\n",
    "vocab_size = 10000  # We will only consider the top 10,000 most frequent words\n",
    "oov_token = \"<OOV>\" # A special token for words not in our vocabulary\n",
    "\n",
    "# Initialize the Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "\n",
    "# Build the vocabulary based on the training text\n",
    "tokenizer.fit_on_texts(df_train['text'])\n",
    "\n",
    "# --- Convert text to sequences of integers ---\n",
    "X_train_sequences = tokenizer.texts_to_sequences(df_train['text'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(df_test['text'])\n",
    "\n",
    "print(\"Original Tweet:\")\n",
    "print(df_train['text'][0])\n",
    "print(\"\\nTweet after Tokenization (converted to numbers):\")\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186e772",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "The tokenizer has a `word_index` attribute that holds the vocabulary map. Print the first 10 items of `tokenizer.word_index` to see what it looks like.\n",
    "\n",
    "**Hint:** You can't slice a dictionary directly, but you can loop through its items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366821b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<OOV>', 1), ('i', 2), ('feel', 3), ('and', 4), ('to', 5), ('the', 6), ('a', 7), ('feeling', 8), ('that', 9), ('of', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here to see the first 10 words in the vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print(list(word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012f8b4",
   "metadata": {},
   "source": [
    "## Topic 4: Padding Sequences & Preparing Labels üìè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd94401",
   "metadata": {},
   "source": [
    "Our sequences of numbers have different lengths because tweets have different lengths. GRU models, however, require inputs to have a uniform length.\n",
    "\n",
    "### Padding\n",
    "We will use **padding** to make all sequences the same length. We'll add zeros to the end of shorter sequences until they match the length of the longest one (or a `maxlen` we define).\n",
    "\n",
    "### Preparing Labels\n",
    "We also need to convert our emotion labels (like 'joy', 'sadness') into numbers. We'll do this in two steps:\n",
    "1.  **Label Encoding:** Convert each emotion string to a unique integer ('joy' -> 0, 'sadness' -> 1, etc.).\n",
    "2.  **One-Hot Encoding:** Convert each integer into a binary vector. This is the standard format for multi-class classification.\n",
    "    - `0` (joy) becomes `[1, 0, 0, 0, 0, 0]`\n",
    "    - `1` (sadness) becomes `[0, 1, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7347abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Padded Sequences ---\n",
      "Shape of training data: (16000, 100)\n",
      "Example padded sequence:\n",
      "[  2 139   3 679   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "\n",
      "--- Prepared Labels ---\n",
      "Original label: 0\n",
      "Encoded label: 0\n",
      "One-hot encoded label:\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Emotion classes: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# --- Padding the Sequences ---\n",
    "max_length = 100 # Maximum length of a sequence\n",
    "padding_type = 'post' # Add padding at the end\n",
    "trunc_type = 'post' # Truncate from the end if longer than max_length\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(\"--- Padded Sequences ---\")\n",
    "print(f\"Shape of training data: {X_train_padded.shape}\")\n",
    "print(f\"Example padded sequence:\\n{X_train_padded[0]}\")\n",
    "\n",
    "# --- Preparing the Labels ---\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(df_train['emotion'])\n",
    "y_test_encoded = le.transform(df_test['emotion'])\n",
    "\n",
    "# One-Hot Encode\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes=6)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes=6)\n",
    "\n",
    "print(\"\\n--- Prepared Labels ---\")\n",
    "print(f\"Original label: {df_train['emotion'][0]}\")\n",
    "print(f\"Encoded label: {y_train_encoded[0]}\")\n",
    "print(f\"One-hot encoded label:\\n{y_train_onehot[0]}\")\n",
    "\n",
    "# For later use, let's store the mapping from class index to emotion name\n",
    "emotion_labels = le.classes_\n",
    "print(f\"\\nEmotion classes: {emotion_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbda6cd",
   "metadata": {},
   "source": [
    "## Topic 5: Building the GRU Model üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953054ab",
   "metadata": {},
   "source": [
    "Time to build our GRU model! It will have three main layers:\n",
    "\n",
    "1.  **Embedding Layer:** This layer learns a dense vector representation for each word in our vocabulary. These vectors capture semantic meaning, so words like 'happy' and 'joyful' will have similar vectors.\n",
    "2.  **GRU Layer:** This is the core of our model. It will process the sequence of word vectors and learn to identify patterns related to different emotions.\n",
    "3.  **Dense Output Layer:** This layer takes the output from the GRU and makes a final prediction. Because we have 6 emotions, it will have 6 neurons and use the `softmax` activation function to output a probability for each emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "479b8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 100, 128)          74496     \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 256)               296448    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,012,486\n",
      "Trainable params: 1,012,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "embedding_dim = 64\n",
    "gru_units = 128\n",
    "\n",
    "model = Sequential([\n",
    "    # 1. Embedding Layer\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    \n",
    "    # 2. GRU Layer\n",
    "    GRU(units=gru_units, return_sequences=True),\n",
    "    GRU(units=256, return_sequences=False),\n",
    "    \n",
    "    # We can add a Dropout layer to prevent overfitting\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # 3. Dense Output Layer\n",
    "    Dense(6, activation='softmax') # 6 units for 6 emotions, softmax for multi-class probability\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # Use this loss for multi-class, one-hot encoded labels\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa278aa",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Explain in one sentence why the final `Dense` layer has `6` units and uses the `'softmax'` activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe412154",
   "metadata": {},
   "source": [
    "## Topic 6: Training the Model üöÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c3f46",
   "metadata": {},
   "source": [
    "Now we feed our prepared data into the model and let it learn! We will train it for a few epochs. An **epoch** is one complete pass through the entire training dataset.\n",
    "\n",
    "We also provide our test data as `validation_data`. This allows us to monitor how well our model is performing on unseen data after each epoch, which helps us spot overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03df373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model training...\n",
      "Epoch 1/10\n",
      "500/500 - 100s - loss: 1.5857 - accuracy: 0.3271 - val_loss: 1.5618 - val_accuracy: 0.3475 - 100s/epoch - 200ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 98s - loss: 1.5808 - accuracy: 0.3311 - val_loss: 1.5646 - val_accuracy: 0.3475 - 98s/epoch - 197ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 109s - loss: 1.5795 - accuracy: 0.3298 - val_loss: 1.5635 - val_accuracy: 0.2905 - 109s/epoch - 219ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 107s - loss: 1.5786 - accuracy: 0.3325 - val_loss: 1.5616 - val_accuracy: 0.3475 - 107s/epoch - 213ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 117s - loss: 1.5783 - accuracy: 0.3322 - val_loss: 1.5603 - val_accuracy: 0.3475 - 117s/epoch - 234ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 106s - loss: 1.5784 - accuracy: 0.3327 - val_loss: 1.5596 - val_accuracy: 0.3475 - 106s/epoch - 213ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 105s - loss: 1.5772 - accuracy: 0.3351 - val_loss: 1.5596 - val_accuracy: 0.3475 - 105s/epoch - 211ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 126s - loss: 1.5776 - accuracy: 0.3344 - val_loss: 1.5599 - val_accuracy: 0.3475 - 126s/epoch - 252ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 194s - loss: 1.5768 - accuracy: 0.3338 - val_loss: 1.5606 - val_accuracy: 0.3475 - 194s/epoch - 389ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 103s - loss: 1.5770 - accuracy: 0.3334 - val_loss: 1.5644 - val_accuracy: 0.3475 - 103s/epoch - 206ms/step\n",
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "print(\"üöÄ Starting model training...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train_onehot, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(X_test_padded, y_test_onehot),\n",
    "    verbose=2 # Show one line per epoch\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8841a7",
   "metadata": {},
   "source": [
    "### üéØ Practice Task\n",
    "\n",
    "Training a model can take time. If you want to train for longer, what single number would you change in the code cell above? What do you think would happen to the training and validation accuracy if you set it to `20`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f1299e",
   "metadata": {},
   "source": [
    "## Topic 7: Evaluating the Model & Making Predictions üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675af931",
   "metadata": {},
   "source": [
    "After training, let's see how well our model performs on the test set, which it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbbdf55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 89ms/step - loss: 1.5610 - accuracy: 0.3475\n",
      "\n",
      "Test Accuracy: 34.75%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test_onehot)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9e604",
   "metadata": {},
   "source": [
    "Now for the most exciting part: let's write our own sentences and see what emotion our model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict emotion of a custom sentence\n",
    "def predict_emotion(sentence):\n",
    "    # 1. Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    # 2. Pad the sequence\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    # 3. Make a prediction\n",
    "    prediction = model.predict(padded)\n",
    "    # 4. Get the emotion label with the highest probability\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    return emotion_labels[predicted_class_index]\n",
    "\n",
    "# --- Let's test it! ---\n",
    "my_sentence_1 = \"I am so happy and excited about the trip tomorrow\"\n",
    "my_sentence_2 = \"I feel so alone and lost right now\"\n",
    "my_sentence_3 = \"that is an awful thing to say\"\n",
    "\n",
    "print(f\"Sentence: '{my_sentence_1}' -> Predicted Emotion: {predict_emotion(my_sentence_1)}\")\n",
    "print(f\"Sentence: '{my_sentence_2}' -> Predicted Emotion: {predict_emotion(my_sentence_2)}\")\n",
    "print(f\"Sentence: '{my_sentence_3}' -> Predicted Emotion: {predict_emotion(my_sentence_3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d03e2d-55fe-4dd4-a8d0-61dcbc2c11f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 128)          98816     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               394240    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,134,598\n",
      "Trainable params: 1,134,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 64\n",
    "lstm_units_1 = 128\n",
    "lstm_units_2 = 256\n",
    "\n",
    "model = Sequential([\n",
    "    # 1. Embedding Layer\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    \n",
    "    # 2. First LSTM Layer (returns sequences for the next LSTM)\n",
    "    LSTM(units=lstm_units_1, return_sequences=True),\n",
    "    \n",
    "    # 3. Second LSTM Layer (last LSTM, no need to return sequences)\n",
    "    LSTM(units=lstm_units_2, return_sequences=False),\n",
    "    \n",
    "    # Optional Dropout to prevent overfitting\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # 4. Dense Output Layer\n",
    "    Dense(6, activation='softmax')  # 6 classes for emotions\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # multi-class classification\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eed68c-739c-460b-b5d1-9b6e503294a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model training...\n",
      "Epoch 1/10\n",
      "500/500 - 8040s - loss: 1.5843 - accuracy: 0.3295 - val_loss: 1.5682 - val_accuracy: 0.3475 - 8040s/epoch - 16s/step\n",
      "Epoch 2/10\n",
      "500/500 - 103s - loss: 1.5799 - accuracy: 0.3307 - val_loss: 1.5610 - val_accuracy: 0.3475 - 103s/epoch - 206ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 106s - loss: 1.5787 - accuracy: 0.3332 - val_loss: 1.5628 - val_accuracy: 0.3475 - 106s/epoch - 213ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 230s - loss: 1.5781 - accuracy: 0.3316 - val_loss: 1.5594 - val_accuracy: 0.3475 - 230s/epoch - 461ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 109s - loss: 1.5780 - accuracy: 0.3314 - val_loss: 1.5585 - val_accuracy: 0.3475 - 109s/epoch - 219ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 118s - loss: 1.5775 - accuracy: 0.3319 - val_loss: 1.5690 - val_accuracy: 0.2905 - 118s/epoch - 236ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 123s - loss: 1.5779 - accuracy: 0.3300 - val_loss: 1.5604 - val_accuracy: 0.3475 - 123s/epoch - 246ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 126s - loss: 1.5776 - accuracy: 0.3328 - val_loss: 1.5597 - val_accuracy: 0.3475 - 126s/epoch - 251ms/step\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "print(\"üöÄ Starting model training...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train_onehot, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(X_test_padded, y_test_onehot),\n",
    "    verbose=2 # Show one line per epoch\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7755fa",
   "metadata": {},
   "source": [
    "## üéì Final Revision Assignment\n",
    "\n",
    "Great job today! Here are a few tasks to help you solidify your understanding of building GRU models for text classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b00eb8",
   "metadata": {},
   "source": [
    "**1. Conceptual Question:** Why is a GRU (or any RNN) a better choice for this emotion classification task than a simple feed-forward neural network that doesn't consider word order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9287ae",
   "metadata": {},
   "source": [
    "**2. Short Answer:** What is the purpose of the `Embedding` layer? What does it do to the integer-encoded sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7722d",
   "metadata": {},
   "source": [
    "**3. Coding Task 1: Hyperparameter Tuning**\n",
    "\n",
    "Go back to the model-building cell (Topic 5) and change some hyperparameters. Try one of the following:\n",
    "- Change the `embedding_dim` to `128`.\n",
    "- Change the `gru_units` to `64`.\n",
    "\n",
    "Re-run the training and evaluation cells. Does the test accuracy improve or get worse? Document your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9b12a",
   "metadata": {},
   "source": [
    "**4. Coding Task 2: Predict Your Own Emotions**\n",
    "\n",
    "In the final prediction cell (Topic 7), add three of your own sentences to test the model. Try to make them tricky! Do the model's predictions match the emotions you were trying to convey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ade1ce",
   "metadata": {},
   "source": [
    "**5. Coding Task 3: Build a Deeper Model**\n",
    "\n",
    "Try building a stacked GRU model with two GRU layers. You will need to add `return_sequences=True` to the first GRU layer so it passes its full output sequence to the next layer. \n",
    "\n",
    "```python\n",
    "# Example of a stacked GRU architecture\n",
    "model = Sequential([\n",
    "    Embedding(...),\n",
    "    GRU(units=128, return_sequences=True), # First layer\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64), # Second layer\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "```\n",
    "Does this deeper model perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7ddff",
   "metadata": {},
   "source": [
    "## üéâ You've built your first end-to-end NLP model! Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
